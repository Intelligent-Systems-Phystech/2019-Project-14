{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from deepSymmetry.src import load_data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from se3cnn import SE3Convolution, SE3Dropout, SE3BNConvolution\n",
    "from se3cnn.blocks import GatedBlock\n",
    "from se3cnn.non_linearities import ScalarActivation\n",
    "from se3cnn.dropout import SE3Dropout\n",
    "from se3cnn import kernel\n",
    "from se3cnn.filter import low_pass_filter\n",
    "\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data/dataReady0\n",
      "Extracting Data/dataReady0_label\n",
      "(39785, 13824)\n"
     ]
    }
   ],
   "source": [
    "train_name = 'Data/dataReady0'\n",
    "train_set = load_data.read_data_set(train_name, dtype=dtypes.float16, seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_custom_loss(output, target):\n",
    "    order_out = output[:, 0 : NUM_CLASSES]\n",
    "    order_target = target[:, 0 : 1].type(torch.LongTensor).squeeze_()\n",
    "    axis_out = output[:, NUM_CLASSES : NUM_CLASSES + 6]\n",
    "    axis_target = target[:, 1 : 7]\n",
    "\n",
    "    loss = 0.5 * nn.CrossEntropyLoss()(order_out, order_target) + nn.MSELoss(reduction='sum')(axis_out, axis_target)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residential network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResEquiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SE3BNConvolution(repr_in_1, repr_out_1, size=4)\n",
    "        self.pool1 = nn.AvgPool3d(pool_size, pool_stride)\n",
    "        self.conv2 = SE3BNConvolution(repr_in_2, repr_out_2, size=4)\n",
    "        self.pool2 = nn.AvgPool3d(pool_size, pool_stride)\n",
    "        \n",
    "        self.lin1 = nn.Linear(n_input_1, n_output_1)\n",
    "        self.drop1 = nn.Dropout(prob)\n",
    "        self.lin2 = nn.Linear(n_output_1, n_output_2)\n",
    "        self.drop2 = nn.Dropout(prob)\n",
    "        self.lin3 = nn.Linear(n_output_2, NUM_CLASSES+6)\n",
    "        \n",
    "        self.best_loss = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        prev_layer = x.expand(100,72,24,24,24).clone()\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        '''\n",
    "        x = torch.cat([torch.zeros(100,72,7,10,10), x, torch.zeros(100,72,7,10,10)], 2)\n",
    "        x = torch.cat([torch.zeros(100,72,24,7,10), x, torch.zeros(100,72,24,7,10)], 3)\n",
    "        x = torch.cat([torch.zeros(100,72,24,24,7), x, torch.zeros(100,72,24,24,7)], 4)\n",
    "        x = torch.add(x, prev_layer).clone()\n",
    "        x = self.pool2(self.conv2(prev_layer))\n",
    "        '''\n",
    "        \n",
    "        x = x.view(batch_size,-1) \n",
    "        x = F.leaky_relu(self.lin1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.leaky_relu(self.lin2(x))\n",
    "        x = self.drop2(x)\n",
    "        return self.lin3(x)\n",
    "    \n",
    "def train(model, device, train_set, batch_size, optimizer, epoch, per_epoch, decr_rate, log_name):\n",
    "    model.train()\n",
    "    flag = True\n",
    "    new_epoch = True\n",
    "    \n",
    "    if new_epoch:\n",
    "        full_loss = 0\n",
    "        batch_idx = 1\n",
    "        new_epoch = False\n",
    "        data, target, _ = train_set.next_batch(batch_size)\n",
    "        data = torch.from_numpy(data.reshape(batch_size,1,24,24,24)).type(torch.FloatTensor)\n",
    "        target = torch.from_numpy(target.reshape(batch_size,-1)).type(torch.FloatTensor)\n",
    "        cnt = epoch // per_epoch\n",
    "        if ((epoch+1) // per_epoch > cnt) and flag:\n",
    "            lr = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            lr *= decr_rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            flag = False\n",
    "        if ((epoch+1) // per_epoch <= cnt):\n",
    "            flag = True\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss_fn = nn.MSELoss(reduction='sum')\n",
    "        # loss = loss_fn(output, target)\n",
    "        loss = weighted_custom_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    while (train_set._index_in_epoch + batch_size) < train_set._num_examples:\n",
    "        full_loss += loss.item()\n",
    "        batch_idx += 1\n",
    "        data, target, _ = train_set.next_batch(batch_size)\n",
    "        data = torch.from_numpy(data.reshape(batch_size,1,24,24,24)).type(torch.FloatTensor)\n",
    "        target = torch.from_numpy(target.reshape(batch_size,-1)).type(torch.FloatTensor)\n",
    "        cnt = epoch // per_epoch\n",
    "        if ((epoch+1) // per_epoch > cnt) and flag:\n",
    "            lr = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            lr *= decr_rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            flag = False\n",
    "        if ((epoch+1) // per_epoch <= cnt):\n",
    "            flag = True\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss_fn = nn.MSELoss(reduction='sum')\n",
    "        # loss = loss_fn(output, target)\n",
    "        loss = weighted_custom_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, train_set._num_examples,\n",
    "                100. * batch_idx / train_set._num_examples, loss.item()))\n",
    "            with open(log_name, 'a') as file:\n",
    "                file.write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                    epoch, batch_idx * batch_size, train_set._num_examples,\n",
    "                    100. * batch_idx / train_set._num_examples, loss.item()))\n",
    "    \n",
    "    if model.best_loss is None:\n",
    "        model.best_loss = full_loss\n",
    "    \n",
    "    if full_loss <= model.best_loss:\n",
    "        model.best_loss = full_loss\n",
    "        torch.save({'epoch': epoch + 1, \n",
    "                    'state_dict': model.state_dict(), \n",
    "                    'optimizer' : optimizer.state_dict()}, 'checkpoint.pth.tar')\n",
    "    \n",
    "\n",
    "def test(model, device, test_set):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_set:\n",
    "            output = model(data)\n",
    "            loss_fn = nn.MSELoss(reduction='sum')\n",
    "            test_loss += loss_fn(output, target) \n",
    "            # pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += int(torch.argmax(output) == torch.argmax(target))\n",
    "\n",
    "    test_loss /= len(test_set)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_set),\n",
    "        100. * correct / len(test_set)))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_in_1 = [(1,0)]\n",
    "repr_out_1 = [(2,0),(2,1),(2,2)] #,(2,3),(2,4),(2,5)] #,(2,6),(2,7),(2,8),(2,9)]\n",
    "repr_in_2 = [(2,0),(2,1),(2,2),(2,3),(2,4),(2,5)] #,(2,6),(2,7),(2,8),(2,9)]\n",
    "repr_out_2 = [(1,0),(2,1),(2,2),(2,3)]\n",
    "size = 4\n",
    "activation = (None, F.leaky_relu)\n",
    "pool_size = 2\n",
    "pool_stride = 2\n",
    "bias = True\n",
    "\n",
    "n_input_1 = 18000\n",
    "n_output_1 = 5000 \n",
    "n_output_2 = 50\n",
    "\n",
    "batch_size = 100\n",
    "prob = 0.5\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1000/6240 (0%)]\tLoss: 1683.684692\n",
      "Train Epoch: 1 [2000/6240 (0%)]\tLoss: 21494.935547\n",
      "Train Epoch: 1 [3000/6240 (0%)]\tLoss: 2754.620361\n",
      "Train Epoch: 1 [4000/6240 (1%)]\tLoss: 11524.377930\n",
      "Train Epoch: 1 [5000/6240 (1%)]\tLoss: 9824.939453\n",
      "Train Epoch: 1 [6000/6240 (1%)]\tLoss: 2726.273193\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "device = torch.device('cpu')\n",
    "torch.manual_seed(1)\n",
    "\n",
    "model_tenth_order = ResEquiNet().to(device)\n",
    "learning_rate = 5e-3\n",
    "optimizer = torch.optim.Adam(model_tenth_order.parameters(), lr=learning_rate)\n",
    "\n",
    "per_epoch = 10\n",
    "decr_rate = 0.995\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model_tenth_order, device, test_set, batch_size, optimizer, epoch, per_epoch, decr_rate, 'reslog.txt')\n",
    "    # test(model_hard, device, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data/density_testf\n",
      "Extracting Data/density_testf_label\n",
      "(6240, 13824)\n"
     ]
    }
   ],
   "source": [
    "test_name = 'Data/density_testf'\n",
    "test_set = load_data.read_data_set(test_name, dtype=dtypes.float16, seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    torch.save({'epoch': epoch + 1, \n",
    "                'state_dict': model_tenth_order.state_dict(), \n",
    "                'optimizer' : optimizer.state_dict()}, 'checkpoint.pth.tar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
