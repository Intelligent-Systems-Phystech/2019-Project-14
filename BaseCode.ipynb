{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import load_data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from se3cnn import SE3Convolution, SE3Dropout\n",
    "from se3cnn.blocks import GatedBlock\n",
    "from se3cnn.non_linearities import ScalarActivation\n",
    "from se3cnn.dropout import SE3Dropout\n",
    "from se3cnn import kernel\n",
    "from se3cnn.filter import low_pass_filter\n",
    "\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_1(t):\n",
    "    return torch.flip(t, (3, )).transpose(4, 3)\n",
    "\n",
    "def untransformed(n, size):\n",
    "    untransf = []\n",
    "    for i in range(n):\n",
    "        a_i = torch.zeros(1,1,size,size,size)\n",
    "        for j in range(size):\n",
    "            a_i[0,0,j,j,0] = np.random.randn()\n",
    "            a_i[0,0,j,0,j] = a_i[0,0,j,j,0]\n",
    "            a_i[0,0,0,j,j] = a_i[0,0,j,j,0]\n",
    "            \n",
    "        untransf.extend([(a_i, torch.tensor([1,1,1], dtype=torch.float32)), \n",
    "                         (rotation_1(a_i), torch.tensor([-1,1,1], dtype=torch.float32)), \n",
    "                         (rotation_1(rotation_1(a_i)), torch.tensor([-1,1,-1], dtype=torch.float32)), \n",
    "                         (rotation_1(rotation_1(rotation_1(a_i))), torch.tensor([1,1,-1], dtype=torch.float32))\n",
    "                        ])\n",
    "    return untransf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.gated = PoolGatedBlock(repr_in, repr_out, size, activation=activation, \n",
    "                                    #pool_size=pool_size, pool_stride=pool_stride, bias=bias)\n",
    "        # self.conv = nn.Conv3d(1, 2, 2, 2, 1)\n",
    "        self.conv1 = SE3Convolution(repr_in_1, repr_out_2, size=4)\n",
    "        self.pool1 = nn.AvgPool3d(pool_size, pool_stride)\n",
    "        self.conv2 = SE3Convolution(repr_in_2, repr_out_2, size=4)\n",
    "        self.pool2 = nn.AvgPool3d(pool_size, pool_stride)\n",
    "        \n",
    "        self.lin1 = nn.Linear(n_input_1, n_output_1)\n",
    "        self.drop1 = nn.Dropout(prob)\n",
    "        self.lin2 = nn.Linear(n_output_1, n_output_2)\n",
    "        self.drop2 = nn.Dropout(prob)\n",
    "        self.lin3 = nn.Linear(n_output_2, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.gated(x)\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        #x = self.pool2(self.conv2(x))\n",
    "        x = x.view(batch_size,-1) # tf.reshape(x,[batchSize,-1])\n",
    "        x = F.leaky_relu(self.lin1(x))\n",
    "        # x = self.drop1(x)\n",
    "        # x = F.relu(self.lin2(x))\n",
    "        # x = self.drop2(x)\n",
    "        return self.lin2(x) #self.lin3(x)\n",
    "    \n",
    "    \n",
    "def train(model, device, train_set, batch_size, optimizer, epoch, per_epoch, decr_rate):\n",
    "    model.train()\n",
    "    flag = True\n",
    "    new_epoch = True\n",
    "    print(train_set._index_in_epoch)\n",
    "    print(train_set._num_examples)\n",
    "    \n",
    "    if new_epoch:\n",
    "        batch_idx = 1\n",
    "        new_epoch = False\n",
    "        data, target, _ = train_set.next_batch(batch_size)\n",
    "        data = torch.from_numpy(data.reshape(batch_size,1,24,24,24)).type(torch.FloatTensor)\n",
    "        target = torch.from_numpy(target.reshape(batch_size,-1)).type(torch.FloatTensor)\n",
    "        cnt = epoch // per_epoch\n",
    "        if ((epoch+1) // per_epoch > cnt) and flag:\n",
    "            lr = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            print(lr)\n",
    "            lr *= decr_rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_fn = nn.MSELoss(reduction='sum')\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    while (train_set._index_in_epoch + batch_size) < train_set._num_examples:\n",
    "        batch_idx += 1\n",
    "        data, target, _ = train_set.next_batch(batch_size)\n",
    "        data = torch.from_numpy(data.reshape(batch_size,1,24,24,24)).type(torch.FloatTensor)\n",
    "        target = torch.from_numpy(target.reshape(batch_size,-1)).type(torch.FloatTensor)\n",
    "        cnt = epoch // per_epoch\n",
    "        if ((epoch+1) // per_epoch > cnt) and flag:\n",
    "            lr = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            print(lr)\n",
    "            lr *= decr_rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_fn = nn.MSELoss(reduction='sum')\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, train_set._num_examples,\n",
    "                100. * batch_idx / train_set._num_examples, loss.item()))\n",
    "\n",
    "def test(model, device, test_set):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_set:\n",
    "            # data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss_fn = nn.MSELoss(reduction='sum')\n",
    "            test_loss += loss_fn(output, target) # sum up batch loss\n",
    "            # pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += int(torch.argmax(output) == torch.argmax(target))\n",
    "\n",
    "    test_loss /= len(test_set)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_set),\n",
    "        100. * correct / len(test_set)))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_in_1 = [(1,0)]\n",
    "repr_out_1 = [(2,0),(2,1),(2,2)]\n",
    "repr_in_2 = [(2,0),(2,1),(2,2)]\n",
    "repr_out_2 = [(1,0),(2,1)]\n",
    "size = 4\n",
    "activation = (None, F.leaky_relu)\n",
    "pool_size = 2\n",
    "pool_stride = 2\n",
    "bias = True\n",
    "\n",
    "n_input_1 = 7000\n",
    "n_output_1 = 3500 \n",
    "n_output_2 = 7 \n",
    "\n",
    "batch_size = 100\n",
    "prob = 0.5\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../deepSymmetry/data/fromScripts/dataReady0\n",
      "Extracting ../deepSymmetry/data/fromScripts/dataReady0_label\n",
      "(39785, 13824)\n"
     ]
    }
   ],
   "source": [
    "train_name = '../deepSymmetry/data/fromScripts/dataReady0'\n",
    "train_set = load_data.read_data_set(train_name, dtype=dtypes.float16, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "39785\n",
      "Train Epoch: 1 [1000/39785 (0%)]\tLoss: 2721.561279\n",
      "Train Epoch: 1 [2000/39785 (0%)]\tLoss: 1074.573975\n",
      "Train Epoch: 1 [3000/39785 (0%)]\tLoss: 973.752808\n",
      "Train Epoch: 1 [4000/39785 (0%)]\tLoss: 829.442627\n",
      "Train Epoch: 1 [5000/39785 (0%)]\tLoss: 574.667908\n",
      "Train Epoch: 1 [6000/39785 (0%)]\tLoss: 739.444763\n",
      "Train Epoch: 1 [7000/39785 (0%)]\tLoss: 558.614746\n",
      "Train Epoch: 1 [8000/39785 (0%)]\tLoss: 556.047913\n",
      "Train Epoch: 1 [9000/39785 (0%)]\tLoss: 472.887207\n",
      "Train Epoch: 1 [10000/39785 (0%)]\tLoss: 657.138245\n",
      "Train Epoch: 1 [11000/39785 (0%)]\tLoss: 715.385254\n",
      "Train Epoch: 1 [12000/39785 (0%)]\tLoss: 561.910278\n",
      "Train Epoch: 1 [13000/39785 (0%)]\tLoss: 463.862244\n",
      "Train Epoch: 1 [14000/39785 (0%)]\tLoss: 558.785278\n",
      "Train Epoch: 1 [15000/39785 (0%)]\tLoss: 489.248169\n",
      "Train Epoch: 1 [16000/39785 (0%)]\tLoss: 550.416016\n",
      "Train Epoch: 1 [17000/39785 (0%)]\tLoss: 524.253906\n",
      "Train Epoch: 1 [18000/39785 (0%)]\tLoss: 577.391418\n",
      "Train Epoch: 1 [19000/39785 (0%)]\tLoss: 370.317200\n",
      "Train Epoch: 1 [20000/39785 (1%)]\tLoss: 350.959076\n",
      "Train Epoch: 1 [21000/39785 (1%)]\tLoss: 376.429077\n",
      "Train Epoch: 1 [22000/39785 (1%)]\tLoss: 438.304474\n",
      "Train Epoch: 1 [23000/39785 (1%)]\tLoss: 499.927246\n",
      "Train Epoch: 1 [24000/39785 (1%)]\tLoss: 527.434448\n",
      "Train Epoch: 1 [25000/39785 (1%)]\tLoss: 357.267456\n",
      "Train Epoch: 1 [26000/39785 (1%)]\tLoss: 393.522675\n",
      "Train Epoch: 1 [27000/39785 (1%)]\tLoss: 407.494904\n",
      "Train Epoch: 1 [28000/39785 (1%)]\tLoss: 455.798004\n",
      "Train Epoch: 1 [29000/39785 (1%)]\tLoss: 417.936035\n",
      "Train Epoch: 1 [30000/39785 (1%)]\tLoss: 426.208160\n",
      "Train Epoch: 1 [31000/39785 (1%)]\tLoss: 356.392303\n",
      "Train Epoch: 1 [32000/39785 (1%)]\tLoss: 387.358856\n",
      "Train Epoch: 1 [33000/39785 (1%)]\tLoss: 372.442474\n",
      "Train Epoch: 1 [34000/39785 (1%)]\tLoss: 553.408936\n",
      "Train Epoch: 1 [35000/39785 (1%)]\tLoss: 362.076813\n",
      "Train Epoch: 1 [36000/39785 (1%)]\tLoss: 466.152374\n",
      "Train Epoch: 1 [37000/39785 (1%)]\tLoss: 363.427643\n",
      "Train Epoch: 1 [38000/39785 (1%)]\tLoss: 379.101440\n",
      "Train Epoch: 1 [39000/39785 (1%)]\tLoss: 366.372711\n",
      "39700\n",
      "39785\n",
      "Train Epoch: 2 [1000/39785 (0%)]\tLoss: 373.133301\n",
      "Train Epoch: 2 [2000/39785 (0%)]\tLoss: 425.483948\n",
      "Train Epoch: 2 [3000/39785 (0%)]\tLoss: 381.429382\n",
      "Train Epoch: 2 [4000/39785 (0%)]\tLoss: 405.510559\n",
      "Train Epoch: 2 [5000/39785 (0%)]\tLoss: 404.624878\n",
      "Train Epoch: 2 [6000/39785 (0%)]\tLoss: 362.764374\n",
      "Train Epoch: 2 [7000/39785 (0%)]\tLoss: 360.868927\n",
      "Train Epoch: 2 [8000/39785 (0%)]\tLoss: 328.055725\n",
      "Train Epoch: 2 [9000/39785 (0%)]\tLoss: 364.830261\n",
      "Train Epoch: 2 [10000/39785 (0%)]\tLoss: 395.013672\n",
      "Train Epoch: 2 [11000/39785 (0%)]\tLoss: 299.700409\n",
      "Train Epoch: 2 [12000/39785 (0%)]\tLoss: 344.179535\n",
      "Train Epoch: 2 [13000/39785 (0%)]\tLoss: 349.492310\n",
      "Train Epoch: 2 [14000/39785 (0%)]\tLoss: 406.132141\n",
      "Train Epoch: 2 [15000/39785 (0%)]\tLoss: 369.175201\n",
      "Train Epoch: 2 [16000/39785 (0%)]\tLoss: 385.865570\n",
      "Train Epoch: 2 [17000/39785 (0%)]\tLoss: 347.413635\n",
      "Train Epoch: 2 [18000/39785 (0%)]\tLoss: 371.663086\n",
      "Train Epoch: 2 [19000/39785 (0%)]\tLoss: 341.475433\n",
      "Train Epoch: 2 [20000/39785 (1%)]\tLoss: 303.618713\n",
      "Train Epoch: 2 [21000/39785 (1%)]\tLoss: 405.141510\n",
      "Train Epoch: 2 [22000/39785 (1%)]\tLoss: 336.218964\n",
      "Train Epoch: 2 [23000/39785 (1%)]\tLoss: 328.992188\n",
      "Train Epoch: 2 [24000/39785 (1%)]\tLoss: 360.944824\n",
      "Train Epoch: 2 [25000/39785 (1%)]\tLoss: 334.881012\n",
      "Train Epoch: 2 [26000/39785 (1%)]\tLoss: 337.808197\n",
      "Train Epoch: 2 [27000/39785 (1%)]\tLoss: 368.615814\n",
      "Train Epoch: 2 [28000/39785 (1%)]\tLoss: 291.533234\n",
      "Train Epoch: 2 [29000/39785 (1%)]\tLoss: 330.601593\n",
      "Train Epoch: 2 [30000/39785 (1%)]\tLoss: 469.786591\n",
      "Train Epoch: 2 [31000/39785 (1%)]\tLoss: 501.517456\n",
      "Train Epoch: 2 [32000/39785 (1%)]\tLoss: 326.504791\n",
      "Train Epoch: 2 [33000/39785 (1%)]\tLoss: 319.685425\n",
      "Train Epoch: 2 [34000/39785 (1%)]\tLoss: 339.767944\n",
      "Train Epoch: 2 [35000/39785 (1%)]\tLoss: 311.304260\n",
      "Train Epoch: 2 [36000/39785 (1%)]\tLoss: 287.793976\n",
      "Train Epoch: 2 [37000/39785 (1%)]\tLoss: 348.792664\n",
      "Train Epoch: 2 [38000/39785 (1%)]\tLoss: 338.446198\n",
      "Train Epoch: 2 [39000/39785 (1%)]\tLoss: 294.141510\n",
      "39715\n",
      "39785\n",
      "Train Epoch: 3 [1000/39785 (0%)]\tLoss: 316.870392\n",
      "Train Epoch: 3 [2000/39785 (0%)]\tLoss: 270.212341\n",
      "Train Epoch: 3 [3000/39785 (0%)]\tLoss: 282.261047\n",
      "Train Epoch: 3 [4000/39785 (0%)]\tLoss: 411.363708\n",
      "Train Epoch: 3 [5000/39785 (0%)]\tLoss: 314.312347\n",
      "Train Epoch: 3 [6000/39785 (0%)]\tLoss: 333.565704\n",
      "Train Epoch: 3 [7000/39785 (0%)]\tLoss: 278.240021\n",
      "Train Epoch: 3 [8000/39785 (0%)]\tLoss: 352.677216\n",
      "Train Epoch: 3 [9000/39785 (0%)]\tLoss: 289.879913\n",
      "Train Epoch: 3 [10000/39785 (0%)]\tLoss: 278.604218\n",
      "Train Epoch: 3 [11000/39785 (0%)]\tLoss: 296.492981\n",
      "Train Epoch: 3 [12000/39785 (0%)]\tLoss: 279.346161\n",
      "Train Epoch: 3 [13000/39785 (0%)]\tLoss: 292.275482\n",
      "Train Epoch: 3 [14000/39785 (0%)]\tLoss: 280.404633\n",
      "Train Epoch: 3 [15000/39785 (0%)]\tLoss: 333.741211\n",
      "Train Epoch: 3 [16000/39785 (0%)]\tLoss: 285.597412\n",
      "Train Epoch: 3 [17000/39785 (0%)]\tLoss: 259.183441\n",
      "Train Epoch: 3 [18000/39785 (0%)]\tLoss: 260.687653\n",
      "Train Epoch: 3 [19000/39785 (0%)]\tLoss: 380.583374\n",
      "Train Epoch: 3 [20000/39785 (1%)]\tLoss: 294.521698\n",
      "Train Epoch: 3 [21000/39785 (1%)]\tLoss: 335.027313\n",
      "Train Epoch: 3 [22000/39785 (1%)]\tLoss: 249.949707\n",
      "Train Epoch: 3 [23000/39785 (1%)]\tLoss: 281.514862\n",
      "Train Epoch: 3 [24000/39785 (1%)]\tLoss: 257.662415\n",
      "Train Epoch: 3 [25000/39785 (1%)]\tLoss: 338.321442\n",
      "Train Epoch: 3 [26000/39785 (1%)]\tLoss: 316.041626\n",
      "Train Epoch: 3 [27000/39785 (1%)]\tLoss: 266.406830\n",
      "Train Epoch: 3 [28000/39785 (1%)]\tLoss: 342.366028\n",
      "Train Epoch: 3 [29000/39785 (1%)]\tLoss: 258.859680\n",
      "Train Epoch: 3 [30000/39785 (1%)]\tLoss: 279.044800\n",
      "Train Epoch: 3 [31000/39785 (1%)]\tLoss: 284.550598\n",
      "Train Epoch: 3 [32000/39785 (1%)]\tLoss: 322.054718\n",
      "Train Epoch: 3 [33000/39785 (1%)]\tLoss: 325.543732\n",
      "Train Epoch: 3 [34000/39785 (1%)]\tLoss: 353.139252\n",
      "Train Epoch: 3 [35000/39785 (1%)]\tLoss: 306.309113\n",
      "Train Epoch: 3 [36000/39785 (1%)]\tLoss: 262.950806\n",
      "Train Epoch: 3 [37000/39785 (1%)]\tLoss: 256.766632\n",
      "Train Epoch: 3 [38000/39785 (1%)]\tLoss: 267.368988\n",
      "Train Epoch: 3 [39000/39785 (1%)]\tLoss: 171.918701\n",
      "39730\n",
      "39785\n",
      "Train Epoch: 4 [1000/39785 (0%)]\tLoss: 272.752502\n",
      "Train Epoch: 4 [2000/39785 (0%)]\tLoss: 233.141037\n",
      "Train Epoch: 4 [3000/39785 (0%)]\tLoss: 234.345337\n",
      "Train Epoch: 4 [4000/39785 (0%)]\tLoss: 199.220322\n",
      "Train Epoch: 4 [5000/39785 (0%)]\tLoss: 227.209961\n",
      "Train Epoch: 4 [6000/39785 (0%)]\tLoss: 250.912537\n",
      "Train Epoch: 4 [7000/39785 (0%)]\tLoss: 465.915558\n",
      "Train Epoch: 4 [8000/39785 (0%)]\tLoss: 308.197601\n",
      "Train Epoch: 4 [9000/39785 (0%)]\tLoss: 273.829407\n",
      "Train Epoch: 4 [10000/39785 (0%)]\tLoss: 246.045761\n",
      "Train Epoch: 4 [11000/39785 (0%)]\tLoss: 262.834961\n",
      "Train Epoch: 4 [12000/39785 (0%)]\tLoss: 248.500732\n",
      "Train Epoch: 4 [13000/39785 (0%)]\tLoss: 199.997101\n",
      "Train Epoch: 4 [14000/39785 (0%)]\tLoss: 269.756958\n",
      "Train Epoch: 4 [15000/39785 (0%)]\tLoss: 210.989182\n",
      "Train Epoch: 4 [16000/39785 (0%)]\tLoss: 247.296967\n",
      "Train Epoch: 4 [17000/39785 (0%)]\tLoss: 283.359497\n",
      "Train Epoch: 4 [18000/39785 (0%)]\tLoss: 273.472076\n",
      "Train Epoch: 4 [19000/39785 (0%)]\tLoss: 305.510468\n",
      "Train Epoch: 4 [20000/39785 (1%)]\tLoss: 253.763931\n",
      "Train Epoch: 4 [21000/39785 (1%)]\tLoss: 277.836029\n",
      "Train Epoch: 4 [22000/39785 (1%)]\tLoss: 234.083496\n",
      "Train Epoch: 4 [23000/39785 (1%)]\tLoss: 229.587708\n",
      "Train Epoch: 4 [24000/39785 (1%)]\tLoss: 250.768646\n",
      "Train Epoch: 4 [25000/39785 (1%)]\tLoss: 258.534393\n",
      "Train Epoch: 4 [26000/39785 (1%)]\tLoss: 243.993622\n",
      "Train Epoch: 4 [27000/39785 (1%)]\tLoss: 285.173004\n",
      "Train Epoch: 4 [28000/39785 (1%)]\tLoss: 210.731186\n",
      "Train Epoch: 4 [29000/39785 (1%)]\tLoss: 254.474396\n",
      "Train Epoch: 4 [30000/39785 (1%)]\tLoss: 265.300354\n",
      "Train Epoch: 4 [31000/39785 (1%)]\tLoss: 193.142319\n",
      "Train Epoch: 4 [32000/39785 (1%)]\tLoss: 298.063263\n",
      "Train Epoch: 4 [33000/39785 (1%)]\tLoss: 280.352142\n",
      "Train Epoch: 4 [34000/39785 (1%)]\tLoss: 273.968170\n",
      "Train Epoch: 4 [35000/39785 (1%)]\tLoss: 269.336151\n",
      "Train Epoch: 4 [36000/39785 (1%)]\tLoss: 249.001038\n",
      "Train Epoch: 4 [37000/39785 (1%)]\tLoss: 270.405121\n",
      "Train Epoch: 4 [38000/39785 (1%)]\tLoss: 232.573257\n",
      "Train Epoch: 4 [39000/39785 (1%)]\tLoss: 195.812042\n",
      "39745\n",
      "39785\n",
      "Train Epoch: 5 [1000/39785 (0%)]\tLoss: 208.323517\n",
      "Train Epoch: 5 [2000/39785 (0%)]\tLoss: 201.830231\n",
      "Train Epoch: 5 [3000/39785 (0%)]\tLoss: 216.887726\n",
      "Train Epoch: 5 [4000/39785 (0%)]\tLoss: 298.961151\n",
      "Train Epoch: 5 [5000/39785 (0%)]\tLoss: 194.217163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [6000/39785 (0%)]\tLoss: 267.314728\n",
      "Train Epoch: 5 [7000/39785 (0%)]\tLoss: 250.307434\n",
      "Train Epoch: 5 [8000/39785 (0%)]\tLoss: 281.507568\n",
      "Train Epoch: 5 [9000/39785 (0%)]\tLoss: 182.773804\n",
      "Train Epoch: 5 [10000/39785 (0%)]\tLoss: 257.191284\n",
      "Train Epoch: 5 [11000/39785 (0%)]\tLoss: 200.231445\n",
      "Train Epoch: 5 [12000/39785 (0%)]\tLoss: 229.285095\n",
      "Train Epoch: 5 [13000/39785 (0%)]\tLoss: 229.772919\n",
      "Train Epoch: 5 [14000/39785 (0%)]\tLoss: 212.128021\n",
      "Train Epoch: 5 [15000/39785 (0%)]\tLoss: 199.787247\n",
      "Train Epoch: 5 [16000/39785 (0%)]\tLoss: 271.689301\n",
      "Train Epoch: 5 [17000/39785 (0%)]\tLoss: 221.210098\n",
      "Train Epoch: 5 [18000/39785 (0%)]\tLoss: 274.165039\n",
      "Train Epoch: 5 [19000/39785 (0%)]\tLoss: 268.127228\n",
      "Train Epoch: 5 [20000/39785 (1%)]\tLoss: 301.598602\n",
      "Train Epoch: 5 [21000/39785 (1%)]\tLoss: 254.278824\n",
      "Train Epoch: 5 [22000/39785 (1%)]\tLoss: 271.156189\n",
      "Train Epoch: 5 [23000/39785 (1%)]\tLoss: 238.583755\n",
      "Train Epoch: 5 [24000/39785 (1%)]\tLoss: 270.412384\n",
      "Train Epoch: 5 [25000/39785 (1%)]\tLoss: 171.141373\n",
      "Train Epoch: 5 [26000/39785 (1%)]\tLoss: 285.815338\n",
      "Train Epoch: 5 [27000/39785 (1%)]\tLoss: 296.217194\n",
      "Train Epoch: 5 [28000/39785 (1%)]\tLoss: 264.005432\n",
      "Train Epoch: 5 [29000/39785 (1%)]\tLoss: 289.116821\n",
      "Train Epoch: 5 [30000/39785 (1%)]\tLoss: 258.763000\n",
      "Train Epoch: 5 [31000/39785 (1%)]\tLoss: 335.968781\n",
      "Train Epoch: 5 [32000/39785 (1%)]\tLoss: 226.553360\n",
      "Train Epoch: 5 [33000/39785 (1%)]\tLoss: 195.801315\n",
      "Train Epoch: 5 [34000/39785 (1%)]\tLoss: 197.727524\n",
      "Train Epoch: 5 [35000/39785 (1%)]\tLoss: 252.461700\n",
      "Train Epoch: 5 [36000/39785 (1%)]\tLoss: 255.407883\n",
      "Train Epoch: 5 [37000/39785 (1%)]\tLoss: 341.161987\n",
      "Train Epoch: 5 [38000/39785 (1%)]\tLoss: 287.979828\n",
      "Train Epoch: 5 [39000/39785 (1%)]\tLoss: 266.048035\n",
      "39760\n",
      "39785\n",
      "Train Epoch: 6 [1000/39785 (0%)]\tLoss: 263.800568\n",
      "Train Epoch: 6 [2000/39785 (0%)]\tLoss: 231.357697\n",
      "Train Epoch: 6 [3000/39785 (0%)]\tLoss: 267.711517\n",
      "Train Epoch: 6 [4000/39785 (0%)]\tLoss: 204.704285\n",
      "Train Epoch: 6 [5000/39785 (0%)]\tLoss: 238.467972\n",
      "Train Epoch: 6 [6000/39785 (0%)]\tLoss: 202.053879\n",
      "Train Epoch: 6 [7000/39785 (0%)]\tLoss: 183.699051\n",
      "Train Epoch: 6 [8000/39785 (0%)]\tLoss: 236.372910\n",
      "Train Epoch: 6 [9000/39785 (0%)]\tLoss: 167.631424\n",
      "Train Epoch: 6 [10000/39785 (0%)]\tLoss: 236.838272\n",
      "Train Epoch: 6 [11000/39785 (0%)]\tLoss: 264.322998\n",
      "Train Epoch: 6 [12000/39785 (0%)]\tLoss: 239.833572\n",
      "Train Epoch: 6 [13000/39785 (0%)]\tLoss: 232.058334\n",
      "Train Epoch: 6 [14000/39785 (0%)]\tLoss: 220.763763\n",
      "Train Epoch: 6 [15000/39785 (0%)]\tLoss: 277.308777\n",
      "Train Epoch: 6 [16000/39785 (0%)]\tLoss: 149.465073\n",
      "Train Epoch: 6 [17000/39785 (0%)]\tLoss: 175.793686\n",
      "Train Epoch: 6 [18000/39785 (0%)]\tLoss: 183.692398\n",
      "Train Epoch: 6 [19000/39785 (0%)]\tLoss: 153.945770\n",
      "Train Epoch: 6 [20000/39785 (1%)]\tLoss: 231.405640\n",
      "Train Epoch: 6 [21000/39785 (1%)]\tLoss: 213.292877\n",
      "Train Epoch: 6 [22000/39785 (1%)]\tLoss: 237.406693\n",
      "Train Epoch: 6 [23000/39785 (1%)]\tLoss: 194.084061\n",
      "Train Epoch: 6 [24000/39785 (1%)]\tLoss: 238.880768\n",
      "Train Epoch: 6 [25000/39785 (1%)]\tLoss: 223.505661\n",
      "Train Epoch: 6 [26000/39785 (1%)]\tLoss: 286.673676\n",
      "Train Epoch: 6 [27000/39785 (1%)]\tLoss: 226.010010\n",
      "Train Epoch: 6 [28000/39785 (1%)]\tLoss: 231.456696\n",
      "Train Epoch: 6 [29000/39785 (1%)]\tLoss: 207.397568\n",
      "Train Epoch: 6 [30000/39785 (1%)]\tLoss: 166.633224\n",
      "Train Epoch: 6 [31000/39785 (1%)]\tLoss: 295.817413\n",
      "Train Epoch: 6 [32000/39785 (1%)]\tLoss: 269.557312\n",
      "Train Epoch: 6 [33000/39785 (1%)]\tLoss: 221.859222\n",
      "Train Epoch: 6 [34000/39785 (1%)]\tLoss: 175.779785\n",
      "Train Epoch: 6 [35000/39785 (1%)]\tLoss: 226.286362\n",
      "Train Epoch: 6 [36000/39785 (1%)]\tLoss: 307.028320\n",
      "Train Epoch: 6 [37000/39785 (1%)]\tLoss: 331.548950\n",
      "Train Epoch: 6 [38000/39785 (1%)]\tLoss: 327.751221\n",
      "Train Epoch: 6 [39000/39785 (1%)]\tLoss: 311.501831\n",
      "39775\n",
      "39785\n",
      "Train Epoch: 7 [1000/39785 (0%)]\tLoss: 302.838654\n",
      "Train Epoch: 7 [2000/39785 (0%)]\tLoss: 190.187134\n",
      "Train Epoch: 7 [3000/39785 (0%)]\tLoss: 162.256699\n",
      "Train Epoch: 7 [4000/39785 (0%)]\tLoss: 495.794250\n",
      "Train Epoch: 7 [5000/39785 (0%)]\tLoss: 182.563705\n",
      "Train Epoch: 7 [6000/39785 (0%)]\tLoss: 209.299438\n",
      "Train Epoch: 7 [7000/39785 (0%)]\tLoss: 216.529419\n",
      "Train Epoch: 7 [8000/39785 (0%)]\tLoss: 213.502701\n",
      "Train Epoch: 7 [9000/39785 (0%)]\tLoss: 198.528061\n",
      "Train Epoch: 7 [10000/39785 (0%)]\tLoss: 275.372162\n",
      "Train Epoch: 7 [11000/39785 (0%)]\tLoss: 211.514252\n",
      "Train Epoch: 7 [12000/39785 (0%)]\tLoss: 172.328201\n",
      "Train Epoch: 7 [13000/39785 (0%)]\tLoss: 184.192688\n",
      "Train Epoch: 7 [14000/39785 (0%)]\tLoss: 308.091766\n",
      "Train Epoch: 7 [15000/39785 (0%)]\tLoss: 172.239334\n",
      "Train Epoch: 7 [16000/39785 (0%)]\tLoss: 158.121887\n",
      "Train Epoch: 7 [17000/39785 (0%)]\tLoss: 223.047531\n",
      "Train Epoch: 7 [18000/39785 (0%)]\tLoss: 227.314392\n",
      "Train Epoch: 7 [19000/39785 (0%)]\tLoss: 270.182892\n",
      "Train Epoch: 7 [20000/39785 (1%)]\tLoss: 181.042130\n",
      "Train Epoch: 7 [21000/39785 (1%)]\tLoss: 239.456635\n",
      "Train Epoch: 7 [22000/39785 (1%)]\tLoss: 168.487335\n",
      "Train Epoch: 7 [23000/39785 (1%)]\tLoss: 182.808945\n",
      "Train Epoch: 7 [24000/39785 (1%)]\tLoss: 192.427444\n",
      "Train Epoch: 7 [25000/39785 (1%)]\tLoss: 252.307770\n",
      "Train Epoch: 7 [26000/39785 (1%)]\tLoss: 200.761108\n",
      "Train Epoch: 7 [27000/39785 (1%)]\tLoss: 213.576279\n",
      "Train Epoch: 7 [28000/39785 (1%)]\tLoss: 221.767838\n",
      "Train Epoch: 7 [29000/39785 (1%)]\tLoss: 221.097778\n",
      "Train Epoch: 7 [30000/39785 (1%)]\tLoss: 234.046494\n",
      "Train Epoch: 7 [31000/39785 (1%)]\tLoss: 167.306320\n",
      "Train Epoch: 7 [32000/39785 (1%)]\tLoss: 212.412384\n",
      "Train Epoch: 7 [33000/39785 (1%)]\tLoss: 181.329895\n",
      "Train Epoch: 7 [34000/39785 (1%)]\tLoss: 181.027084\n",
      "Train Epoch: 7 [35000/39785 (1%)]\tLoss: 144.770279\n",
      "Train Epoch: 7 [36000/39785 (1%)]\tLoss: 239.940018\n",
      "Train Epoch: 7 [37000/39785 (1%)]\tLoss: 226.200668\n",
      "Train Epoch: 7 [38000/39785 (1%)]\tLoss: 275.451050\n",
      "Train Epoch: 7 [39000/39785 (1%)]\tLoss: 197.696213\n",
      "39690\n",
      "39785\n",
      "Train Epoch: 8 [1000/39785 (0%)]\tLoss: 148.969437\n",
      "Train Epoch: 8 [2000/39785 (0%)]\tLoss: 207.253403\n",
      "Train Epoch: 8 [3000/39785 (0%)]\tLoss: 151.632401\n",
      "Train Epoch: 8 [4000/39785 (0%)]\tLoss: 136.538239\n",
      "Train Epoch: 8 [5000/39785 (0%)]\tLoss: 146.508438\n",
      "Train Epoch: 8 [6000/39785 (0%)]\tLoss: 178.905167\n",
      "Train Epoch: 8 [7000/39785 (0%)]\tLoss: 227.331055\n",
      "Train Epoch: 8 [8000/39785 (0%)]\tLoss: 183.494949\n",
      "Train Epoch: 8 [9000/39785 (0%)]\tLoss: 145.746811\n",
      "Train Epoch: 8 [10000/39785 (0%)]\tLoss: 164.911667\n",
      "Train Epoch: 8 [11000/39785 (0%)]\tLoss: 194.936813\n",
      "Train Epoch: 8 [12000/39785 (0%)]\tLoss: 166.797989\n",
      "Train Epoch: 8 [13000/39785 (0%)]\tLoss: 310.727020\n",
      "Train Epoch: 8 [14000/39785 (0%)]\tLoss: 174.815811\n",
      "Train Epoch: 8 [15000/39785 (0%)]\tLoss: 247.253876\n",
      "Train Epoch: 8 [16000/39785 (0%)]\tLoss: 262.578217\n",
      "Train Epoch: 8 [17000/39785 (0%)]\tLoss: 250.863892\n",
      "Train Epoch: 8 [18000/39785 (0%)]\tLoss: 214.875305\n",
      "Train Epoch: 8 [19000/39785 (0%)]\tLoss: 182.387283\n",
      "Train Epoch: 8 [20000/39785 (1%)]\tLoss: 162.222168\n",
      "Train Epoch: 8 [21000/39785 (1%)]\tLoss: 185.984848\n",
      "Train Epoch: 8 [22000/39785 (1%)]\tLoss: 178.189987\n",
      "Train Epoch: 8 [23000/39785 (1%)]\tLoss: 224.184052\n",
      "Train Epoch: 8 [24000/39785 (1%)]\tLoss: 228.403229\n",
      "Train Epoch: 8 [25000/39785 (1%)]\tLoss: 193.338257\n",
      "Train Epoch: 8 [26000/39785 (1%)]\tLoss: 240.927536\n",
      "Train Epoch: 8 [27000/39785 (1%)]\tLoss: 242.149551\n",
      "Train Epoch: 8 [28000/39785 (1%)]\tLoss: 178.824097\n",
      "Train Epoch: 8 [29000/39785 (1%)]\tLoss: 153.323349\n",
      "Train Epoch: 8 [30000/39785 (1%)]\tLoss: 206.422195\n",
      "Train Epoch: 8 [31000/39785 (1%)]\tLoss: 191.701828\n",
      "Train Epoch: 8 [32000/39785 (1%)]\tLoss: 214.426285\n",
      "Train Epoch: 8 [33000/39785 (1%)]\tLoss: 196.492615\n",
      "Train Epoch: 8 [34000/39785 (1%)]\tLoss: 209.156418\n",
      "Train Epoch: 8 [35000/39785 (1%)]\tLoss: 154.070480\n",
      "Train Epoch: 8 [36000/39785 (1%)]\tLoss: 301.522308\n",
      "Train Epoch: 8 [37000/39785 (1%)]\tLoss: 261.089844\n",
      "Train Epoch: 8 [38000/39785 (1%)]\tLoss: 214.429153\n",
      "Train Epoch: 8 [39000/39785 (1%)]\tLoss: 246.100494\n",
      "39705\n",
      "39785\n",
      "Train Epoch: 9 [1000/39785 (0%)]\tLoss: 214.725769\n",
      "Train Epoch: 9 [2000/39785 (0%)]\tLoss: 231.771149\n",
      "Train Epoch: 9 [3000/39785 (0%)]\tLoss: 283.021454\n",
      "Train Epoch: 9 [4000/39785 (0%)]\tLoss: 167.552567\n",
      "Train Epoch: 9 [5000/39785 (0%)]\tLoss: 151.115280\n",
      "Train Epoch: 9 [6000/39785 (0%)]\tLoss: 130.986206\n",
      "Train Epoch: 9 [7000/39785 (0%)]\tLoss: 160.879303\n",
      "Train Epoch: 9 [8000/39785 (0%)]\tLoss: 217.411621\n",
      "Train Epoch: 9 [9000/39785 (0%)]\tLoss: 179.991440\n",
      "Train Epoch: 9 [10000/39785 (0%)]\tLoss: 202.476959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [11000/39785 (0%)]\tLoss: 159.807724\n",
      "Train Epoch: 9 [12000/39785 (0%)]\tLoss: 203.979172\n",
      "Train Epoch: 9 [13000/39785 (0%)]\tLoss: 256.048859\n",
      "Train Epoch: 9 [14000/39785 (0%)]\tLoss: 234.559647\n",
      "Train Epoch: 9 [15000/39785 (0%)]\tLoss: 132.976486\n",
      "Train Epoch: 9 [16000/39785 (0%)]\tLoss: 133.622467\n",
      "Train Epoch: 9 [17000/39785 (0%)]\tLoss: 250.525925\n",
      "Train Epoch: 9 [18000/39785 (0%)]\tLoss: 147.256393\n",
      "Train Epoch: 9 [19000/39785 (0%)]\tLoss: 201.704956\n",
      "Train Epoch: 9 [20000/39785 (1%)]\tLoss: 182.743790\n",
      "Train Epoch: 9 [21000/39785 (1%)]\tLoss: 169.760818\n",
      "Train Epoch: 9 [22000/39785 (1%)]\tLoss: 213.394302\n",
      "Train Epoch: 9 [23000/39785 (1%)]\tLoss: 196.216064\n",
      "Train Epoch: 9 [24000/39785 (1%)]\tLoss: 182.808990\n",
      "Train Epoch: 9 [25000/39785 (1%)]\tLoss: 180.911133\n",
      "Train Epoch: 9 [26000/39785 (1%)]\tLoss: 185.825043\n",
      "Train Epoch: 9 [27000/39785 (1%)]\tLoss: 224.164673\n",
      "Train Epoch: 9 [28000/39785 (1%)]\tLoss: 137.947769\n",
      "Train Epoch: 9 [29000/39785 (1%)]\tLoss: 258.190155\n",
      "Train Epoch: 9 [30000/39785 (1%)]\tLoss: 200.913788\n",
      "Train Epoch: 9 [31000/39785 (1%)]\tLoss: 196.239487\n",
      "Train Epoch: 9 [32000/39785 (1%)]\tLoss: 207.063309\n",
      "Train Epoch: 9 [33000/39785 (1%)]\tLoss: 162.872055\n",
      "Train Epoch: 9 [34000/39785 (1%)]\tLoss: 217.594345\n",
      "Train Epoch: 9 [35000/39785 (1%)]\tLoss: 176.365646\n",
      "Train Epoch: 9 [36000/39785 (1%)]\tLoss: 258.563141\n",
      "Train Epoch: 9 [37000/39785 (1%)]\tLoss: 205.594925\n",
      "Train Epoch: 9 [38000/39785 (1%)]\tLoss: 164.951828\n",
      "Train Epoch: 9 [39000/39785 (1%)]\tLoss: 147.557068\n",
      "39720\n",
      "39785\n",
      "Train Epoch: 10 [1000/39785 (0%)]\tLoss: 156.747986\n",
      "Train Epoch: 10 [2000/39785 (0%)]\tLoss: 144.986832\n",
      "Train Epoch: 10 [3000/39785 (0%)]\tLoss: 237.510803\n",
      "Train Epoch: 10 [4000/39785 (0%)]\tLoss: 186.856781\n",
      "Train Epoch: 10 [5000/39785 (0%)]\tLoss: 222.245712\n",
      "Train Epoch: 10 [6000/39785 (0%)]\tLoss: 122.862068\n",
      "Train Epoch: 10 [7000/39785 (0%)]\tLoss: 196.174591\n",
      "Train Epoch: 10 [8000/39785 (0%)]\tLoss: 261.163300\n",
      "Train Epoch: 10 [9000/39785 (0%)]\tLoss: 176.698425\n",
      "Train Epoch: 10 [10000/39785 (0%)]\tLoss: 227.805908\n",
      "Train Epoch: 10 [11000/39785 (0%)]\tLoss: 159.263123\n",
      "Train Epoch: 10 [12000/39785 (0%)]\tLoss: 138.509705\n",
      "Train Epoch: 10 [13000/39785 (0%)]\tLoss: 214.354065\n",
      "Train Epoch: 10 [14000/39785 (0%)]\tLoss: 185.748703\n",
      "Train Epoch: 10 [15000/39785 (0%)]\tLoss: 147.103424\n",
      "Train Epoch: 10 [16000/39785 (0%)]\tLoss: 134.028351\n",
      "Train Epoch: 10 [17000/39785 (0%)]\tLoss: 189.388565\n",
      "Train Epoch: 10 [18000/39785 (0%)]\tLoss: 181.052872\n",
      "Train Epoch: 10 [19000/39785 (0%)]\tLoss: 141.033112\n",
      "Train Epoch: 10 [20000/39785 (1%)]\tLoss: 131.952911\n",
      "Train Epoch: 10 [21000/39785 (1%)]\tLoss: 157.408676\n",
      "Train Epoch: 10 [22000/39785 (1%)]\tLoss: 222.304962\n",
      "Train Epoch: 10 [23000/39785 (1%)]\tLoss: 207.045334\n",
      "Train Epoch: 10 [24000/39785 (1%)]\tLoss: 177.712799\n",
      "Train Epoch: 10 [25000/39785 (1%)]\tLoss: 208.411972\n",
      "Train Epoch: 10 [26000/39785 (1%)]\tLoss: 181.510330\n",
      "Train Epoch: 10 [27000/39785 (1%)]\tLoss: 177.166458\n",
      "Train Epoch: 10 [28000/39785 (1%)]\tLoss: 165.944763\n",
      "Train Epoch: 10 [29000/39785 (1%)]\tLoss: 176.857513\n",
      "Train Epoch: 10 [30000/39785 (1%)]\tLoss: 192.744034\n",
      "Train Epoch: 10 [31000/39785 (1%)]\tLoss: 169.394638\n",
      "Train Epoch: 10 [32000/39785 (1%)]\tLoss: 191.452225\n",
      "Train Epoch: 10 [33000/39785 (1%)]\tLoss: 172.984375\n",
      "Train Epoch: 10 [34000/39785 (1%)]\tLoss: 230.592651\n",
      "Train Epoch: 10 [35000/39785 (1%)]\tLoss: 189.556427\n",
      "Train Epoch: 10 [36000/39785 (1%)]\tLoss: 163.990158\n",
      "Train Epoch: 10 [37000/39785 (1%)]\tLoss: 146.303436\n",
      "Train Epoch: 10 [38000/39785 (1%)]\tLoss: 187.130905\n",
      "Train Epoch: 10 [39000/39785 (1%)]\tLoss: 204.112976\n",
      "39735\n",
      "39785\n",
      "Train Epoch: 11 [1000/39785 (0%)]\tLoss: 148.713928\n",
      "Train Epoch: 11 [2000/39785 (0%)]\tLoss: 204.431473\n",
      "Train Epoch: 11 [3000/39785 (0%)]\tLoss: 220.582047\n",
      "Train Epoch: 11 [4000/39785 (0%)]\tLoss: 160.419342\n",
      "Train Epoch: 11 [5000/39785 (0%)]\tLoss: 174.637466\n",
      "Train Epoch: 11 [6000/39785 (0%)]\tLoss: 184.049042\n",
      "Train Epoch: 11 [7000/39785 (0%)]\tLoss: 175.898727\n",
      "Train Epoch: 11 [8000/39785 (0%)]\tLoss: 133.698105\n",
      "Train Epoch: 11 [9000/39785 (0%)]\tLoss: 178.938995\n",
      "Train Epoch: 11 [10000/39785 (0%)]\tLoss: 173.863510\n",
      "Train Epoch: 11 [11000/39785 (0%)]\tLoss: 205.096313\n",
      "Train Epoch: 11 [12000/39785 (0%)]\tLoss: 173.613586\n",
      "Train Epoch: 11 [13000/39785 (0%)]\tLoss: 179.921616\n",
      "Train Epoch: 11 [14000/39785 (0%)]\tLoss: 135.932739\n",
      "Train Epoch: 11 [15000/39785 (0%)]\tLoss: 171.500366\n",
      "Train Epoch: 11 [16000/39785 (0%)]\tLoss: 173.691055\n",
      "Train Epoch: 11 [17000/39785 (0%)]\tLoss: 189.298691\n",
      "Train Epoch: 11 [18000/39785 (0%)]\tLoss: 193.725159\n",
      "Train Epoch: 11 [19000/39785 (0%)]\tLoss: 138.608292\n",
      "Train Epoch: 11 [20000/39785 (1%)]\tLoss: 214.864761\n",
      "Train Epoch: 11 [21000/39785 (1%)]\tLoss: 184.035858\n",
      "Train Epoch: 11 [22000/39785 (1%)]\tLoss: 189.560516\n",
      "Train Epoch: 11 [23000/39785 (1%)]\tLoss: 208.757721\n",
      "Train Epoch: 11 [24000/39785 (1%)]\tLoss: 159.328369\n",
      "Train Epoch: 11 [25000/39785 (1%)]\tLoss: 166.038101\n",
      "Train Epoch: 11 [26000/39785 (1%)]\tLoss: 157.509003\n",
      "Train Epoch: 11 [27000/39785 (1%)]\tLoss: 140.805252\n",
      "Train Epoch: 11 [28000/39785 (1%)]\tLoss: 136.942093\n",
      "Train Epoch: 11 [29000/39785 (1%)]\tLoss: 139.097122\n",
      "Train Epoch: 11 [30000/39785 (1%)]\tLoss: 155.823853\n",
      "Train Epoch: 11 [31000/39785 (1%)]\tLoss: 132.457153\n",
      "Train Epoch: 11 [32000/39785 (1%)]\tLoss: 150.167038\n",
      "Train Epoch: 11 [33000/39785 (1%)]\tLoss: 222.322479\n",
      "Train Epoch: 11 [34000/39785 (1%)]\tLoss: 228.558426\n",
      "Train Epoch: 11 [35000/39785 (1%)]\tLoss: 180.689362\n",
      "Train Epoch: 11 [36000/39785 (1%)]\tLoss: 216.550354\n",
      "Train Epoch: 11 [37000/39785 (1%)]\tLoss: 348.592194\n",
      "Train Epoch: 11 [38000/39785 (1%)]\tLoss: 219.962677\n",
      "Train Epoch: 11 [39000/39785 (1%)]\tLoss: 206.382996\n",
      "39750\n",
      "39785\n",
      "Train Epoch: 12 [1000/39785 (0%)]\tLoss: 165.176178\n",
      "Train Epoch: 12 [2000/39785 (0%)]\tLoss: 144.636108\n",
      "Train Epoch: 12 [3000/39785 (0%)]\tLoss: 119.619553\n",
      "Train Epoch: 12 [4000/39785 (0%)]\tLoss: 169.418457\n",
      "Train Epoch: 12 [5000/39785 (0%)]\tLoss: 281.774414\n",
      "Train Epoch: 12 [6000/39785 (0%)]\tLoss: 158.440033\n",
      "Train Epoch: 12 [7000/39785 (0%)]\tLoss: 200.181213\n",
      "Train Epoch: 12 [8000/39785 (0%)]\tLoss: 150.123840\n",
      "Train Epoch: 12 [9000/39785 (0%)]\tLoss: 155.236450\n",
      "Train Epoch: 12 [10000/39785 (0%)]\tLoss: 129.489197\n",
      "Train Epoch: 12 [11000/39785 (0%)]\tLoss: 130.818298\n",
      "Train Epoch: 12 [12000/39785 (0%)]\tLoss: 115.732971\n",
      "Train Epoch: 12 [13000/39785 (0%)]\tLoss: 171.341904\n",
      "Train Epoch: 12 [14000/39785 (0%)]\tLoss: 139.046570\n",
      "Train Epoch: 12 [15000/39785 (0%)]\tLoss: 146.131058\n",
      "Train Epoch: 12 [16000/39785 (0%)]\tLoss: 115.127205\n",
      "Train Epoch: 12 [17000/39785 (0%)]\tLoss: 140.024155\n",
      "Train Epoch: 12 [18000/39785 (0%)]\tLoss: 145.112610\n",
      "Train Epoch: 12 [19000/39785 (0%)]\tLoss: 218.613876\n",
      "Train Epoch: 12 [20000/39785 (1%)]\tLoss: 167.818909\n",
      "Train Epoch: 12 [21000/39785 (1%)]\tLoss: 166.058914\n",
      "Train Epoch: 12 [22000/39785 (1%)]\tLoss: 200.884567\n",
      "Train Epoch: 12 [23000/39785 (1%)]\tLoss: 127.194771\n",
      "Train Epoch: 12 [24000/39785 (1%)]\tLoss: 131.837952\n",
      "Train Epoch: 12 [25000/39785 (1%)]\tLoss: 159.538452\n",
      "Train Epoch: 12 [26000/39785 (1%)]\tLoss: 113.382393\n",
      "Train Epoch: 12 [27000/39785 (1%)]\tLoss: 184.769730\n",
      "Train Epoch: 12 [28000/39785 (1%)]\tLoss: 269.131073\n",
      "Train Epoch: 12 [29000/39785 (1%)]\tLoss: 420.088104\n",
      "Train Epoch: 12 [30000/39785 (1%)]\tLoss: 302.933624\n",
      "Train Epoch: 12 [31000/39785 (1%)]\tLoss: 299.044342\n",
      "Train Epoch: 12 [32000/39785 (1%)]\tLoss: 272.777405\n",
      "Train Epoch: 12 [33000/39785 (1%)]\tLoss: 238.378738\n",
      "Train Epoch: 12 [34000/39785 (1%)]\tLoss: 180.049850\n",
      "Train Epoch: 12 [35000/39785 (1%)]\tLoss: 171.318253\n",
      "Train Epoch: 12 [36000/39785 (1%)]\tLoss: 176.913651\n",
      "Train Epoch: 12 [37000/39785 (1%)]\tLoss: 150.993362\n",
      "Train Epoch: 12 [38000/39785 (1%)]\tLoss: 221.238083\n",
      "Train Epoch: 12 [39000/39785 (1%)]\tLoss: 289.645172\n",
      "39765\n",
      "39785\n",
      "Train Epoch: 13 [1000/39785 (0%)]\tLoss: 121.303268\n",
      "Train Epoch: 13 [2000/39785 (0%)]\tLoss: 139.365250\n",
      "Train Epoch: 13 [3000/39785 (0%)]\tLoss: 255.885727\n",
      "Train Epoch: 13 [4000/39785 (0%)]\tLoss: 288.075806\n",
      "Train Epoch: 13 [5000/39785 (0%)]\tLoss: 131.974869\n",
      "Train Epoch: 13 [6000/39785 (0%)]\tLoss: 176.902939\n",
      "Train Epoch: 13 [7000/39785 (0%)]\tLoss: 182.111755\n",
      "Train Epoch: 13 [8000/39785 (0%)]\tLoss: 151.403656\n",
      "Train Epoch: 13 [9000/39785 (0%)]\tLoss: 121.689301\n",
      "Train Epoch: 13 [10000/39785 (0%)]\tLoss: 167.341736\n",
      "Train Epoch: 13 [11000/39785 (0%)]\tLoss: 158.681366\n",
      "Train Epoch: 13 [12000/39785 (0%)]\tLoss: 152.022049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [13000/39785 (0%)]\tLoss: 106.710693\n",
      "Train Epoch: 13 [14000/39785 (0%)]\tLoss: 134.318665\n",
      "Train Epoch: 13 [15000/39785 (0%)]\tLoss: 139.144791\n",
      "Train Epoch: 13 [16000/39785 (0%)]\tLoss: 133.970505\n",
      "Train Epoch: 13 [17000/39785 (0%)]\tLoss: 160.414810\n",
      "Train Epoch: 13 [18000/39785 (0%)]\tLoss: 145.912033\n",
      "Train Epoch: 13 [19000/39785 (0%)]\tLoss: 104.092735\n",
      "Train Epoch: 13 [20000/39785 (1%)]\tLoss: 200.826721\n",
      "Train Epoch: 13 [21000/39785 (1%)]\tLoss: 155.518921\n",
      "Train Epoch: 13 [22000/39785 (1%)]\tLoss: 137.370850\n",
      "Train Epoch: 13 [23000/39785 (1%)]\tLoss: 159.411438\n",
      "Train Epoch: 13 [24000/39785 (1%)]\tLoss: 1535.654663\n",
      "Train Epoch: 13 [25000/39785 (1%)]\tLoss: 1012.830933\n",
      "Train Epoch: 13 [26000/39785 (1%)]\tLoss: 5798.372070\n",
      "Train Epoch: 13 [27000/39785 (1%)]\tLoss: 750.386230\n",
      "Train Epoch: 13 [28000/39785 (1%)]\tLoss: 436.498260\n",
      "Train Epoch: 13 [29000/39785 (1%)]\tLoss: 425.761597\n",
      "Train Epoch: 13 [30000/39785 (1%)]\tLoss: 410.461060\n",
      "Train Epoch: 13 [31000/39785 (1%)]\tLoss: 365.869720\n",
      "Train Epoch: 13 [32000/39785 (1%)]\tLoss: 256.866272\n",
      "Train Epoch: 13 [33000/39785 (1%)]\tLoss: 342.960815\n",
      "Train Epoch: 13 [34000/39785 (1%)]\tLoss: 258.756287\n",
      "Train Epoch: 13 [35000/39785 (1%)]\tLoss: 250.893463\n",
      "Train Epoch: 13 [36000/39785 (1%)]\tLoss: 213.257172\n",
      "Train Epoch: 13 [37000/39785 (1%)]\tLoss: 246.573059\n",
      "Train Epoch: 13 [38000/39785 (1%)]\tLoss: 257.026398\n",
      "Train Epoch: 13 [39000/39785 (1%)]\tLoss: 258.547760\n",
      "39780\n",
      "39785\n",
      "Train Epoch: 14 [1000/39785 (0%)]\tLoss: 842.447571\n",
      "Train Epoch: 14 [2000/39785 (0%)]\tLoss: 9190.779297\n",
      "Train Epoch: 14 [3000/39785 (0%)]\tLoss: 1164.495239\n",
      "Train Epoch: 14 [4000/39785 (0%)]\tLoss: 703.057922\n",
      "Train Epoch: 14 [5000/39785 (0%)]\tLoss: 868.481018\n",
      "Train Epoch: 14 [6000/39785 (0%)]\tLoss: 738.331421\n",
      "Train Epoch: 14 [7000/39785 (0%)]\tLoss: 510.548157\n",
      "Train Epoch: 14 [8000/39785 (0%)]\tLoss: 553.758911\n",
      "Train Epoch: 14 [9000/39785 (0%)]\tLoss: 465.566681\n",
      "Train Epoch: 14 [10000/39785 (0%)]\tLoss: 524.275574\n",
      "Train Epoch: 14 [11000/39785 (0%)]\tLoss: 421.862518\n",
      "Train Epoch: 14 [12000/39785 (0%)]\tLoss: 437.805206\n",
      "Train Epoch: 14 [13000/39785 (0%)]\tLoss: 389.646759\n",
      "Train Epoch: 14 [14000/39785 (0%)]\tLoss: 541.741394\n",
      "Train Epoch: 14 [15000/39785 (0%)]\tLoss: 484.275238\n",
      "Train Epoch: 14 [16000/39785 (0%)]\tLoss: 367.468231\n",
      "Train Epoch: 14 [17000/39785 (0%)]\tLoss: 519.649231\n",
      "Train Epoch: 14 [18000/39785 (0%)]\tLoss: 328.996155\n",
      "Train Epoch: 14 [19000/39785 (0%)]\tLoss: 290.773834\n",
      "Train Epoch: 14 [20000/39785 (1%)]\tLoss: 284.464874\n",
      "Train Epoch: 14 [21000/39785 (1%)]\tLoss: 390.648102\n",
      "Train Epoch: 14 [22000/39785 (1%)]\tLoss: 324.877472\n",
      "Train Epoch: 14 [23000/39785 (1%)]\tLoss: 387.107788\n",
      "Train Epoch: 14 [24000/39785 (1%)]\tLoss: 360.706512\n",
      "Train Epoch: 14 [25000/39785 (1%)]\tLoss: 390.417816\n",
      "Train Epoch: 14 [26000/39785 (1%)]\tLoss: 567.196167\n",
      "Train Epoch: 14 [27000/39785 (1%)]\tLoss: 411.522369\n",
      "Train Epoch: 14 [28000/39785 (1%)]\tLoss: 386.725616\n",
      "Train Epoch: 14 [29000/39785 (1%)]\tLoss: 305.016998\n",
      "Train Epoch: 14 [30000/39785 (1%)]\tLoss: 353.020966\n",
      "Train Epoch: 14 [31000/39785 (1%)]\tLoss: 317.530304\n",
      "Train Epoch: 14 [32000/39785 (1%)]\tLoss: 359.245026\n",
      "Train Epoch: 14 [33000/39785 (1%)]\tLoss: 349.470459\n",
      "Train Epoch: 14 [34000/39785 (1%)]\tLoss: 293.489227\n",
      "Train Epoch: 14 [35000/39785 (1%)]\tLoss: 301.701721\n",
      "Train Epoch: 14 [36000/39785 (1%)]\tLoss: 257.165131\n",
      "Train Epoch: 14 [37000/39785 (1%)]\tLoss: 313.090546\n",
      "Train Epoch: 14 [38000/39785 (1%)]\tLoss: 252.077698\n",
      "Train Epoch: 14 [39000/39785 (1%)]\tLoss: 402.838623\n",
      "39695\n",
      "39785\n",
      "Train Epoch: 15 [1000/39785 (0%)]\tLoss: 191.767349\n",
      "Train Epoch: 15 [2000/39785 (0%)]\tLoss: 348.947174\n",
      "Train Epoch: 15 [3000/39785 (0%)]\tLoss: 224.994339\n",
      "Train Epoch: 15 [4000/39785 (0%)]\tLoss: 198.230728\n",
      "Train Epoch: 15 [5000/39785 (0%)]\tLoss: 232.214066\n",
      "Train Epoch: 15 [6000/39785 (0%)]\tLoss: 324.142181\n",
      "Train Epoch: 15 [7000/39785 (0%)]\tLoss: 222.405273\n",
      "Train Epoch: 15 [8000/39785 (0%)]\tLoss: 265.953491\n",
      "Train Epoch: 15 [9000/39785 (0%)]\tLoss: 347.234955\n",
      "Train Epoch: 15 [10000/39785 (0%)]\tLoss: 226.738770\n",
      "Train Epoch: 15 [11000/39785 (0%)]\tLoss: 262.553680\n",
      "Train Epoch: 15 [12000/39785 (0%)]\tLoss: 221.785934\n",
      "Train Epoch: 15 [13000/39785 (0%)]\tLoss: 261.955170\n",
      "Train Epoch: 15 [14000/39785 (0%)]\tLoss: 232.323929\n",
      "Train Epoch: 15 [15000/39785 (0%)]\tLoss: 292.329071\n",
      "Train Epoch: 15 [16000/39785 (0%)]\tLoss: 218.486237\n",
      "Train Epoch: 15 [17000/39785 (0%)]\tLoss: 256.027283\n",
      "Train Epoch: 15 [18000/39785 (0%)]\tLoss: 252.969971\n",
      "Train Epoch: 15 [19000/39785 (0%)]\tLoss: 291.078979\n",
      "Train Epoch: 15 [20000/39785 (1%)]\tLoss: 289.920502\n",
      "Train Epoch: 15 [21000/39785 (1%)]\tLoss: 286.349915\n",
      "Train Epoch: 15 [22000/39785 (1%)]\tLoss: 302.296967\n",
      "Train Epoch: 15 [23000/39785 (1%)]\tLoss: 222.439499\n",
      "Train Epoch: 15 [24000/39785 (1%)]\tLoss: 244.644638\n",
      "Train Epoch: 15 [25000/39785 (1%)]\tLoss: 293.329498\n",
      "Train Epoch: 15 [26000/39785 (1%)]\tLoss: 182.315125\n",
      "Train Epoch: 15 [27000/39785 (1%)]\tLoss: 250.388214\n",
      "Train Epoch: 15 [28000/39785 (1%)]\tLoss: 251.297836\n",
      "Train Epoch: 15 [29000/39785 (1%)]\tLoss: 5356.240234\n",
      "Train Epoch: 15 [30000/39785 (1%)]\tLoss: 3133.477783\n",
      "Train Epoch: 15 [31000/39785 (1%)]\tLoss: 1051.229370\n",
      "Train Epoch: 15 [32000/39785 (1%)]\tLoss: 800.526123\n",
      "Train Epoch: 15 [33000/39785 (1%)]\tLoss: 610.539856\n",
      "Train Epoch: 15 [34000/39785 (1%)]\tLoss: 480.001282\n",
      "Train Epoch: 15 [35000/39785 (1%)]\tLoss: 401.391327\n",
      "Train Epoch: 15 [36000/39785 (1%)]\tLoss: 395.440491\n",
      "Train Epoch: 15 [37000/39785 (1%)]\tLoss: 447.638214\n",
      "Train Epoch: 15 [38000/39785 (1%)]\tLoss: 306.164490\n",
      "Train Epoch: 15 [39000/39785 (1%)]\tLoss: 287.846893\n",
      "39710\n",
      "39785\n",
      "Train Epoch: 16 [1000/39785 (0%)]\tLoss: 281.395386\n",
      "Train Epoch: 16 [2000/39785 (0%)]\tLoss: 480.362671\n",
      "Train Epoch: 16 [3000/39785 (0%)]\tLoss: 437.126556\n",
      "Train Epoch: 16 [4000/39785 (0%)]\tLoss: 628.821472\n",
      "Train Epoch: 16 [5000/39785 (0%)]\tLoss: 361.501251\n",
      "Train Epoch: 16 [6000/39785 (0%)]\tLoss: 226.575333\n",
      "Train Epoch: 16 [7000/39785 (0%)]\tLoss: 283.000854\n",
      "Train Epoch: 16 [8000/39785 (0%)]\tLoss: 255.876846\n",
      "Train Epoch: 16 [9000/39785 (0%)]\tLoss: 246.060043\n",
      "Train Epoch: 16 [10000/39785 (0%)]\tLoss: 462.149506\n",
      "Train Epoch: 16 [11000/39785 (0%)]\tLoss: 339.911163\n",
      "Train Epoch: 16 [12000/39785 (0%)]\tLoss: 263.094299\n",
      "Train Epoch: 16 [13000/39785 (0%)]\tLoss: 288.788910\n",
      "Train Epoch: 16 [14000/39785 (0%)]\tLoss: 356.493744\n",
      "Train Epoch: 16 [15000/39785 (0%)]\tLoss: 189.483490\n",
      "Train Epoch: 16 [16000/39785 (0%)]\tLoss: 277.234528\n",
      "Train Epoch: 16 [17000/39785 (0%)]\tLoss: 240.689209\n",
      "Train Epoch: 16 [18000/39785 (0%)]\tLoss: 188.256393\n",
      "Train Epoch: 16 [19000/39785 (0%)]\tLoss: 291.643127\n",
      "Train Epoch: 16 [20000/39785 (1%)]\tLoss: 287.582794\n",
      "Train Epoch: 16 [21000/39785 (1%)]\tLoss: 243.081818\n",
      "Train Epoch: 16 [22000/39785 (1%)]\tLoss: 731.354614\n",
      "Train Epoch: 16 [23000/39785 (1%)]\tLoss: 373.485382\n",
      "Train Epoch: 16 [24000/39785 (1%)]\tLoss: 299.579865\n",
      "Train Epoch: 16 [25000/39785 (1%)]\tLoss: 239.135742\n",
      "Train Epoch: 16 [26000/39785 (1%)]\tLoss: 203.932663\n",
      "Train Epoch: 16 [27000/39785 (1%)]\tLoss: 204.868378\n",
      "Train Epoch: 16 [28000/39785 (1%)]\tLoss: 193.989731\n",
      "Train Epoch: 16 [29000/39785 (1%)]\tLoss: 246.263519\n",
      "Train Epoch: 16 [30000/39785 (1%)]\tLoss: 280.900360\n",
      "Train Epoch: 16 [31000/39785 (1%)]\tLoss: 411.867371\n",
      "Train Epoch: 16 [32000/39785 (1%)]\tLoss: 218.443817\n",
      "Train Epoch: 16 [33000/39785 (1%)]\tLoss: 184.863968\n",
      "Train Epoch: 16 [34000/39785 (1%)]\tLoss: 210.542480\n",
      "Train Epoch: 16 [35000/39785 (1%)]\tLoss: 187.342361\n",
      "Train Epoch: 16 [36000/39785 (1%)]\tLoss: 278.209686\n",
      "Train Epoch: 16 [37000/39785 (1%)]\tLoss: 243.984467\n",
      "Train Epoch: 16 [38000/39785 (1%)]\tLoss: 304.634338\n",
      "Train Epoch: 16 [39000/39785 (1%)]\tLoss: 269.896057\n",
      "39725\n",
      "39785\n",
      "Train Epoch: 17 [1000/39785 (0%)]\tLoss: 3665.122559\n",
      "Train Epoch: 17 [2000/39785 (0%)]\tLoss: 1183.149658\n",
      "Train Epoch: 17 [3000/39785 (0%)]\tLoss: 822.130676\n",
      "Train Epoch: 17 [4000/39785 (0%)]\tLoss: 690.165222\n",
      "Train Epoch: 17 [5000/39785 (0%)]\tLoss: 491.772583\n",
      "Train Epoch: 17 [6000/39785 (0%)]\tLoss: 381.899750\n",
      "Train Epoch: 17 [7000/39785 (0%)]\tLoss: 408.696533\n",
      "Train Epoch: 17 [8000/39785 (0%)]\tLoss: 368.030121\n",
      "Train Epoch: 17 [9000/39785 (0%)]\tLoss: 293.344604\n",
      "Train Epoch: 17 [10000/39785 (0%)]\tLoss: 345.813843\n",
      "Train Epoch: 17 [11000/39785 (0%)]\tLoss: 390.810822\n",
      "Train Epoch: 17 [12000/39785 (0%)]\tLoss: 350.321991\n",
      "Train Epoch: 17 [13000/39785 (0%)]\tLoss: 292.660492\n",
      "Train Epoch: 17 [14000/39785 (0%)]\tLoss: 427.906158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [15000/39785 (0%)]\tLoss: 305.975708\n",
      "Train Epoch: 17 [16000/39785 (0%)]\tLoss: 338.773560\n",
      "Train Epoch: 17 [17000/39785 (0%)]\tLoss: 365.518311\n",
      "Train Epoch: 17 [18000/39785 (0%)]\tLoss: 254.789993\n",
      "Train Epoch: 17 [19000/39785 (0%)]\tLoss: 391.823395\n",
      "Train Epoch: 17 [20000/39785 (1%)]\tLoss: 361.137756\n",
      "Train Epoch: 17 [21000/39785 (1%)]\tLoss: 238.595108\n",
      "Train Epoch: 17 [22000/39785 (1%)]\tLoss: 311.331635\n",
      "Train Epoch: 17 [23000/39785 (1%)]\tLoss: 269.847229\n",
      "Train Epoch: 17 [24000/39785 (1%)]\tLoss: 256.381744\n",
      "Train Epoch: 17 [25000/39785 (1%)]\tLoss: 238.524673\n",
      "Train Epoch: 17 [26000/39785 (1%)]\tLoss: 196.899612\n",
      "Train Epoch: 17 [27000/39785 (1%)]\tLoss: 305.801971\n",
      "Train Epoch: 17 [28000/39785 (1%)]\tLoss: 268.996948\n",
      "Train Epoch: 17 [29000/39785 (1%)]\tLoss: 13286.778320\n",
      "Train Epoch: 17 [30000/39785 (1%)]\tLoss: 1313.130127\n",
      "Train Epoch: 17 [31000/39785 (1%)]\tLoss: 1179.322876\n",
      "Train Epoch: 17 [32000/39785 (1%)]\tLoss: 782.432617\n",
      "Train Epoch: 17 [33000/39785 (1%)]\tLoss: 605.575012\n",
      "Train Epoch: 17 [34000/39785 (1%)]\tLoss: 521.162964\n",
      "Train Epoch: 17 [35000/39785 (1%)]\tLoss: 385.380493\n",
      "Train Epoch: 17 [36000/39785 (1%)]\tLoss: 418.665253\n",
      "Train Epoch: 17 [37000/39785 (1%)]\tLoss: 431.931946\n",
      "Train Epoch: 17 [38000/39785 (1%)]\tLoss: 450.378235\n",
      "Train Epoch: 17 [39000/39785 (1%)]\tLoss: 361.119141\n",
      "39740\n",
      "39785\n",
      "Train Epoch: 18 [1000/39785 (0%)]\tLoss: 325.444519\n",
      "Train Epoch: 18 [2000/39785 (0%)]\tLoss: 284.352997\n",
      "Train Epoch: 18 [3000/39785 (0%)]\tLoss: 286.142181\n",
      "Train Epoch: 18 [4000/39785 (0%)]\tLoss: 380.714081\n",
      "Train Epoch: 18 [5000/39785 (0%)]\tLoss: 274.201660\n",
      "Train Epoch: 18 [6000/39785 (0%)]\tLoss: 276.249756\n",
      "Train Epoch: 18 [7000/39785 (0%)]\tLoss: 328.944000\n",
      "Train Epoch: 18 [8000/39785 (0%)]\tLoss: 263.188293\n",
      "Train Epoch: 18 [9000/39785 (0%)]\tLoss: 55526.093750\n",
      "Train Epoch: 18 [10000/39785 (0%)]\tLoss: 3004.002686\n",
      "Train Epoch: 18 [11000/39785 (0%)]\tLoss: 3071.859863\n",
      "Train Epoch: 18 [12000/39785 (0%)]\tLoss: 1079.282104\n",
      "Train Epoch: 18 [13000/39785 (0%)]\tLoss: 854.202820\n",
      "Train Epoch: 18 [14000/39785 (0%)]\tLoss: 685.489014\n",
      "Train Epoch: 18 [15000/39785 (0%)]\tLoss: 724.813110\n",
      "Train Epoch: 18 [16000/39785 (0%)]\tLoss: 911.608582\n",
      "Train Epoch: 18 [17000/39785 (0%)]\tLoss: 648.094543\n",
      "Train Epoch: 18 [18000/39785 (0%)]\tLoss: 479.612640\n",
      "Train Epoch: 18 [19000/39785 (0%)]\tLoss: 502.122925\n",
      "Train Epoch: 18 [20000/39785 (1%)]\tLoss: 396.732574\n",
      "Train Epoch: 18 [21000/39785 (1%)]\tLoss: 358.245270\n",
      "Train Epoch: 18 [22000/39785 (1%)]\tLoss: 503.396790\n",
      "Train Epoch: 18 [23000/39785 (1%)]\tLoss: 448.938293\n",
      "Train Epoch: 18 [24000/39785 (1%)]\tLoss: 426.070465\n",
      "Train Epoch: 18 [25000/39785 (1%)]\tLoss: 334.272858\n",
      "Train Epoch: 18 [26000/39785 (1%)]\tLoss: 330.701813\n",
      "Train Epoch: 18 [27000/39785 (1%)]\tLoss: 334.357086\n",
      "Train Epoch: 18 [28000/39785 (1%)]\tLoss: 323.293091\n",
      "Train Epoch: 18 [29000/39785 (1%)]\tLoss: 421.364777\n",
      "Train Epoch: 18 [30000/39785 (1%)]\tLoss: 351.050812\n",
      "Train Epoch: 18 [31000/39785 (1%)]\tLoss: 293.255005\n",
      "Train Epoch: 18 [32000/39785 (1%)]\tLoss: 302.278107\n",
      "Train Epoch: 18 [33000/39785 (1%)]\tLoss: 285.820709\n",
      "Train Epoch: 18 [34000/39785 (1%)]\tLoss: 306.227234\n",
      "Train Epoch: 18 [35000/39785 (1%)]\tLoss: 392.354370\n",
      "Train Epoch: 18 [36000/39785 (1%)]\tLoss: 421.794617\n",
      "Train Epoch: 18 [37000/39785 (1%)]\tLoss: 288.702545\n",
      "Train Epoch: 18 [38000/39785 (1%)]\tLoss: 264.963318\n",
      "Train Epoch: 18 [39000/39785 (1%)]\tLoss: 221.828827\n",
      "39755\n",
      "39785\n",
      "Train Epoch: 19 [1000/39785 (0%)]\tLoss: 293.585632\n",
      "Train Epoch: 19 [2000/39785 (0%)]\tLoss: 235.296265\n",
      "Train Epoch: 19 [3000/39785 (0%)]\tLoss: 302.727356\n",
      "Train Epoch: 19 [4000/39785 (0%)]\tLoss: 220.303665\n",
      "Train Epoch: 19 [5000/39785 (0%)]\tLoss: 228.726181\n",
      "Train Epoch: 19 [6000/39785 (0%)]\tLoss: 224.548706\n",
      "Train Epoch: 19 [7000/39785 (0%)]\tLoss: 251.011154\n",
      "Train Epoch: 19 [8000/39785 (0%)]\tLoss: 226.837372\n",
      "Train Epoch: 19 [9000/39785 (0%)]\tLoss: 8817.190430\n",
      "Train Epoch: 19 [10000/39785 (0%)]\tLoss: 7143.634766\n",
      "Train Epoch: 19 [11000/39785 (0%)]\tLoss: 2948.967041\n",
      "Train Epoch: 19 [12000/39785 (0%)]\tLoss: 2563.681396\n",
      "Train Epoch: 19 [13000/39785 (0%)]\tLoss: 904.395386\n",
      "Train Epoch: 19 [14000/39785 (0%)]\tLoss: 742.718384\n",
      "Train Epoch: 19 [15000/39785 (0%)]\tLoss: 680.586914\n",
      "Train Epoch: 19 [16000/39785 (0%)]\tLoss: 710.862549\n",
      "Train Epoch: 19 [17000/39785 (0%)]\tLoss: 509.232574\n",
      "Train Epoch: 19 [18000/39785 (0%)]\tLoss: 553.945557\n",
      "Train Epoch: 19 [19000/39785 (0%)]\tLoss: 462.745880\n",
      "Train Epoch: 19 [20000/39785 (1%)]\tLoss: 448.273438\n",
      "Train Epoch: 19 [21000/39785 (1%)]\tLoss: 382.000488\n",
      "Train Epoch: 19 [22000/39785 (1%)]\tLoss: 395.991394\n",
      "Train Epoch: 19 [23000/39785 (1%)]\tLoss: 508.906525\n",
      "Train Epoch: 19 [24000/39785 (1%)]\tLoss: 427.815338\n",
      "Train Epoch: 19 [25000/39785 (1%)]\tLoss: 474.843353\n",
      "Train Epoch: 19 [26000/39785 (1%)]\tLoss: 385.486084\n",
      "Train Epoch: 19 [27000/39785 (1%)]\tLoss: 375.747528\n",
      "Train Epoch: 19 [28000/39785 (1%)]\tLoss: 317.974243\n",
      "Train Epoch: 19 [29000/39785 (1%)]\tLoss: 300.945801\n",
      "Train Epoch: 19 [30000/39785 (1%)]\tLoss: 394.131073\n",
      "Train Epoch: 19 [31000/39785 (1%)]\tLoss: 342.387573\n",
      "Train Epoch: 19 [32000/39785 (1%)]\tLoss: 393.078156\n",
      "Train Epoch: 19 [33000/39785 (1%)]\tLoss: 310.992035\n",
      "Train Epoch: 19 [34000/39785 (1%)]\tLoss: 313.101135\n",
      "Train Epoch: 19 [35000/39785 (1%)]\tLoss: 381.650848\n",
      "Train Epoch: 19 [36000/39785 (1%)]\tLoss: 303.821594\n",
      "Train Epoch: 19 [37000/39785 (1%)]\tLoss: 358.617554\n",
      "Train Epoch: 19 [38000/39785 (1%)]\tLoss: 313.614014\n",
      "Train Epoch: 19 [39000/39785 (1%)]\tLoss: 432.304199\n",
      "39770\n",
      "39785\n",
      "Train Epoch: 20 [1000/39785 (0%)]\tLoss: 352.042786\n",
      "Train Epoch: 20 [2000/39785 (0%)]\tLoss: 364.473755\n",
      "Train Epoch: 20 [3000/39785 (0%)]\tLoss: 302.142700\n",
      "Train Epoch: 20 [4000/39785 (0%)]\tLoss: 253.196091\n",
      "Train Epoch: 20 [5000/39785 (0%)]\tLoss: 260.038818\n",
      "Train Epoch: 20 [6000/39785 (0%)]\tLoss: 319.981873\n",
      "Train Epoch: 20 [7000/39785 (0%)]\tLoss: 309.136169\n",
      "Train Epoch: 20 [8000/39785 (0%)]\tLoss: 5394.937500\n",
      "Train Epoch: 20 [9000/39785 (0%)]\tLoss: 6812.352539\n",
      "Train Epoch: 20 [10000/39785 (0%)]\tLoss: 7145.721680\n",
      "Train Epoch: 20 [11000/39785 (0%)]\tLoss: 2797.677979\n",
      "Train Epoch: 20 [12000/39785 (0%)]\tLoss: 2030.055542\n",
      "Train Epoch: 20 [13000/39785 (0%)]\tLoss: 1779.241333\n",
      "Train Epoch: 20 [14000/39785 (0%)]\tLoss: 1043.632324\n",
      "Train Epoch: 20 [15000/39785 (0%)]\tLoss: 926.387451\n",
      "Train Epoch: 20 [16000/39785 (0%)]\tLoss: 814.169861\n",
      "Train Epoch: 20 [17000/39785 (0%)]\tLoss: 542.141907\n",
      "Train Epoch: 20 [18000/39785 (0%)]\tLoss: 721.216797\n",
      "Train Epoch: 20 [19000/39785 (0%)]\tLoss: 575.312439\n",
      "Train Epoch: 20 [20000/39785 (1%)]\tLoss: 766.586731\n",
      "Train Epoch: 20 [21000/39785 (1%)]\tLoss: 564.664429\n",
      "Train Epoch: 20 [22000/39785 (1%)]\tLoss: 577.468201\n",
      "Train Epoch: 20 [23000/39785 (1%)]\tLoss: 463.271851\n",
      "Train Epoch: 20 [24000/39785 (1%)]\tLoss: 409.179077\n",
      "Train Epoch: 20 [25000/39785 (1%)]\tLoss: 841.081665\n",
      "Train Epoch: 20 [26000/39785 (1%)]\tLoss: 331.284485\n",
      "Train Epoch: 20 [27000/39785 (1%)]\tLoss: 399.166016\n",
      "Train Epoch: 20 [28000/39785 (1%)]\tLoss: 325.508759\n",
      "Train Epoch: 20 [29000/39785 (1%)]\tLoss: 323.367310\n",
      "Train Epoch: 20 [30000/39785 (1%)]\tLoss: 402.524536\n",
      "Train Epoch: 20 [31000/39785 (1%)]\tLoss: 381.541779\n",
      "Train Epoch: 20 [32000/39785 (1%)]\tLoss: 412.035248\n",
      "Train Epoch: 20 [33000/39785 (1%)]\tLoss: 336.929657\n",
      "Train Epoch: 20 [34000/39785 (1%)]\tLoss: 314.776611\n",
      "Train Epoch: 20 [35000/39785 (1%)]\tLoss: 580.365295\n",
      "Train Epoch: 20 [36000/39785 (1%)]\tLoss: 362.400574\n",
      "Train Epoch: 20 [37000/39785 (1%)]\tLoss: 370.177094\n",
      "Train Epoch: 20 [38000/39785 (1%)]\tLoss: 440.155029\n",
      "Train Epoch: 20 [39000/39785 (1%)]\tLoss: 446.862640\n",
      "39685\n",
      "39785\n",
      "39785\n",
      "39785\n",
      "Train Epoch: 22 [1000/39785 (0%)]\tLoss: 233.560532\n",
      "Train Epoch: 22 [2000/39785 (0%)]\tLoss: 314.883789\n",
      "Train Epoch: 22 [3000/39785 (0%)]\tLoss: 292.048645\n",
      "Train Epoch: 22 [4000/39785 (0%)]\tLoss: 329.069061\n",
      "Train Epoch: 22 [5000/39785 (0%)]\tLoss: 260.688477\n",
      "Train Epoch: 22 [6000/39785 (0%)]\tLoss: 250.417740\n",
      "Train Epoch: 22 [7000/39785 (0%)]\tLoss: 304.046692\n",
      "Train Epoch: 22 [8000/39785 (0%)]\tLoss: 232.882782\n",
      "Train Epoch: 22 [9000/39785 (0%)]\tLoss: 245.301285\n",
      "Train Epoch: 22 [10000/39785 (0%)]\tLoss: 251.094650\n",
      "Train Epoch: 22 [11000/39785 (0%)]\tLoss: 240.409149\n",
      "Train Epoch: 22 [12000/39785 (0%)]\tLoss: 276.884216\n",
      "Train Epoch: 22 [13000/39785 (0%)]\tLoss: 308.958893\n",
      "Train Epoch: 22 [14000/39785 (0%)]\tLoss: 232.672974\n",
      "Train Epoch: 22 [15000/39785 (0%)]\tLoss: 237.052887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 22 [16000/39785 (0%)]\tLoss: 279.963654\n",
      "Train Epoch: 22 [17000/39785 (0%)]\tLoss: 2179.580811\n",
      "Train Epoch: 22 [18000/39785 (0%)]\tLoss: 7381.197754\n",
      "Train Epoch: 22 [19000/39785 (0%)]\tLoss: 1068.999756\n",
      "Train Epoch: 22 [20000/39785 (1%)]\tLoss: 551.577698\n",
      "Train Epoch: 22 [21000/39785 (1%)]\tLoss: 496.914124\n",
      "Train Epoch: 22 [22000/39785 (1%)]\tLoss: 410.606445\n",
      "Train Epoch: 22 [23000/39785 (1%)]\tLoss: 404.816223\n",
      "Train Epoch: 22 [24000/39785 (1%)]\tLoss: 410.224945\n",
      "Train Epoch: 22 [25000/39785 (1%)]\tLoss: 302.351227\n",
      "Train Epoch: 22 [26000/39785 (1%)]\tLoss: 307.191986\n",
      "Train Epoch: 22 [27000/39785 (1%)]\tLoss: 299.496155\n",
      "Train Epoch: 22 [28000/39785 (1%)]\tLoss: 324.316345\n",
      "Train Epoch: 22 [29000/39785 (1%)]\tLoss: 305.786896\n",
      "Train Epoch: 22 [30000/39785 (1%)]\tLoss: 255.715149\n",
      "Train Epoch: 22 [31000/39785 (1%)]\tLoss: 267.314819\n",
      "Train Epoch: 22 [32000/39785 (1%)]\tLoss: 259.120636\n",
      "Train Epoch: 22 [33000/39785 (1%)]\tLoss: 278.312378\n",
      "Train Epoch: 22 [34000/39785 (1%)]\tLoss: 367.373291\n",
      "Train Epoch: 22 [35000/39785 (1%)]\tLoss: 235.951096\n",
      "Train Epoch: 22 [36000/39785 (1%)]\tLoss: 291.570129\n",
      "Train Epoch: 22 [37000/39785 (1%)]\tLoss: 250.316162\n",
      "Train Epoch: 22 [38000/39785 (1%)]\tLoss: 285.249786\n",
      "Train Epoch: 22 [39000/39785 (1%)]\tLoss: 250.265656\n",
      "39700\n",
      "39785\n",
      "Train Epoch: 23 [1000/39785 (0%)]\tLoss: 270.667206\n",
      "Train Epoch: 23 [2000/39785 (0%)]\tLoss: 297.322937\n",
      "Train Epoch: 23 [3000/39785 (0%)]\tLoss: 291.404449\n",
      "Train Epoch: 23 [4000/39785 (0%)]\tLoss: 236.384308\n",
      "Train Epoch: 23 [5000/39785 (0%)]\tLoss: 246.456085\n",
      "Train Epoch: 23 [6000/39785 (0%)]\tLoss: 195.733719\n",
      "Train Epoch: 23 [7000/39785 (0%)]\tLoss: 181.209625\n",
      "Train Epoch: 23 [8000/39785 (0%)]\tLoss: 212.368317\n",
      "Train Epoch: 23 [9000/39785 (0%)]\tLoss: 315.489532\n",
      "Train Epoch: 23 [10000/39785 (0%)]\tLoss: 664.526062\n",
      "Train Epoch: 23 [11000/39785 (0%)]\tLoss: 369.963348\n",
      "Train Epoch: 23 [12000/39785 (0%)]\tLoss: 342.985596\n",
      "Train Epoch: 23 [13000/39785 (0%)]\tLoss: 303.610138\n",
      "Train Epoch: 23 [14000/39785 (0%)]\tLoss: 248.856842\n",
      "Train Epoch: 23 [15000/39785 (0%)]\tLoss: 228.648697\n",
      "Train Epoch: 23 [16000/39785 (0%)]\tLoss: 196.081772\n",
      "Train Epoch: 23 [17000/39785 (0%)]\tLoss: 249.406143\n",
      "Train Epoch: 23 [18000/39785 (0%)]\tLoss: 204.727585\n",
      "Train Epoch: 23 [19000/39785 (0%)]\tLoss: 265.704926\n",
      "Train Epoch: 23 [20000/39785 (1%)]\tLoss: 205.689529\n",
      "Train Epoch: 23 [21000/39785 (1%)]\tLoss: 255.250900\n",
      "Train Epoch: 23 [22000/39785 (1%)]\tLoss: 275.887695\n",
      "Train Epoch: 23 [23000/39785 (1%)]\tLoss: 259.441864\n",
      "Train Epoch: 23 [24000/39785 (1%)]\tLoss: 233.946548\n",
      "Train Epoch: 23 [25000/39785 (1%)]\tLoss: 215.396667\n",
      "Train Epoch: 23 [26000/39785 (1%)]\tLoss: 224.860886\n",
      "Train Epoch: 23 [27000/39785 (1%)]\tLoss: 162.766891\n",
      "Train Epoch: 23 [28000/39785 (1%)]\tLoss: 174.592422\n",
      "Train Epoch: 23 [29000/39785 (1%)]\tLoss: 222.495865\n",
      "Train Epoch: 23 [30000/39785 (1%)]\tLoss: 186.320236\n",
      "Train Epoch: 23 [31000/39785 (1%)]\tLoss: 214.917801\n",
      "Train Epoch: 23 [32000/39785 (1%)]\tLoss: 222.125778\n",
      "Train Epoch: 23 [33000/39785 (1%)]\tLoss: 247.167114\n",
      "Train Epoch: 23 [34000/39785 (1%)]\tLoss: 192.748062\n",
      "Train Epoch: 23 [35000/39785 (1%)]\tLoss: 195.616028\n",
      "Train Epoch: 23 [36000/39785 (1%)]\tLoss: 195.657257\n",
      "Train Epoch: 23 [37000/39785 (1%)]\tLoss: 241.853745\n",
      "Train Epoch: 23 [38000/39785 (1%)]\tLoss: 212.705261\n",
      "Train Epoch: 23 [39000/39785 (1%)]\tLoss: 188.172989\n",
      "39715\n",
      "39785\n",
      "Train Epoch: 24 [1000/39785 (0%)]\tLoss: 200.149063\n",
      "Train Epoch: 24 [2000/39785 (0%)]\tLoss: 185.599808\n",
      "Train Epoch: 24 [3000/39785 (0%)]\tLoss: 164.362930\n",
      "Train Epoch: 24 [4000/39785 (0%)]\tLoss: 210.895020\n",
      "Train Epoch: 24 [5000/39785 (0%)]\tLoss: 168.043777\n",
      "Train Epoch: 24 [6000/39785 (0%)]\tLoss: 228.630081\n",
      "Train Epoch: 24 [7000/39785 (0%)]\tLoss: 203.066849\n",
      "Train Epoch: 24 [8000/39785 (0%)]\tLoss: 279.496643\n",
      "Train Epoch: 24 [9000/39785 (0%)]\tLoss: 186.672119\n",
      "Train Epoch: 24 [10000/39785 (0%)]\tLoss: 167.313553\n",
      "Train Epoch: 24 [11000/39785 (0%)]\tLoss: 159.468643\n",
      "Train Epoch: 24 [12000/39785 (0%)]\tLoss: 152.378632\n",
      "Train Epoch: 24 [13000/39785 (0%)]\tLoss: 171.051147\n",
      "Train Epoch: 24 [14000/39785 (0%)]\tLoss: 266.293060\n",
      "Train Epoch: 24 [15000/39785 (0%)]\tLoss: 273.549774\n",
      "Train Epoch: 24 [16000/39785 (0%)]\tLoss: 244.179810\n",
      "Train Epoch: 24 [17000/39785 (0%)]\tLoss: 287.377380\n",
      "Train Epoch: 24 [18000/39785 (0%)]\tLoss: 195.469574\n",
      "Train Epoch: 24 [19000/39785 (0%)]\tLoss: 239.151566\n",
      "Train Epoch: 24 [20000/39785 (1%)]\tLoss: 202.194382\n",
      "Train Epoch: 24 [21000/39785 (1%)]\tLoss: 185.196472\n",
      "Train Epoch: 24 [22000/39785 (1%)]\tLoss: 218.149078\n",
      "Train Epoch: 24 [23000/39785 (1%)]\tLoss: 242.869370\n",
      "Train Epoch: 24 [24000/39785 (1%)]\tLoss: 254.489380\n",
      "Train Epoch: 24 [25000/39785 (1%)]\tLoss: 218.149765\n",
      "Train Epoch: 24 [26000/39785 (1%)]\tLoss: 218.715149\n",
      "Train Epoch: 24 [27000/39785 (1%)]\tLoss: 293.461670\n",
      "Train Epoch: 24 [28000/39785 (1%)]\tLoss: 179.078186\n",
      "Train Epoch: 24 [29000/39785 (1%)]\tLoss: 189.055710\n",
      "Train Epoch: 24 [30000/39785 (1%)]\tLoss: 187.542511\n",
      "Train Epoch: 24 [31000/39785 (1%)]\tLoss: 292.992340\n",
      "Train Epoch: 24 [32000/39785 (1%)]\tLoss: 178.487396\n",
      "Train Epoch: 24 [33000/39785 (1%)]\tLoss: 238.465179\n",
      "Train Epoch: 24 [34000/39785 (1%)]\tLoss: 238.917191\n",
      "Train Epoch: 24 [35000/39785 (1%)]\tLoss: 193.768448\n",
      "Train Epoch: 24 [36000/39785 (1%)]\tLoss: 210.708359\n",
      "Train Epoch: 24 [37000/39785 (1%)]\tLoss: 207.281403\n",
      "Train Epoch: 24 [38000/39785 (1%)]\tLoss: 211.250443\n",
      "Train Epoch: 24 [39000/39785 (1%)]\tLoss: 197.210495\n",
      "39730\n",
      "39785\n",
      "Train Epoch: 25 [1000/39785 (0%)]\tLoss: 158.905655\n",
      "Train Epoch: 25 [2000/39785 (0%)]\tLoss: 195.837982\n",
      "Train Epoch: 25 [3000/39785 (0%)]\tLoss: 152.740021\n",
      "Train Epoch: 25 [4000/39785 (0%)]\tLoss: 178.126648\n",
      "Train Epoch: 25 [5000/39785 (0%)]\tLoss: 211.969620\n",
      "Train Epoch: 25 [6000/39785 (0%)]\tLoss: 213.297638\n",
      "Train Epoch: 25 [7000/39785 (0%)]\tLoss: 197.268677\n",
      "Train Epoch: 25 [8000/39785 (0%)]\tLoss: 225.345032\n",
      "Train Epoch: 25 [9000/39785 (0%)]\tLoss: 275.343750\n",
      "Train Epoch: 25 [10000/39785 (0%)]\tLoss: 204.995804\n",
      "Train Epoch: 25 [11000/39785 (0%)]\tLoss: 152.908417\n",
      "Train Epoch: 25 [12000/39785 (0%)]\tLoss: 269.848511\n",
      "Train Epoch: 25 [13000/39785 (0%)]\tLoss: 173.663467\n",
      "Train Epoch: 25 [14000/39785 (0%)]\tLoss: 233.260437\n",
      "Train Epoch: 25 [15000/39785 (0%)]\tLoss: 193.393387\n",
      "Train Epoch: 25 [16000/39785 (0%)]\tLoss: 211.661911\n",
      "Train Epoch: 25 [17000/39785 (0%)]\tLoss: 201.483047\n",
      "Train Epoch: 25 [18000/39785 (0%)]\tLoss: 172.853287\n",
      "Train Epoch: 25 [19000/39785 (0%)]\tLoss: 191.285110\n",
      "Train Epoch: 25 [20000/39785 (1%)]\tLoss: 231.746964\n",
      "Train Epoch: 25 [21000/39785 (1%)]\tLoss: 288.033264\n",
      "Train Epoch: 25 [22000/39785 (1%)]\tLoss: 185.255936\n",
      "Train Epoch: 25 [23000/39785 (1%)]\tLoss: 171.148285\n",
      "Train Epoch: 25 [24000/39785 (1%)]\tLoss: 187.691010\n",
      "Train Epoch: 25 [25000/39785 (1%)]\tLoss: 163.146179\n",
      "Train Epoch: 25 [26000/39785 (1%)]\tLoss: 191.037048\n",
      "Train Epoch: 25 [27000/39785 (1%)]\tLoss: 140.848587\n",
      "Train Epoch: 25 [28000/39785 (1%)]\tLoss: 165.568634\n",
      "Train Epoch: 25 [29000/39785 (1%)]\tLoss: 237.683273\n",
      "Train Epoch: 25 [30000/39785 (1%)]\tLoss: 238.248306\n",
      "Train Epoch: 25 [31000/39785 (1%)]\tLoss: 184.434738\n",
      "Train Epoch: 25 [32000/39785 (1%)]\tLoss: 187.577499\n",
      "Train Epoch: 25 [33000/39785 (1%)]\tLoss: 193.063812\n",
      "Train Epoch: 25 [34000/39785 (1%)]\tLoss: 216.101532\n",
      "Train Epoch: 25 [35000/39785 (1%)]\tLoss: 139.657486\n",
      "Train Epoch: 25 [36000/39785 (1%)]\tLoss: 199.945068\n",
      "Train Epoch: 25 [37000/39785 (1%)]\tLoss: 145.533417\n",
      "Train Epoch: 25 [38000/39785 (1%)]\tLoss: 192.923111\n",
      "Train Epoch: 25 [39000/39785 (1%)]\tLoss: 143.459259\n",
      "39745\n",
      "39785\n",
      "Train Epoch: 26 [1000/39785 (0%)]\tLoss: 175.587204\n",
      "Train Epoch: 26 [2000/39785 (0%)]\tLoss: 189.712296\n",
      "Train Epoch: 26 [3000/39785 (0%)]\tLoss: 204.961349\n",
      "Train Epoch: 26 [4000/39785 (0%)]\tLoss: 194.694412\n",
      "Train Epoch: 26 [5000/39785 (0%)]\tLoss: 135.972748\n",
      "Train Epoch: 26 [6000/39785 (0%)]\tLoss: 134.598053\n",
      "Train Epoch: 26 [7000/39785 (0%)]\tLoss: 194.555344\n",
      "Train Epoch: 26 [8000/39785 (0%)]\tLoss: 164.119125\n",
      "Train Epoch: 26 [9000/39785 (0%)]\tLoss: 164.532578\n",
      "Train Epoch: 26 [10000/39785 (0%)]\tLoss: 172.028595\n",
      "Train Epoch: 26 [11000/39785 (0%)]\tLoss: 149.249130\n",
      "Train Epoch: 26 [12000/39785 (0%)]\tLoss: 156.804535\n",
      "Train Epoch: 26 [13000/39785 (0%)]\tLoss: 185.547104\n",
      "Train Epoch: 26 [14000/39785 (0%)]\tLoss: 177.159286\n",
      "Train Epoch: 26 [15000/39785 (0%)]\tLoss: 187.008698\n",
      "Train Epoch: 26 [16000/39785 (0%)]\tLoss: 145.200165\n",
      "Train Epoch: 26 [17000/39785 (0%)]\tLoss: 188.755127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 26 [18000/39785 (0%)]\tLoss: 204.682831\n",
      "Train Epoch: 26 [19000/39785 (0%)]\tLoss: 158.754074\n",
      "Train Epoch: 26 [20000/39785 (1%)]\tLoss: 122.989746\n",
      "Train Epoch: 26 [21000/39785 (1%)]\tLoss: 166.034042\n",
      "Train Epoch: 26 [22000/39785 (1%)]\tLoss: 202.003830\n",
      "Train Epoch: 26 [23000/39785 (1%)]\tLoss: 167.957031\n",
      "Train Epoch: 26 [24000/39785 (1%)]\tLoss: 156.162399\n",
      "Train Epoch: 26 [25000/39785 (1%)]\tLoss: 209.467346\n",
      "Train Epoch: 26 [26000/39785 (1%)]\tLoss: 167.713791\n",
      "Train Epoch: 26 [27000/39785 (1%)]\tLoss: 211.907700\n",
      "Train Epoch: 26 [28000/39785 (1%)]\tLoss: 206.505707\n",
      "Train Epoch: 26 [29000/39785 (1%)]\tLoss: 168.556961\n",
      "Train Epoch: 26 [30000/39785 (1%)]\tLoss: 195.849121\n",
      "Train Epoch: 26 [31000/39785 (1%)]\tLoss: 210.711945\n",
      "Train Epoch: 26 [32000/39785 (1%)]\tLoss: 207.860504\n",
      "Train Epoch: 26 [33000/39785 (1%)]\tLoss: 227.628281\n",
      "Train Epoch: 26 [34000/39785 (1%)]\tLoss: 176.535400\n",
      "Train Epoch: 26 [35000/39785 (1%)]\tLoss: 188.408142\n",
      "Train Epoch: 26 [36000/39785 (1%)]\tLoss: 197.222870\n",
      "Train Epoch: 26 [37000/39785 (1%)]\tLoss: 192.551086\n",
      "Train Epoch: 26 [38000/39785 (1%)]\tLoss: 167.612183\n",
      "Train Epoch: 26 [39000/39785 (1%)]\tLoss: 220.320786\n",
      "39760\n",
      "39785\n",
      "Train Epoch: 27 [1000/39785 (0%)]\tLoss: 145.073517\n",
      "Train Epoch: 27 [2000/39785 (0%)]\tLoss: 124.464920\n",
      "Train Epoch: 27 [3000/39785 (0%)]\tLoss: 172.851868\n",
      "Train Epoch: 27 [4000/39785 (0%)]\tLoss: 205.253265\n",
      "Train Epoch: 27 [5000/39785 (0%)]\tLoss: 195.868195\n",
      "Train Epoch: 27 [6000/39785 (0%)]\tLoss: 143.945450\n",
      "Train Epoch: 27 [7000/39785 (0%)]\tLoss: 170.719116\n",
      "Train Epoch: 27 [8000/39785 (0%)]\tLoss: 152.333527\n",
      "Train Epoch: 27 [9000/39785 (0%)]\tLoss: 191.029968\n",
      "Train Epoch: 27 [10000/39785 (0%)]\tLoss: 136.879959\n",
      "Train Epoch: 27 [11000/39785 (0%)]\tLoss: 166.397766\n",
      "Train Epoch: 27 [12000/39785 (0%)]\tLoss: 143.850342\n",
      "Train Epoch: 27 [13000/39785 (0%)]\tLoss: 166.385513\n",
      "Train Epoch: 27 [14000/39785 (0%)]\tLoss: 157.899536\n",
      "Train Epoch: 27 [15000/39785 (0%)]\tLoss: 195.134857\n",
      "Train Epoch: 27 [16000/39785 (0%)]\tLoss: 146.341522\n",
      "Train Epoch: 27 [17000/39785 (0%)]\tLoss: 170.245071\n",
      "Train Epoch: 27 [18000/39785 (0%)]\tLoss: 179.081528\n",
      "Train Epoch: 27 [19000/39785 (0%)]\tLoss: 175.347519\n",
      "Train Epoch: 27 [20000/39785 (1%)]\tLoss: 168.048584\n",
      "Train Epoch: 27 [21000/39785 (1%)]\tLoss: 138.609879\n",
      "Train Epoch: 27 [22000/39785 (1%)]\tLoss: 165.731232\n",
      "Train Epoch: 27 [23000/39785 (1%)]\tLoss: 148.000641\n",
      "Train Epoch: 27 [24000/39785 (1%)]\tLoss: 173.937454\n",
      "Train Epoch: 27 [25000/39785 (1%)]\tLoss: 128.895294\n",
      "Train Epoch: 27 [26000/39785 (1%)]\tLoss: 185.090317\n",
      "Train Epoch: 27 [27000/39785 (1%)]\tLoss: 132.803360\n",
      "Train Epoch: 27 [28000/39785 (1%)]\tLoss: 190.369736\n",
      "Train Epoch: 27 [29000/39785 (1%)]\tLoss: 136.357086\n",
      "Train Epoch: 27 [30000/39785 (1%)]\tLoss: 170.119354\n",
      "Train Epoch: 27 [31000/39785 (1%)]\tLoss: 121.718246\n",
      "Train Epoch: 27 [32000/39785 (1%)]\tLoss: 141.859863\n",
      "Train Epoch: 27 [33000/39785 (1%)]\tLoss: 179.890900\n",
      "Train Epoch: 27 [34000/39785 (1%)]\tLoss: 219.383087\n",
      "Train Epoch: 27 [35000/39785 (1%)]\tLoss: 150.992966\n",
      "Train Epoch: 27 [36000/39785 (1%)]\tLoss: 120.752289\n",
      "Train Epoch: 27 [37000/39785 (1%)]\tLoss: 183.103760\n",
      "Train Epoch: 27 [38000/39785 (1%)]\tLoss: 143.745895\n",
      "Train Epoch: 27 [39000/39785 (1%)]\tLoss: 211.105560\n",
      "39775\n",
      "39785\n",
      "Train Epoch: 28 [1000/39785 (0%)]\tLoss: 169.526428\n",
      "Train Epoch: 28 [2000/39785 (0%)]\tLoss: 152.293564\n",
      "Train Epoch: 28 [3000/39785 (0%)]\tLoss: 138.667206\n",
      "Train Epoch: 28 [4000/39785 (0%)]\tLoss: 170.785904\n",
      "Train Epoch: 28 [5000/39785 (0%)]\tLoss: 128.972000\n",
      "Train Epoch: 28 [6000/39785 (0%)]\tLoss: 140.324051\n",
      "Train Epoch: 28 [7000/39785 (0%)]\tLoss: 128.931900\n",
      "Train Epoch: 28 [8000/39785 (0%)]\tLoss: 114.886444\n",
      "Train Epoch: 28 [9000/39785 (0%)]\tLoss: 152.448608\n",
      "Train Epoch: 28 [10000/39785 (0%)]\tLoss: 123.761055\n",
      "Train Epoch: 28 [11000/39785 (0%)]\tLoss: 114.167343\n",
      "Train Epoch: 28 [12000/39785 (0%)]\tLoss: 160.678040\n",
      "Train Epoch: 28 [13000/39785 (0%)]\tLoss: 173.227005\n",
      "Train Epoch: 28 [14000/39785 (0%)]\tLoss: 149.236832\n",
      "Train Epoch: 28 [15000/39785 (0%)]\tLoss: 155.784592\n",
      "Train Epoch: 28 [16000/39785 (0%)]\tLoss: 112.220833\n",
      "Train Epoch: 28 [17000/39785 (0%)]\tLoss: 172.985306\n",
      "Train Epoch: 28 [18000/39785 (0%)]\tLoss: 207.583908\n",
      "Train Epoch: 28 [19000/39785 (0%)]\tLoss: 161.964371\n",
      "Train Epoch: 28 [20000/39785 (1%)]\tLoss: 109.006805\n",
      "Train Epoch: 28 [21000/39785 (1%)]\tLoss: 153.043762\n",
      "Train Epoch: 28 [22000/39785 (1%)]\tLoss: 122.943001\n",
      "Train Epoch: 28 [23000/39785 (1%)]\tLoss: 114.348824\n",
      "Train Epoch: 28 [24000/39785 (1%)]\tLoss: 130.652420\n",
      "Train Epoch: 28 [25000/39785 (1%)]\tLoss: 186.326782\n",
      "Train Epoch: 28 [26000/39785 (1%)]\tLoss: 155.410904\n",
      "Train Epoch: 28 [27000/39785 (1%)]\tLoss: 144.142792\n",
      "Train Epoch: 28 [28000/39785 (1%)]\tLoss: 165.233276\n",
      "Train Epoch: 28 [29000/39785 (1%)]\tLoss: 131.612015\n",
      "Train Epoch: 28 [30000/39785 (1%)]\tLoss: 169.273956\n",
      "Train Epoch: 28 [31000/39785 (1%)]\tLoss: 199.579483\n",
      "Train Epoch: 28 [32000/39785 (1%)]\tLoss: 152.695465\n",
      "Train Epoch: 28 [33000/39785 (1%)]\tLoss: 125.024574\n",
      "Train Epoch: 28 [34000/39785 (1%)]\tLoss: 163.826080\n",
      "Train Epoch: 28 [35000/39785 (1%)]\tLoss: 175.638687\n",
      "Train Epoch: 28 [36000/39785 (1%)]\tLoss: 117.435486\n",
      "Train Epoch: 28 [37000/39785 (1%)]\tLoss: 152.498672\n",
      "Train Epoch: 28 [38000/39785 (1%)]\tLoss: 158.441544\n",
      "Train Epoch: 28 [39000/39785 (1%)]\tLoss: 132.367279\n",
      "39690\n",
      "39785\n",
      "Train Epoch: 29 [1000/39785 (0%)]\tLoss: 101.619385\n",
      "Train Epoch: 29 [2000/39785 (0%)]\tLoss: 190.027939\n",
      "Train Epoch: 29 [3000/39785 (0%)]\tLoss: 154.706421\n",
      "Train Epoch: 29 [4000/39785 (0%)]\tLoss: 99.598656\n",
      "Train Epoch: 29 [5000/39785 (0%)]\tLoss: 136.121887\n",
      "Train Epoch: 29 [6000/39785 (0%)]\tLoss: 195.049316\n",
      "Train Epoch: 29 [7000/39785 (0%)]\tLoss: 119.857513\n",
      "Train Epoch: 29 [8000/39785 (0%)]\tLoss: 174.321350\n",
      "Train Epoch: 29 [9000/39785 (0%)]\tLoss: 129.630188\n",
      "Train Epoch: 29 [10000/39785 (0%)]\tLoss: 131.939514\n",
      "Train Epoch: 29 [11000/39785 (0%)]\tLoss: 170.785522\n",
      "Train Epoch: 29 [12000/39785 (0%)]\tLoss: 180.079239\n",
      "Train Epoch: 29 [13000/39785 (0%)]\tLoss: 153.443069\n",
      "Train Epoch: 29 [14000/39785 (0%)]\tLoss: 153.477951\n",
      "Train Epoch: 29 [15000/39785 (0%)]\tLoss: 179.622299\n",
      "Train Epoch: 29 [16000/39785 (0%)]\tLoss: 140.727386\n",
      "Train Epoch: 29 [17000/39785 (0%)]\tLoss: 172.388489\n",
      "Train Epoch: 29 [18000/39785 (0%)]\tLoss: 184.772980\n",
      "Train Epoch: 29 [19000/39785 (0%)]\tLoss: 130.615402\n",
      "Train Epoch: 29 [20000/39785 (1%)]\tLoss: 132.645554\n",
      "Train Epoch: 29 [21000/39785 (1%)]\tLoss: 127.505150\n",
      "Train Epoch: 29 [22000/39785 (1%)]\tLoss: 131.143173\n",
      "Train Epoch: 29 [23000/39785 (1%)]\tLoss: 122.043777\n",
      "Train Epoch: 29 [24000/39785 (1%)]\tLoss: 134.715134\n",
      "Train Epoch: 29 [25000/39785 (1%)]\tLoss: 122.075470\n",
      "Train Epoch: 29 [26000/39785 (1%)]\tLoss: 175.905212\n",
      "Train Epoch: 29 [27000/39785 (1%)]\tLoss: 105.379372\n",
      "Train Epoch: 29 [28000/39785 (1%)]\tLoss: 117.022911\n",
      "Train Epoch: 29 [29000/39785 (1%)]\tLoss: 119.769501\n",
      "Train Epoch: 29 [30000/39785 (1%)]\tLoss: 141.633789\n",
      "Train Epoch: 29 [31000/39785 (1%)]\tLoss: 148.432220\n",
      "Train Epoch: 29 [32000/39785 (1%)]\tLoss: 146.627014\n",
      "Train Epoch: 29 [33000/39785 (1%)]\tLoss: 165.188644\n",
      "Train Epoch: 29 [34000/39785 (1%)]\tLoss: 118.452728\n",
      "Train Epoch: 29 [35000/39785 (1%)]\tLoss: 168.719711\n",
      "Train Epoch: 29 [36000/39785 (1%)]\tLoss: 181.727921\n",
      "Train Epoch: 29 [37000/39785 (1%)]\tLoss: 138.416611\n",
      "Train Epoch: 29 [38000/39785 (1%)]\tLoss: 187.096512\n",
      "Train Epoch: 29 [39000/39785 (1%)]\tLoss: 161.688461\n",
      "39705\n",
      "39785\n",
      "Train Epoch: 30 [1000/39785 (0%)]\tLoss: 99.363693\n",
      "Train Epoch: 30 [2000/39785 (0%)]\tLoss: 115.478294\n",
      "Train Epoch: 30 [3000/39785 (0%)]\tLoss: 146.050262\n",
      "Train Epoch: 30 [4000/39785 (0%)]\tLoss: 122.103256\n",
      "Train Epoch: 30 [5000/39785 (0%)]\tLoss: 120.181404\n",
      "Train Epoch: 30 [6000/39785 (0%)]\tLoss: 111.867134\n",
      "Train Epoch: 30 [7000/39785 (0%)]\tLoss: 126.549255\n",
      "Train Epoch: 30 [8000/39785 (0%)]\tLoss: 114.304543\n",
      "Train Epoch: 30 [9000/39785 (0%)]\tLoss: 143.778458\n",
      "Train Epoch: 30 [10000/39785 (0%)]\tLoss: 127.934776\n",
      "Train Epoch: 30 [11000/39785 (0%)]\tLoss: 128.581146\n",
      "Train Epoch: 30 [12000/39785 (0%)]\tLoss: 119.419182\n",
      "Train Epoch: 30 [13000/39785 (0%)]\tLoss: 120.790207\n",
      "Train Epoch: 30 [14000/39785 (0%)]\tLoss: 133.408981\n",
      "Train Epoch: 30 [15000/39785 (0%)]\tLoss: 165.288055\n",
      "Train Epoch: 30 [16000/39785 (0%)]\tLoss: 107.020889\n",
      "Train Epoch: 30 [17000/39785 (0%)]\tLoss: 151.228394\n",
      "Train Epoch: 30 [18000/39785 (0%)]\tLoss: 115.301651\n",
      "Train Epoch: 30 [19000/39785 (0%)]\tLoss: 157.793396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 30 [20000/39785 (1%)]\tLoss: 102.993111\n",
      "Train Epoch: 30 [21000/39785 (1%)]\tLoss: 156.144836\n",
      "Train Epoch: 30 [22000/39785 (1%)]\tLoss: 135.597580\n",
      "Train Epoch: 30 [23000/39785 (1%)]\tLoss: 119.716614\n",
      "Train Epoch: 30 [24000/39785 (1%)]\tLoss: 202.235580\n",
      "Train Epoch: 30 [25000/39785 (1%)]\tLoss: 121.147942\n",
      "Train Epoch: 30 [26000/39785 (1%)]\tLoss: 143.485031\n",
      "Train Epoch: 30 [27000/39785 (1%)]\tLoss: 127.660789\n",
      "Train Epoch: 30 [28000/39785 (1%)]\tLoss: 143.791718\n",
      "Train Epoch: 30 [29000/39785 (1%)]\tLoss: 139.654984\n",
      "Train Epoch: 30 [30000/39785 (1%)]\tLoss: 168.066574\n",
      "Train Epoch: 30 [31000/39785 (1%)]\tLoss: 109.261139\n",
      "Train Epoch: 30 [32000/39785 (1%)]\tLoss: 106.828537\n",
      "Train Epoch: 30 [33000/39785 (1%)]\tLoss: 199.258713\n",
      "Train Epoch: 30 [34000/39785 (1%)]\tLoss: 133.750885\n",
      "Train Epoch: 30 [35000/39785 (1%)]\tLoss: 146.484116\n",
      "Train Epoch: 30 [36000/39785 (1%)]\tLoss: 156.100174\n",
      "Train Epoch: 30 [37000/39785 (1%)]\tLoss: 145.571762\n",
      "Train Epoch: 30 [38000/39785 (1%)]\tLoss: 134.673340\n",
      "Train Epoch: 30 [39000/39785 (1%)]\tLoss: 170.987335\n",
      "39720\n",
      "39785\n",
      "Train Epoch: 31 [1000/39785 (0%)]\tLoss: 104.812363\n",
      "Train Epoch: 31 [2000/39785 (0%)]\tLoss: 152.447449\n",
      "Train Epoch: 31 [3000/39785 (0%)]\tLoss: 124.876717\n",
      "Train Epoch: 31 [4000/39785 (0%)]\tLoss: 121.485268\n",
      "Train Epoch: 31 [5000/39785 (0%)]\tLoss: 111.069069\n",
      "Train Epoch: 31 [6000/39785 (0%)]\tLoss: 156.951263\n",
      "Train Epoch: 31 [7000/39785 (0%)]\tLoss: 125.998085\n",
      "Train Epoch: 31 [8000/39785 (0%)]\tLoss: 118.367180\n",
      "Train Epoch: 31 [9000/39785 (0%)]\tLoss: 114.340614\n",
      "Train Epoch: 31 [10000/39785 (0%)]\tLoss: 132.762299\n",
      "Train Epoch: 31 [11000/39785 (0%)]\tLoss: 98.642403\n",
      "Train Epoch: 31 [12000/39785 (0%)]\tLoss: 134.670441\n",
      "Train Epoch: 31 [13000/39785 (0%)]\tLoss: 132.395340\n",
      "Train Epoch: 31 [14000/39785 (0%)]\tLoss: 106.660271\n",
      "Train Epoch: 31 [15000/39785 (0%)]\tLoss: 120.516853\n",
      "Train Epoch: 31 [16000/39785 (0%)]\tLoss: 118.319550\n",
      "Train Epoch: 31 [17000/39785 (0%)]\tLoss: 156.601898\n",
      "Train Epoch: 31 [18000/39785 (0%)]\tLoss: 120.756500\n",
      "Train Epoch: 31 [19000/39785 (0%)]\tLoss: 178.689926\n",
      "Train Epoch: 31 [20000/39785 (1%)]\tLoss: 121.254295\n",
      "Train Epoch: 31 [21000/39785 (1%)]\tLoss: 116.954742\n",
      "Train Epoch: 31 [22000/39785 (1%)]\tLoss: 149.601349\n",
      "Train Epoch: 31 [23000/39785 (1%)]\tLoss: 156.529694\n",
      "Train Epoch: 31 [24000/39785 (1%)]\tLoss: 134.478928\n",
      "Train Epoch: 31 [25000/39785 (1%)]\tLoss: 149.235443\n",
      "Train Epoch: 31 [26000/39785 (1%)]\tLoss: 145.609741\n",
      "Train Epoch: 31 [27000/39785 (1%)]\tLoss: 123.616196\n",
      "Train Epoch: 31 [28000/39785 (1%)]\tLoss: 109.403008\n",
      "Train Epoch: 31 [29000/39785 (1%)]\tLoss: 113.960648\n",
      "Train Epoch: 31 [30000/39785 (1%)]\tLoss: 97.170052\n",
      "Train Epoch: 31 [31000/39785 (1%)]\tLoss: 162.850143\n",
      "Train Epoch: 31 [32000/39785 (1%)]\tLoss: 151.977448\n",
      "Train Epoch: 31 [33000/39785 (1%)]\tLoss: 140.813797\n",
      "Train Epoch: 31 [34000/39785 (1%)]\tLoss: 106.886932\n",
      "Train Epoch: 31 [35000/39785 (1%)]\tLoss: 150.696503\n",
      "Train Epoch: 31 [36000/39785 (1%)]\tLoss: 140.269669\n",
      "Train Epoch: 31 [37000/39785 (1%)]\tLoss: 139.885513\n",
      "Train Epoch: 31 [38000/39785 (1%)]\tLoss: 94.320351\n",
      "Train Epoch: 31 [39000/39785 (1%)]\tLoss: 180.753403\n",
      "39735\n",
      "39785\n",
      "Train Epoch: 32 [1000/39785 (0%)]\tLoss: 118.414040\n",
      "Train Epoch: 32 [2000/39785 (0%)]\tLoss: 94.978874\n",
      "Train Epoch: 32 [3000/39785 (0%)]\tLoss: 127.252419\n",
      "Train Epoch: 32 [4000/39785 (0%)]\tLoss: 91.112648\n",
      "Train Epoch: 32 [5000/39785 (0%)]\tLoss: 97.979912\n",
      "Train Epoch: 32 [6000/39785 (0%)]\tLoss: 109.662605\n",
      "Train Epoch: 32 [7000/39785 (0%)]\tLoss: 121.920372\n",
      "Train Epoch: 32 [8000/39785 (0%)]\tLoss: 118.302994\n",
      "Train Epoch: 32 [9000/39785 (0%)]\tLoss: 109.731201\n",
      "Train Epoch: 32 [10000/39785 (0%)]\tLoss: 133.592102\n",
      "Train Epoch: 32 [11000/39785 (0%)]\tLoss: 132.355988\n",
      "Train Epoch: 32 [12000/39785 (0%)]\tLoss: 125.017876\n",
      "Train Epoch: 32 [13000/39785 (0%)]\tLoss: 145.688263\n",
      "Train Epoch: 32 [14000/39785 (0%)]\tLoss: 114.127060\n",
      "Train Epoch: 32 [15000/39785 (0%)]\tLoss: 93.708595\n",
      "Train Epoch: 32 [16000/39785 (0%)]\tLoss: 123.668121\n",
      "Train Epoch: 32 [17000/39785 (0%)]\tLoss: 88.708099\n",
      "Train Epoch: 32 [18000/39785 (0%)]\tLoss: 85.080070\n",
      "Train Epoch: 32 [19000/39785 (0%)]\tLoss: 135.257370\n",
      "Train Epoch: 32 [20000/39785 (1%)]\tLoss: 127.903412\n",
      "Train Epoch: 32 [21000/39785 (1%)]\tLoss: 89.145775\n",
      "Train Epoch: 32 [22000/39785 (1%)]\tLoss: 1053.205322\n",
      "Train Epoch: 32 [23000/39785 (1%)]\tLoss: 142.150284\n",
      "Train Epoch: 32 [24000/39785 (1%)]\tLoss: 127.219078\n",
      "Train Epoch: 32 [25000/39785 (1%)]\tLoss: 97.082382\n",
      "Train Epoch: 32 [26000/39785 (1%)]\tLoss: 147.470932\n",
      "Train Epoch: 32 [27000/39785 (1%)]\tLoss: 140.442459\n",
      "Train Epoch: 32 [28000/39785 (1%)]\tLoss: 97.548355\n",
      "Train Epoch: 32 [29000/39785 (1%)]\tLoss: 108.479706\n",
      "Train Epoch: 32 [30000/39785 (1%)]\tLoss: 163.473404\n",
      "Train Epoch: 32 [31000/39785 (1%)]\tLoss: 161.619019\n",
      "Train Epoch: 32 [32000/39785 (1%)]\tLoss: 209.421509\n",
      "Train Epoch: 32 [33000/39785 (1%)]\tLoss: 143.120743\n",
      "Train Epoch: 32 [34000/39785 (1%)]\tLoss: 94.995872\n",
      "Train Epoch: 32 [35000/39785 (1%)]\tLoss: 127.955551\n",
      "Train Epoch: 32 [36000/39785 (1%)]\tLoss: 96.321465\n",
      "Train Epoch: 32 [37000/39785 (1%)]\tLoss: 125.834633\n",
      "Train Epoch: 32 [38000/39785 (1%)]\tLoss: 98.846489\n",
      "Train Epoch: 32 [39000/39785 (1%)]\tLoss: 141.007675\n",
      "39750\n",
      "39785\n",
      "Train Epoch: 33 [1000/39785 (0%)]\tLoss: 104.548454\n",
      "Train Epoch: 33 [2000/39785 (0%)]\tLoss: 140.354797\n",
      "Train Epoch: 33 [3000/39785 (0%)]\tLoss: 82.794235\n",
      "Train Epoch: 33 [4000/39785 (0%)]\tLoss: 142.074203\n",
      "Train Epoch: 33 [5000/39785 (0%)]\tLoss: 111.299461\n",
      "Train Epoch: 33 [6000/39785 (0%)]\tLoss: 100.099220\n",
      "Train Epoch: 33 [7000/39785 (0%)]\tLoss: 100.876656\n",
      "Train Epoch: 33 [8000/39785 (0%)]\tLoss: 102.612778\n",
      "Train Epoch: 33 [9000/39785 (0%)]\tLoss: 116.528992\n",
      "Train Epoch: 33 [10000/39785 (0%)]\tLoss: 101.262184\n",
      "Train Epoch: 33 [11000/39785 (0%)]\tLoss: 128.602020\n",
      "Train Epoch: 33 [12000/39785 (0%)]\tLoss: 76.986191\n",
      "Train Epoch: 33 [13000/39785 (0%)]\tLoss: 114.477997\n",
      "Train Epoch: 33 [14000/39785 (0%)]\tLoss: 118.134499\n",
      "Train Epoch: 33 [15000/39785 (0%)]\tLoss: 111.870384\n",
      "Train Epoch: 33 [16000/39785 (0%)]\tLoss: 110.196625\n",
      "Train Epoch: 33 [17000/39785 (0%)]\tLoss: 111.057152\n",
      "Train Epoch: 33 [18000/39785 (0%)]\tLoss: 75.301353\n",
      "Train Epoch: 33 [19000/39785 (0%)]\tLoss: 118.471008\n",
      "Train Epoch: 33 [20000/39785 (1%)]\tLoss: 81.343262\n",
      "Train Epoch: 33 [21000/39785 (1%)]\tLoss: 102.327164\n",
      "Train Epoch: 33 [22000/39785 (1%)]\tLoss: 116.295563\n",
      "Train Epoch: 33 [23000/39785 (1%)]\tLoss: 90.645836\n",
      "Train Epoch: 33 [24000/39785 (1%)]\tLoss: 93.488258\n",
      "Train Epoch: 33 [25000/39785 (1%)]\tLoss: 132.819382\n",
      "Train Epoch: 33 [26000/39785 (1%)]\tLoss: 106.994331\n",
      "Train Epoch: 33 [27000/39785 (1%)]\tLoss: 123.000259\n",
      "Train Epoch: 33 [28000/39785 (1%)]\tLoss: 86.492638\n",
      "Train Epoch: 33 [29000/39785 (1%)]\tLoss: 84.171211\n",
      "Train Epoch: 33 [30000/39785 (1%)]\tLoss: 138.654984\n",
      "Train Epoch: 33 [31000/39785 (1%)]\tLoss: 161.472382\n",
      "Train Epoch: 33 [32000/39785 (1%)]\tLoss: 90.241386\n",
      "Train Epoch: 33 [33000/39785 (1%)]\tLoss: 141.864090\n",
      "Train Epoch: 33 [34000/39785 (1%)]\tLoss: 174.379883\n",
      "Train Epoch: 33 [35000/39785 (1%)]\tLoss: 104.570656\n",
      "Train Epoch: 33 [36000/39785 (1%)]\tLoss: 119.100624\n",
      "Train Epoch: 33 [37000/39785 (1%)]\tLoss: 103.117615\n",
      "Train Epoch: 33 [38000/39785 (1%)]\tLoss: 92.025734\n",
      "Train Epoch: 33 [39000/39785 (1%)]\tLoss: 120.729477\n",
      "39765\n",
      "39785\n",
      "Train Epoch: 34 [1000/39785 (0%)]\tLoss: 72.308113\n",
      "Train Epoch: 34 [2000/39785 (0%)]\tLoss: 92.013794\n",
      "Train Epoch: 34 [3000/39785 (0%)]\tLoss: 102.804588\n",
      "Train Epoch: 34 [4000/39785 (0%)]\tLoss: 102.022797\n",
      "Train Epoch: 34 [5000/39785 (0%)]\tLoss: 94.254036\n",
      "Train Epoch: 34 [6000/39785 (0%)]\tLoss: 102.303963\n",
      "Train Epoch: 34 [7000/39785 (0%)]\tLoss: 79.998413\n",
      "Train Epoch: 34 [8000/39785 (0%)]\tLoss: 147.471512\n",
      "Train Epoch: 34 [9000/39785 (0%)]\tLoss: 98.151611\n",
      "Train Epoch: 34 [10000/39785 (0%)]\tLoss: 92.270271\n",
      "Train Epoch: 34 [11000/39785 (0%)]\tLoss: 113.692642\n",
      "Train Epoch: 34 [12000/39785 (0%)]\tLoss: 115.824440\n",
      "Train Epoch: 34 [13000/39785 (0%)]\tLoss: 130.641785\n",
      "Train Epoch: 34 [14000/39785 (0%)]\tLoss: 107.483871\n",
      "Train Epoch: 34 [15000/39785 (0%)]\tLoss: 97.802750\n",
      "Train Epoch: 34 [16000/39785 (0%)]\tLoss: 92.882919\n",
      "Train Epoch: 34 [17000/39785 (0%)]\tLoss: 90.876289\n",
      "Train Epoch: 34 [18000/39785 (0%)]\tLoss: 84.039810\n",
      "Train Epoch: 34 [19000/39785 (0%)]\tLoss: 101.164276\n",
      "Train Epoch: 34 [20000/39785 (1%)]\tLoss: 93.884941\n",
      "Train Epoch: 34 [21000/39785 (1%)]\tLoss: 99.433746\n",
      "Train Epoch: 34 [22000/39785 (1%)]\tLoss: 86.524658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 34 [23000/39785 (1%)]\tLoss: 84.283195\n",
      "Train Epoch: 34 [24000/39785 (1%)]\tLoss: 101.605988\n",
      "Train Epoch: 34 [25000/39785 (1%)]\tLoss: 98.696114\n",
      "Train Epoch: 34 [26000/39785 (1%)]\tLoss: 97.509071\n",
      "Train Epoch: 34 [27000/39785 (1%)]\tLoss: 131.259415\n",
      "Train Epoch: 34 [28000/39785 (1%)]\tLoss: 150.176559\n",
      "Train Epoch: 34 [29000/39785 (1%)]\tLoss: 125.832466\n",
      "Train Epoch: 34 [30000/39785 (1%)]\tLoss: 120.225510\n",
      "Train Epoch: 34 [31000/39785 (1%)]\tLoss: 119.179420\n",
      "Train Epoch: 34 [32000/39785 (1%)]\tLoss: 83.392609\n",
      "Train Epoch: 34 [33000/39785 (1%)]\tLoss: 103.816917\n",
      "Train Epoch: 34 [34000/39785 (1%)]\tLoss: 113.577629\n",
      "Train Epoch: 34 [35000/39785 (1%)]\tLoss: 111.218735\n",
      "Train Epoch: 34 [36000/39785 (1%)]\tLoss: 103.738350\n",
      "Train Epoch: 34 [37000/39785 (1%)]\tLoss: 109.434402\n",
      "Train Epoch: 34 [38000/39785 (1%)]\tLoss: 135.677673\n",
      "Train Epoch: 34 [39000/39785 (1%)]\tLoss: 102.696358\n",
      "39780\n",
      "39785\n",
      "Train Epoch: 35 [1000/39785 (0%)]\tLoss: 70.871025\n",
      "Train Epoch: 35 [2000/39785 (0%)]\tLoss: 106.460487\n",
      "Train Epoch: 35 [3000/39785 (0%)]\tLoss: 63.737278\n",
      "Train Epoch: 35 [4000/39785 (0%)]\tLoss: 62.624893\n",
      "Train Epoch: 35 [5000/39785 (0%)]\tLoss: 88.571274\n",
      "Train Epoch: 35 [6000/39785 (0%)]\tLoss: 96.224121\n",
      "Train Epoch: 35 [7000/39785 (0%)]\tLoss: 96.964653\n",
      "Train Epoch: 35 [8000/39785 (0%)]\tLoss: 121.257164\n",
      "Train Epoch: 35 [9000/39785 (0%)]\tLoss: 109.345871\n",
      "Train Epoch: 35 [10000/39785 (0%)]\tLoss: 131.536224\n",
      "Train Epoch: 35 [11000/39785 (0%)]\tLoss: 92.433998\n",
      "Train Epoch: 35 [12000/39785 (0%)]\tLoss: 115.997864\n",
      "Train Epoch: 35 [13000/39785 (0%)]\tLoss: 97.487175\n",
      "Train Epoch: 35 [14000/39785 (0%)]\tLoss: 85.773331\n",
      "Train Epoch: 35 [15000/39785 (0%)]\tLoss: 105.466270\n",
      "Train Epoch: 35 [16000/39785 (0%)]\tLoss: 63.891888\n",
      "Train Epoch: 35 [17000/39785 (0%)]\tLoss: 79.653297\n",
      "Train Epoch: 35 [18000/39785 (0%)]\tLoss: 95.618141\n",
      "Train Epoch: 35 [19000/39785 (0%)]\tLoss: 72.050613\n",
      "Train Epoch: 35 [20000/39785 (1%)]\tLoss: 105.021492\n",
      "Train Epoch: 35 [21000/39785 (1%)]\tLoss: 59.069252\n",
      "Train Epoch: 35 [22000/39785 (1%)]\tLoss: 83.856209\n",
      "Train Epoch: 35 [23000/39785 (1%)]\tLoss: 81.968086\n",
      "Train Epoch: 35 [24000/39785 (1%)]\tLoss: 88.942490\n",
      "Train Epoch: 35 [25000/39785 (1%)]\tLoss: 68.316391\n",
      "Train Epoch: 35 [26000/39785 (1%)]\tLoss: 91.209358\n",
      "Train Epoch: 35 [27000/39785 (1%)]\tLoss: 94.160606\n",
      "Train Epoch: 35 [28000/39785 (1%)]\tLoss: 84.148621\n",
      "Train Epoch: 35 [29000/39785 (1%)]\tLoss: 136.703842\n",
      "Train Epoch: 35 [30000/39785 (1%)]\tLoss: 84.484825\n",
      "Train Epoch: 35 [31000/39785 (1%)]\tLoss: 78.744873\n",
      "Train Epoch: 35 [32000/39785 (1%)]\tLoss: 103.351387\n",
      "Train Epoch: 35 [33000/39785 (1%)]\tLoss: 97.085800\n",
      "Train Epoch: 35 [34000/39785 (1%)]\tLoss: 113.347610\n",
      "Train Epoch: 35 [35000/39785 (1%)]\tLoss: 119.158768\n",
      "Train Epoch: 35 [36000/39785 (1%)]\tLoss: 105.587708\n",
      "Train Epoch: 35 [37000/39785 (1%)]\tLoss: 78.376198\n",
      "Train Epoch: 35 [38000/39785 (1%)]\tLoss: 104.914948\n",
      "Train Epoch: 35 [39000/39785 (1%)]\tLoss: 106.820274\n",
      "39695\n",
      "39785\n",
      "Train Epoch: 36 [1000/39785 (0%)]\tLoss: 59.052696\n",
      "Train Epoch: 36 [2000/39785 (0%)]\tLoss: 72.777962\n",
      "Train Epoch: 36 [3000/39785 (0%)]\tLoss: 81.077263\n",
      "Train Epoch: 36 [4000/39785 (0%)]\tLoss: 83.692467\n",
      "Train Epoch: 36 [5000/39785 (0%)]\tLoss: 70.343788\n",
      "Train Epoch: 36 [6000/39785 (0%)]\tLoss: 73.957718\n",
      "Train Epoch: 36 [7000/39785 (0%)]\tLoss: 83.853806\n",
      "Train Epoch: 36 [8000/39785 (0%)]\tLoss: 73.800011\n",
      "Train Epoch: 36 [9000/39785 (0%)]\tLoss: 77.316711\n",
      "Train Epoch: 36 [10000/39785 (0%)]\tLoss: 66.480339\n",
      "Train Epoch: 36 [11000/39785 (0%)]\tLoss: 68.640564\n",
      "Train Epoch: 36 [12000/39785 (0%)]\tLoss: 105.688820\n",
      "Train Epoch: 36 [13000/39785 (0%)]\tLoss: 102.103180\n",
      "Train Epoch: 36 [14000/39785 (0%)]\tLoss: 92.358345\n",
      "Train Epoch: 36 [15000/39785 (0%)]\tLoss: 104.125549\n",
      "Train Epoch: 36 [16000/39785 (0%)]\tLoss: 73.248749\n",
      "Train Epoch: 36 [17000/39785 (0%)]\tLoss: 79.278030\n",
      "Train Epoch: 36 [18000/39785 (0%)]\tLoss: 92.128082\n",
      "Train Epoch: 36 [19000/39785 (0%)]\tLoss: 82.978371\n",
      "Train Epoch: 36 [20000/39785 (1%)]\tLoss: 98.539543\n",
      "Train Epoch: 36 [21000/39785 (1%)]\tLoss: 71.631935\n",
      "Train Epoch: 36 [22000/39785 (1%)]\tLoss: 57.490501\n",
      "Train Epoch: 36 [23000/39785 (1%)]\tLoss: 77.658852\n",
      "Train Epoch: 36 [24000/39785 (1%)]\tLoss: 69.261772\n",
      "Train Epoch: 36 [25000/39785 (1%)]\tLoss: 98.379372\n",
      "Train Epoch: 36 [26000/39785 (1%)]\tLoss: 82.950287\n",
      "Train Epoch: 36 [27000/39785 (1%)]\tLoss: 119.284576\n",
      "Train Epoch: 36 [28000/39785 (1%)]\tLoss: 113.310799\n",
      "Train Epoch: 36 [29000/39785 (1%)]\tLoss: 127.429825\n",
      "Train Epoch: 36 [30000/39785 (1%)]\tLoss: 95.527733\n",
      "Train Epoch: 36 [31000/39785 (1%)]\tLoss: 80.362297\n",
      "Train Epoch: 36 [32000/39785 (1%)]\tLoss: 93.968834\n",
      "Train Epoch: 36 [33000/39785 (1%)]\tLoss: 125.219749\n",
      "Train Epoch: 36 [34000/39785 (1%)]\tLoss: 86.242516\n",
      "Train Epoch: 36 [35000/39785 (1%)]\tLoss: 77.154900\n",
      "Train Epoch: 36 [36000/39785 (1%)]\tLoss: 69.423309\n",
      "Train Epoch: 36 [37000/39785 (1%)]\tLoss: 77.486221\n",
      "Train Epoch: 36 [38000/39785 (1%)]\tLoss: 103.549553\n",
      "Train Epoch: 36 [39000/39785 (1%)]\tLoss: 76.902336\n",
      "39710\n",
      "39785\n",
      "Train Epoch: 37 [1000/39785 (0%)]\tLoss: 64.141571\n",
      "Train Epoch: 37 [2000/39785 (0%)]\tLoss: 66.969994\n",
      "Train Epoch: 37 [3000/39785 (0%)]\tLoss: 71.829971\n",
      "Train Epoch: 37 [4000/39785 (0%)]\tLoss: 95.093010\n",
      "Train Epoch: 37 [5000/39785 (0%)]\tLoss: 91.167236\n",
      "Train Epoch: 37 [6000/39785 (0%)]\tLoss: 63.402874\n",
      "Train Epoch: 37 [7000/39785 (0%)]\tLoss: 81.741753\n",
      "Train Epoch: 37 [8000/39785 (0%)]\tLoss: 82.039742\n",
      "Train Epoch: 37 [9000/39785 (0%)]\tLoss: 75.464493\n",
      "Train Epoch: 37 [10000/39785 (0%)]\tLoss: 64.114449\n",
      "Train Epoch: 37 [11000/39785 (0%)]\tLoss: 94.897011\n",
      "Train Epoch: 37 [12000/39785 (0%)]\tLoss: 76.829277\n",
      "Train Epoch: 37 [13000/39785 (0%)]\tLoss: 89.729828\n",
      "Train Epoch: 37 [14000/39785 (0%)]\tLoss: 88.106621\n",
      "Train Epoch: 37 [15000/39785 (0%)]\tLoss: 94.483719\n",
      "Train Epoch: 37 [16000/39785 (0%)]\tLoss: 108.060394\n",
      "Train Epoch: 37 [17000/39785 (0%)]\tLoss: 103.210587\n",
      "Train Epoch: 37 [18000/39785 (0%)]\tLoss: 78.821754\n",
      "Train Epoch: 37 [19000/39785 (0%)]\tLoss: 46.594875\n",
      "Train Epoch: 37 [20000/39785 (1%)]\tLoss: 73.217514\n",
      "Train Epoch: 37 [21000/39785 (1%)]\tLoss: 85.088982\n",
      "Train Epoch: 37 [22000/39785 (1%)]\tLoss: 70.056168\n",
      "Train Epoch: 37 [23000/39785 (1%)]\tLoss: 81.553902\n",
      "Train Epoch: 37 [24000/39785 (1%)]\tLoss: 89.774529\n",
      "Train Epoch: 37 [25000/39785 (1%)]\tLoss: 69.925613\n",
      "Train Epoch: 37 [26000/39785 (1%)]\tLoss: 72.259468\n",
      "Train Epoch: 37 [27000/39785 (1%)]\tLoss: 93.230370\n",
      "Train Epoch: 37 [28000/39785 (1%)]\tLoss: 77.074280\n",
      "Train Epoch: 37 [29000/39785 (1%)]\tLoss: 83.753822\n",
      "Train Epoch: 37 [30000/39785 (1%)]\tLoss: 76.561455\n",
      "Train Epoch: 37 [31000/39785 (1%)]\tLoss: 103.631119\n",
      "Train Epoch: 37 [32000/39785 (1%)]\tLoss: 117.319489\n",
      "Train Epoch: 37 [33000/39785 (1%)]\tLoss: 76.286583\n",
      "Train Epoch: 37 [34000/39785 (1%)]\tLoss: 64.555237\n",
      "Train Epoch: 37 [35000/39785 (1%)]\tLoss: 66.709381\n",
      "Train Epoch: 37 [36000/39785 (1%)]\tLoss: 74.987465\n",
      "Train Epoch: 37 [37000/39785 (1%)]\tLoss: 92.858269\n",
      "Train Epoch: 37 [38000/39785 (1%)]\tLoss: 75.722000\n",
      "Train Epoch: 37 [39000/39785 (1%)]\tLoss: 73.093842\n",
      "39725\n",
      "39785\n",
      "Train Epoch: 38 [1000/39785 (0%)]\tLoss: 90.153404\n",
      "Train Epoch: 38 [2000/39785 (0%)]\tLoss: 82.391861\n",
      "Train Epoch: 38 [3000/39785 (0%)]\tLoss: 60.655243\n",
      "Train Epoch: 38 [4000/39785 (0%)]\tLoss: 54.661743\n",
      "Train Epoch: 38 [5000/39785 (0%)]\tLoss: 74.727303\n",
      "Train Epoch: 38 [6000/39785 (0%)]\tLoss: 62.099884\n",
      "Train Epoch: 38 [7000/39785 (0%)]\tLoss: 51.786114\n",
      "Train Epoch: 38 [8000/39785 (0%)]\tLoss: 50.515331\n",
      "Train Epoch: 38 [9000/39785 (0%)]\tLoss: 53.604137\n",
      "Train Epoch: 38 [10000/39785 (0%)]\tLoss: 59.594589\n",
      "Train Epoch: 38 [11000/39785 (0%)]\tLoss: 74.540741\n",
      "Train Epoch: 38 [12000/39785 (0%)]\tLoss: 65.114899\n",
      "Train Epoch: 38 [13000/39785 (0%)]\tLoss: 73.000740\n",
      "Train Epoch: 38 [14000/39785 (0%)]\tLoss: 64.580795\n",
      "Train Epoch: 38 [15000/39785 (0%)]\tLoss: 83.946213\n",
      "Train Epoch: 38 [16000/39785 (0%)]\tLoss: 71.868904\n",
      "Train Epoch: 38 [17000/39785 (0%)]\tLoss: 98.861488\n",
      "Train Epoch: 38 [18000/39785 (0%)]\tLoss: 57.977352\n",
      "Train Epoch: 38 [19000/39785 (0%)]\tLoss: 82.856766\n",
      "Train Epoch: 38 [20000/39785 (1%)]\tLoss: 68.882874\n",
      "Train Epoch: 38 [21000/39785 (1%)]\tLoss: 72.247276\n",
      "Train Epoch: 38 [22000/39785 (1%)]\tLoss: 69.271057\n",
      "Train Epoch: 38 [23000/39785 (1%)]\tLoss: 66.508797\n",
      "Train Epoch: 38 [24000/39785 (1%)]\tLoss: 70.610535\n",
      "Train Epoch: 38 [25000/39785 (1%)]\tLoss: 90.883202\n",
      "Train Epoch: 38 [26000/39785 (1%)]\tLoss: 92.648247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [27000/39785 (1%)]\tLoss: 85.271873\n",
      "Train Epoch: 38 [28000/39785 (1%)]\tLoss: 71.789284\n",
      "Train Epoch: 38 [29000/39785 (1%)]\tLoss: 54.209515\n",
      "Train Epoch: 38 [30000/39785 (1%)]\tLoss: 51.324833\n",
      "Train Epoch: 38 [31000/39785 (1%)]\tLoss: 73.448151\n",
      "Train Epoch: 38 [32000/39785 (1%)]\tLoss: 81.495018\n",
      "Train Epoch: 38 [33000/39785 (1%)]\tLoss: 65.185158\n",
      "Train Epoch: 38 [34000/39785 (1%)]\tLoss: 73.700813\n",
      "Train Epoch: 38 [35000/39785 (1%)]\tLoss: 57.396877\n",
      "Train Epoch: 38 [36000/39785 (1%)]\tLoss: 73.465363\n",
      "Train Epoch: 38 [37000/39785 (1%)]\tLoss: 75.900993\n",
      "Train Epoch: 38 [38000/39785 (1%)]\tLoss: 106.960281\n",
      "Train Epoch: 38 [39000/39785 (1%)]\tLoss: 82.596451\n",
      "39740\n",
      "39785\n",
      "Train Epoch: 39 [1000/39785 (0%)]\tLoss: 58.526257\n",
      "Train Epoch: 39 [2000/39785 (0%)]\tLoss: 65.158775\n",
      "Train Epoch: 39 [3000/39785 (0%)]\tLoss: 58.407074\n",
      "Train Epoch: 39 [4000/39785 (0%)]\tLoss: 101.377762\n",
      "Train Epoch: 39 [5000/39785 (0%)]\tLoss: 55.757027\n",
      "Train Epoch: 39 [6000/39785 (0%)]\tLoss: 61.581757\n",
      "Train Epoch: 39 [7000/39785 (0%)]\tLoss: 52.279469\n",
      "Train Epoch: 39 [8000/39785 (0%)]\tLoss: 93.421761\n",
      "Train Epoch: 39 [9000/39785 (0%)]\tLoss: 64.619171\n",
      "Train Epoch: 39 [10000/39785 (0%)]\tLoss: 67.072685\n",
      "Train Epoch: 39 [11000/39785 (0%)]\tLoss: 48.098312\n",
      "Train Epoch: 39 [12000/39785 (0%)]\tLoss: 61.543034\n",
      "Train Epoch: 39 [13000/39785 (0%)]\tLoss: 63.154152\n",
      "Train Epoch: 39 [14000/39785 (0%)]\tLoss: 54.840221\n",
      "Train Epoch: 39 [15000/39785 (0%)]\tLoss: 59.998188\n",
      "Train Epoch: 39 [16000/39785 (0%)]\tLoss: 67.717979\n",
      "Train Epoch: 39 [17000/39785 (0%)]\tLoss: 78.773247\n",
      "Train Epoch: 39 [18000/39785 (0%)]\tLoss: 95.864952\n",
      "Train Epoch: 39 [19000/39785 (0%)]\tLoss: 47.718975\n",
      "Train Epoch: 39 [20000/39785 (1%)]\tLoss: 65.622757\n",
      "Train Epoch: 39 [21000/39785 (1%)]\tLoss: 82.036438\n",
      "Train Epoch: 39 [22000/39785 (1%)]\tLoss: 89.753082\n",
      "Train Epoch: 39 [23000/39785 (1%)]\tLoss: 77.483856\n",
      "Train Epoch: 39 [24000/39785 (1%)]\tLoss: 64.538284\n",
      "Train Epoch: 39 [25000/39785 (1%)]\tLoss: 66.379021\n",
      "Train Epoch: 39 [26000/39785 (1%)]\tLoss: 109.453705\n",
      "Train Epoch: 39 [27000/39785 (1%)]\tLoss: 81.275513\n",
      "Train Epoch: 39 [28000/39785 (1%)]\tLoss: 70.403770\n",
      "Train Epoch: 39 [29000/39785 (1%)]\tLoss: 61.666286\n",
      "Train Epoch: 39 [30000/39785 (1%)]\tLoss: 79.533348\n",
      "Train Epoch: 39 [31000/39785 (1%)]\tLoss: 53.818199\n",
      "Train Epoch: 39 [32000/39785 (1%)]\tLoss: 67.048775\n",
      "Train Epoch: 39 [33000/39785 (1%)]\tLoss: 54.670311\n",
      "Train Epoch: 39 [34000/39785 (1%)]\tLoss: 366.570221\n",
      "Train Epoch: 39 [35000/39785 (1%)]\tLoss: 78.257034\n",
      "Train Epoch: 39 [36000/39785 (1%)]\tLoss: 85.804672\n",
      "Train Epoch: 39 [37000/39785 (1%)]\tLoss: 56.628220\n",
      "Train Epoch: 39 [38000/39785 (1%)]\tLoss: 60.639545\n",
      "Train Epoch: 39 [39000/39785 (1%)]\tLoss: 75.166161\n",
      "39755\n",
      "39785\n",
      "Train Epoch: 40 [1000/39785 (0%)]\tLoss: 43.197937\n",
      "Train Epoch: 40 [2000/39785 (0%)]\tLoss: 44.675285\n",
      "Train Epoch: 40 [3000/39785 (0%)]\tLoss: 76.200882\n",
      "Train Epoch: 40 [4000/39785 (0%)]\tLoss: 69.412064\n",
      "Train Epoch: 40 [5000/39785 (0%)]\tLoss: 59.622143\n",
      "Train Epoch: 40 [6000/39785 (0%)]\tLoss: 62.884712\n",
      "Train Epoch: 40 [7000/39785 (0%)]\tLoss: 54.532925\n",
      "Train Epoch: 40 [8000/39785 (0%)]\tLoss: 54.342236\n",
      "Train Epoch: 40 [9000/39785 (0%)]\tLoss: 56.083023\n",
      "Train Epoch: 40 [10000/39785 (0%)]\tLoss: 74.787971\n",
      "Train Epoch: 40 [11000/39785 (0%)]\tLoss: 50.459862\n",
      "Train Epoch: 40 [12000/39785 (0%)]\tLoss: 44.516125\n",
      "Train Epoch: 40 [13000/39785 (0%)]\tLoss: 67.550377\n",
      "Train Epoch: 40 [14000/39785 (0%)]\tLoss: 54.442886\n",
      "Train Epoch: 40 [15000/39785 (0%)]\tLoss: 58.434544\n",
      "Train Epoch: 40 [16000/39785 (0%)]\tLoss: 72.115585\n",
      "Train Epoch: 40 [17000/39785 (0%)]\tLoss: 48.161747\n",
      "Train Epoch: 40 [18000/39785 (0%)]\tLoss: 52.411892\n",
      "Train Epoch: 40 [19000/39785 (0%)]\tLoss: 63.506042\n",
      "Train Epoch: 40 [20000/39785 (1%)]\tLoss: 52.481804\n",
      "Train Epoch: 40 [21000/39785 (1%)]\tLoss: 1624.329834\n",
      "Train Epoch: 40 [22000/39785 (1%)]\tLoss: 62.383503\n",
      "Train Epoch: 40 [23000/39785 (1%)]\tLoss: 50.878250\n",
      "Train Epoch: 40 [24000/39785 (1%)]\tLoss: 62.913033\n",
      "Train Epoch: 40 [25000/39785 (1%)]\tLoss: 86.516350\n",
      "Train Epoch: 40 [26000/39785 (1%)]\tLoss: 59.480156\n",
      "Train Epoch: 40 [27000/39785 (1%)]\tLoss: 65.790894\n",
      "Train Epoch: 40 [28000/39785 (1%)]\tLoss: 60.697803\n",
      "Train Epoch: 40 [29000/39785 (1%)]\tLoss: 72.601662\n",
      "Train Epoch: 40 [30000/39785 (1%)]\tLoss: 58.634300\n",
      "Train Epoch: 40 [31000/39785 (1%)]\tLoss: 58.313313\n",
      "Train Epoch: 40 [32000/39785 (1%)]\tLoss: 63.654823\n",
      "Train Epoch: 40 [33000/39785 (1%)]\tLoss: 65.516693\n",
      "Train Epoch: 40 [34000/39785 (1%)]\tLoss: 50.459278\n",
      "Train Epoch: 40 [35000/39785 (1%)]\tLoss: 67.703812\n",
      "Train Epoch: 40 [36000/39785 (1%)]\tLoss: 62.152527\n",
      "Train Epoch: 40 [37000/39785 (1%)]\tLoss: 59.934059\n",
      "Train Epoch: 40 [38000/39785 (1%)]\tLoss: 67.376038\n",
      "Train Epoch: 40 [39000/39785 (1%)]\tLoss: 67.415756\n",
      "39770\n",
      "39785\n",
      "Train Epoch: 41 [1000/39785 (0%)]\tLoss: 45.916145\n",
      "Train Epoch: 41 [2000/39785 (0%)]\tLoss: 64.630112\n",
      "Train Epoch: 41 [3000/39785 (0%)]\tLoss: 54.017296\n",
      "Train Epoch: 41 [4000/39785 (0%)]\tLoss: 71.268021\n",
      "Train Epoch: 41 [5000/39785 (0%)]\tLoss: 54.435131\n",
      "Train Epoch: 41 [6000/39785 (0%)]\tLoss: 49.171032\n",
      "Train Epoch: 41 [7000/39785 (0%)]\tLoss: 49.818535\n",
      "Train Epoch: 41 [8000/39785 (0%)]\tLoss: 43.417336\n",
      "Train Epoch: 41 [9000/39785 (0%)]\tLoss: 50.524529\n",
      "Train Epoch: 41 [10000/39785 (0%)]\tLoss: 43.965199\n",
      "Train Epoch: 41 [11000/39785 (0%)]\tLoss: 55.511379\n",
      "Train Epoch: 41 [12000/39785 (0%)]\tLoss: 44.020672\n",
      "Train Epoch: 41 [13000/39785 (0%)]\tLoss: 59.424641\n",
      "Train Epoch: 41 [14000/39785 (0%)]\tLoss: 60.635639\n",
      "Train Epoch: 41 [15000/39785 (0%)]\tLoss: 63.009403\n",
      "Train Epoch: 41 [16000/39785 (0%)]\tLoss: 70.333488\n",
      "Train Epoch: 41 [17000/39785 (0%)]\tLoss: 49.025528\n",
      "Train Epoch: 41 [18000/39785 (0%)]\tLoss: 57.634998\n",
      "Train Epoch: 41 [19000/39785 (0%)]\tLoss: 68.875481\n",
      "Train Epoch: 41 [20000/39785 (1%)]\tLoss: 52.529320\n",
      "Train Epoch: 41 [21000/39785 (1%)]\tLoss: 87.498474\n",
      "Train Epoch: 41 [22000/39785 (1%)]\tLoss: 52.123566\n",
      "Train Epoch: 41 [23000/39785 (1%)]\tLoss: 63.170139\n",
      "Train Epoch: 41 [24000/39785 (1%)]\tLoss: 73.910286\n",
      "Train Epoch: 41 [25000/39785 (1%)]\tLoss: 49.590679\n",
      "Train Epoch: 41 [26000/39785 (1%)]\tLoss: 46.420498\n",
      "Train Epoch: 41 [27000/39785 (1%)]\tLoss: 66.518005\n",
      "Train Epoch: 41 [28000/39785 (1%)]\tLoss: 59.453674\n",
      "Train Epoch: 41 [29000/39785 (1%)]\tLoss: 49.630363\n",
      "Train Epoch: 41 [30000/39785 (1%)]\tLoss: 48.785591\n",
      "Train Epoch: 41 [31000/39785 (1%)]\tLoss: 43.418777\n",
      "Train Epoch: 41 [32000/39785 (1%)]\tLoss: 47.126904\n",
      "Train Epoch: 41 [33000/39785 (1%)]\tLoss: 53.909519\n",
      "Train Epoch: 41 [34000/39785 (1%)]\tLoss: 57.609978\n",
      "Train Epoch: 41 [35000/39785 (1%)]\tLoss: 48.730545\n",
      "Train Epoch: 41 [36000/39785 (1%)]\tLoss: 53.526314\n",
      "Train Epoch: 41 [37000/39785 (1%)]\tLoss: 43.455875\n",
      "Train Epoch: 41 [38000/39785 (1%)]\tLoss: 66.053825\n",
      "Train Epoch: 41 [39000/39785 (1%)]\tLoss: 76.013817\n",
      "39685\n",
      "39785\n",
      "39785\n",
      "39785\n",
      "Train Epoch: 43 [1000/39785 (0%)]\tLoss: 63.353733\n",
      "Train Epoch: 43 [2000/39785 (0%)]\tLoss: 56.566124\n",
      "Train Epoch: 43 [3000/39785 (0%)]\tLoss: 64.655334\n",
      "Train Epoch: 43 [4000/39785 (0%)]\tLoss: 57.724461\n",
      "Train Epoch: 43 [5000/39785 (0%)]\tLoss: 46.065746\n",
      "Train Epoch: 43 [6000/39785 (0%)]\tLoss: 62.375385\n",
      "Train Epoch: 43 [7000/39785 (0%)]\tLoss: 55.432869\n",
      "Train Epoch: 43 [8000/39785 (0%)]\tLoss: 50.437859\n",
      "Train Epoch: 43 [9000/39785 (0%)]\tLoss: 38.669666\n",
      "Train Epoch: 43 [10000/39785 (0%)]\tLoss: 53.368500\n",
      "Train Epoch: 43 [11000/39785 (0%)]\tLoss: 47.543221\n",
      "Train Epoch: 43 [12000/39785 (0%)]\tLoss: 38.873547\n",
      "Train Epoch: 43 [13000/39785 (0%)]\tLoss: 68.286270\n",
      "Train Epoch: 43 [14000/39785 (0%)]\tLoss: 51.099560\n",
      "Train Epoch: 43 [15000/39785 (0%)]\tLoss: 51.490124\n",
      "Train Epoch: 43 [16000/39785 (0%)]\tLoss: 42.264969\n",
      "Train Epoch: 43 [17000/39785 (0%)]\tLoss: 51.701256\n",
      "Train Epoch: 43 [18000/39785 (0%)]\tLoss: 108.607040\n",
      "Train Epoch: 43 [19000/39785 (0%)]\tLoss: 148.018890\n",
      "Train Epoch: 43 [20000/39785 (1%)]\tLoss: 106.394135\n",
      "Train Epoch: 43 [21000/39785 (1%)]\tLoss: 118.933380\n",
      "Train Epoch: 43 [22000/39785 (1%)]\tLoss: 67.872864\n",
      "Train Epoch: 43 [23000/39785 (1%)]\tLoss: 68.548180\n",
      "Train Epoch: 43 [24000/39785 (1%)]\tLoss: 59.223492\n",
      "Train Epoch: 43 [25000/39785 (1%)]\tLoss: 76.876289\n",
      "Train Epoch: 43 [26000/39785 (1%)]\tLoss: 51.175823\n",
      "Train Epoch: 43 [27000/39785 (1%)]\tLoss: 80.689331\n",
      "Train Epoch: 43 [28000/39785 (1%)]\tLoss: 63.167339\n",
      "Train Epoch: 43 [29000/39785 (1%)]\tLoss: 72.332382\n",
      "Train Epoch: 43 [30000/39785 (1%)]\tLoss: 82.880798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 43 [31000/39785 (1%)]\tLoss: 55.441082\n",
      "Train Epoch: 43 [32000/39785 (1%)]\tLoss: 70.763359\n",
      "Train Epoch: 43 [33000/39785 (1%)]\tLoss: 83.316055\n",
      "Train Epoch: 43 [34000/39785 (1%)]\tLoss: 50.247536\n",
      "Train Epoch: 43 [35000/39785 (1%)]\tLoss: 51.767487\n",
      "Train Epoch: 43 [36000/39785 (1%)]\tLoss: 62.485100\n",
      "Train Epoch: 43 [37000/39785 (1%)]\tLoss: 44.350361\n",
      "Train Epoch: 43 [38000/39785 (1%)]\tLoss: 54.388588\n",
      "Train Epoch: 43 [39000/39785 (1%)]\tLoss: 68.870529\n",
      "39700\n",
      "39785\n",
      "Train Epoch: 44 [1000/39785 (0%)]\tLoss: 67.387611\n",
      "Train Epoch: 44 [2000/39785 (0%)]\tLoss: 54.107941\n",
      "Train Epoch: 44 [3000/39785 (0%)]\tLoss: 48.686325\n",
      "Train Epoch: 44 [4000/39785 (0%)]\tLoss: 49.457703\n",
      "Train Epoch: 44 [5000/39785 (0%)]\tLoss: 43.195225\n",
      "Train Epoch: 44 [6000/39785 (0%)]\tLoss: 59.877464\n",
      "Train Epoch: 44 [7000/39785 (0%)]\tLoss: 48.908813\n",
      "Train Epoch: 44 [8000/39785 (0%)]\tLoss: 54.660458\n",
      "Train Epoch: 44 [9000/39785 (0%)]\tLoss: 45.513191\n",
      "Train Epoch: 44 [10000/39785 (0%)]\tLoss: 60.584431\n",
      "Train Epoch: 44 [11000/39785 (0%)]\tLoss: 61.676350\n",
      "Train Epoch: 44 [12000/39785 (0%)]\tLoss: 37.718468\n",
      "Train Epoch: 44 [13000/39785 (0%)]\tLoss: 49.824238\n",
      "Train Epoch: 44 [14000/39785 (0%)]\tLoss: 49.195469\n",
      "Train Epoch: 44 [15000/39785 (0%)]\tLoss: 40.975368\n",
      "Train Epoch: 44 [16000/39785 (0%)]\tLoss: 53.165745\n",
      "Train Epoch: 44 [17000/39785 (0%)]\tLoss: 54.453075\n",
      "Train Epoch: 44 [18000/39785 (0%)]\tLoss: 47.598373\n",
      "Train Epoch: 44 [19000/39785 (0%)]\tLoss: 44.021275\n",
      "Train Epoch: 44 [20000/39785 (1%)]\tLoss: 93.396225\n",
      "Train Epoch: 44 [21000/39785 (1%)]\tLoss: 49.460804\n",
      "Train Epoch: 44 [22000/39785 (1%)]\tLoss: 49.784904\n",
      "Train Epoch: 44 [23000/39785 (1%)]\tLoss: 46.278404\n",
      "Train Epoch: 44 [24000/39785 (1%)]\tLoss: 56.485935\n",
      "Train Epoch: 44 [25000/39785 (1%)]\tLoss: 47.631706\n",
      "Train Epoch: 44 [26000/39785 (1%)]\tLoss: 57.888641\n",
      "Train Epoch: 44 [27000/39785 (1%)]\tLoss: 58.455570\n",
      "Train Epoch: 44 [28000/39785 (1%)]\tLoss: 43.839035\n",
      "Train Epoch: 44 [29000/39785 (1%)]\tLoss: 58.646652\n",
      "Train Epoch: 44 [30000/39785 (1%)]\tLoss: 42.223591\n",
      "Train Epoch: 44 [31000/39785 (1%)]\tLoss: 69.323509\n",
      "Train Epoch: 44 [32000/39785 (1%)]\tLoss: 72.824013\n",
      "Train Epoch: 44 [33000/39785 (1%)]\tLoss: 54.976685\n",
      "Train Epoch: 44 [34000/39785 (1%)]\tLoss: 63.094120\n",
      "Train Epoch: 44 [35000/39785 (1%)]\tLoss: 43.392735\n",
      "Train Epoch: 44 [36000/39785 (1%)]\tLoss: 52.768093\n",
      "Train Epoch: 44 [37000/39785 (1%)]\tLoss: 51.901779\n",
      "Train Epoch: 44 [38000/39785 (1%)]\tLoss: 53.524025\n",
      "Train Epoch: 44 [39000/39785 (1%)]\tLoss: 52.430370\n",
      "39715\n",
      "39785\n",
      "Train Epoch: 45 [1000/39785 (0%)]\tLoss: 32.226761\n",
      "Train Epoch: 45 [2000/39785 (0%)]\tLoss: 45.177631\n",
      "Train Epoch: 45 [3000/39785 (0%)]\tLoss: 154.612167\n",
      "Train Epoch: 45 [4000/39785 (0%)]\tLoss: 40.969181\n",
      "Train Epoch: 45 [5000/39785 (0%)]\tLoss: 43.697292\n",
      "Train Epoch: 45 [6000/39785 (0%)]\tLoss: 36.353142\n",
      "Train Epoch: 45 [7000/39785 (0%)]\tLoss: 45.630173\n",
      "Train Epoch: 45 [8000/39785 (0%)]\tLoss: 65.631523\n",
      "Train Epoch: 45 [9000/39785 (0%)]\tLoss: 44.056694\n",
      "Train Epoch: 45 [10000/39785 (0%)]\tLoss: 56.817398\n",
      "Train Epoch: 45 [11000/39785 (0%)]\tLoss: 42.040192\n",
      "Train Epoch: 45 [12000/39785 (0%)]\tLoss: 38.078297\n",
      "Train Epoch: 45 [13000/39785 (0%)]\tLoss: 42.393993\n",
      "Train Epoch: 45 [14000/39785 (0%)]\tLoss: 43.246132\n",
      "Train Epoch: 45 [15000/39785 (0%)]\tLoss: 51.074703\n",
      "Train Epoch: 45 [16000/39785 (0%)]\tLoss: 39.478683\n",
      "Train Epoch: 45 [17000/39785 (0%)]\tLoss: 53.656624\n",
      "Train Epoch: 45 [18000/39785 (0%)]\tLoss: 52.875145\n",
      "Train Epoch: 45 [19000/39785 (0%)]\tLoss: 49.591774\n",
      "Train Epoch: 45 [20000/39785 (1%)]\tLoss: 36.931171\n",
      "Train Epoch: 45 [21000/39785 (1%)]\tLoss: 45.325180\n",
      "Train Epoch: 45 [22000/39785 (1%)]\tLoss: 52.447845\n",
      "Train Epoch: 45 [23000/39785 (1%)]\tLoss: 65.351318\n",
      "Train Epoch: 45 [24000/39785 (1%)]\tLoss: 38.949097\n",
      "Train Epoch: 45 [25000/39785 (1%)]\tLoss: 35.725700\n",
      "Train Epoch: 45 [26000/39785 (1%)]\tLoss: 59.110641\n",
      "Train Epoch: 45 [27000/39785 (1%)]\tLoss: 57.175705\n",
      "Train Epoch: 45 [28000/39785 (1%)]\tLoss: 37.545856\n",
      "Train Epoch: 45 [29000/39785 (1%)]\tLoss: 43.992176\n",
      "Train Epoch: 45 [30000/39785 (1%)]\tLoss: 40.896053\n",
      "Train Epoch: 45 [31000/39785 (1%)]\tLoss: 56.125126\n",
      "Train Epoch: 45 [32000/39785 (1%)]\tLoss: 73.046593\n",
      "Train Epoch: 45 [33000/39785 (1%)]\tLoss: 50.167278\n",
      "Train Epoch: 45 [34000/39785 (1%)]\tLoss: 46.517334\n",
      "Train Epoch: 45 [35000/39785 (1%)]\tLoss: 36.526043\n",
      "Train Epoch: 45 [36000/39785 (1%)]\tLoss: 53.556324\n",
      "Train Epoch: 45 [37000/39785 (1%)]\tLoss: 43.856251\n",
      "Train Epoch: 45 [38000/39785 (1%)]\tLoss: 46.056458\n",
      "Train Epoch: 45 [39000/39785 (1%)]\tLoss: 55.807758\n",
      "39730\n",
      "39785\n",
      "Train Epoch: 46 [1000/39785 (0%)]\tLoss: 33.830353\n",
      "Train Epoch: 46 [2000/39785 (0%)]\tLoss: 47.916229\n",
      "Train Epoch: 46 [3000/39785 (0%)]\tLoss: 45.679466\n",
      "Train Epoch: 46 [4000/39785 (0%)]\tLoss: 35.704659\n",
      "Train Epoch: 46 [5000/39785 (0%)]\tLoss: 57.351299\n",
      "Train Epoch: 46 [6000/39785 (0%)]\tLoss: 59.360588\n",
      "Train Epoch: 46 [7000/39785 (0%)]\tLoss: 65.493599\n",
      "Train Epoch: 46 [8000/39785 (0%)]\tLoss: 54.986290\n",
      "Train Epoch: 46 [9000/39785 (0%)]\tLoss: 37.317699\n",
      "Train Epoch: 46 [10000/39785 (0%)]\tLoss: 53.621277\n",
      "Train Epoch: 46 [11000/39785 (0%)]\tLoss: 45.137192\n",
      "Train Epoch: 46 [12000/39785 (0%)]\tLoss: 32.546619\n",
      "Train Epoch: 46 [13000/39785 (0%)]\tLoss: 29.682827\n",
      "Train Epoch: 46 [14000/39785 (0%)]\tLoss: 32.119045\n",
      "Train Epoch: 46 [15000/39785 (0%)]\tLoss: 39.642773\n",
      "Train Epoch: 46 [16000/39785 (0%)]\tLoss: 49.415176\n",
      "Train Epoch: 46 [17000/39785 (0%)]\tLoss: 69.062393\n",
      "Train Epoch: 46 [18000/39785 (0%)]\tLoss: 39.404732\n",
      "Train Epoch: 46 [19000/39785 (0%)]\tLoss: 36.642536\n",
      "Train Epoch: 46 [20000/39785 (1%)]\tLoss: 35.863045\n",
      "Train Epoch: 46 [21000/39785 (1%)]\tLoss: 37.685932\n",
      "Train Epoch: 46 [22000/39785 (1%)]\tLoss: 56.451500\n",
      "Train Epoch: 46 [23000/39785 (1%)]\tLoss: 36.600605\n",
      "Train Epoch: 46 [24000/39785 (1%)]\tLoss: 48.123878\n",
      "Train Epoch: 46 [25000/39785 (1%)]\tLoss: 44.264114\n",
      "Train Epoch: 46 [26000/39785 (1%)]\tLoss: 37.208199\n",
      "Train Epoch: 46 [27000/39785 (1%)]\tLoss: 44.183056\n",
      "Train Epoch: 46 [28000/39785 (1%)]\tLoss: 40.153690\n",
      "Train Epoch: 46 [29000/39785 (1%)]\tLoss: 40.518429\n",
      "Train Epoch: 46 [30000/39785 (1%)]\tLoss: 42.166214\n",
      "Train Epoch: 46 [31000/39785 (1%)]\tLoss: 32.706474\n",
      "Train Epoch: 46 [32000/39785 (1%)]\tLoss: 61.093277\n",
      "Train Epoch: 46 [33000/39785 (1%)]\tLoss: 50.658424\n",
      "Train Epoch: 46 [34000/39785 (1%)]\tLoss: 42.915958\n",
      "Train Epoch: 46 [35000/39785 (1%)]\tLoss: 55.003128\n",
      "Train Epoch: 46 [36000/39785 (1%)]\tLoss: 68.927559\n",
      "Train Epoch: 46 [37000/39785 (1%)]\tLoss: 59.499130\n",
      "Train Epoch: 46 [38000/39785 (1%)]\tLoss: 56.043823\n",
      "Train Epoch: 46 [39000/39785 (1%)]\tLoss: 59.135803\n",
      "39745\n",
      "39785\n",
      "Train Epoch: 47 [1000/39785 (0%)]\tLoss: 38.273945\n",
      "Train Epoch: 47 [2000/39785 (0%)]\tLoss: 44.156754\n",
      "Train Epoch: 47 [3000/39785 (0%)]\tLoss: 34.081215\n",
      "Train Epoch: 47 [4000/39785 (0%)]\tLoss: 41.093117\n",
      "Train Epoch: 47 [5000/39785 (0%)]\tLoss: 38.693356\n",
      "Train Epoch: 47 [6000/39785 (0%)]\tLoss: 77.536934\n",
      "Train Epoch: 47 [7000/39785 (0%)]\tLoss: 49.231236\n",
      "Train Epoch: 47 [8000/39785 (0%)]\tLoss: 54.418312\n",
      "Train Epoch: 47 [9000/39785 (0%)]\tLoss: 60.898521\n",
      "Train Epoch: 47 [10000/39785 (0%)]\tLoss: 76.592033\n",
      "Train Epoch: 47 [11000/39785 (0%)]\tLoss: 38.678669\n",
      "Train Epoch: 47 [12000/39785 (0%)]\tLoss: 48.367302\n",
      "Train Epoch: 47 [13000/39785 (0%)]\tLoss: 37.965309\n",
      "Train Epoch: 47 [14000/39785 (0%)]\tLoss: 29.015501\n",
      "Train Epoch: 47 [15000/39785 (0%)]\tLoss: 39.508450\n",
      "Train Epoch: 47 [16000/39785 (0%)]\tLoss: 30.674826\n",
      "Train Epoch: 47 [17000/39785 (0%)]\tLoss: 42.339069\n",
      "Train Epoch: 47 [18000/39785 (0%)]\tLoss: 36.600098\n",
      "Train Epoch: 47 [19000/39785 (0%)]\tLoss: 35.577835\n",
      "Train Epoch: 47 [20000/39785 (1%)]\tLoss: 33.689800\n",
      "Train Epoch: 47 [21000/39785 (1%)]\tLoss: 44.646004\n",
      "Train Epoch: 47 [22000/39785 (1%)]\tLoss: 42.284756\n",
      "Train Epoch: 47 [23000/39785 (1%)]\tLoss: 35.723282\n",
      "Train Epoch: 47 [24000/39785 (1%)]\tLoss: 41.603405\n",
      "Train Epoch: 47 [25000/39785 (1%)]\tLoss: 38.990055\n",
      "Train Epoch: 47 [26000/39785 (1%)]\tLoss: 52.563660\n",
      "Train Epoch: 47 [27000/39785 (1%)]\tLoss: 44.193180\n",
      "Train Epoch: 47 [28000/39785 (1%)]\tLoss: 36.433094\n",
      "Train Epoch: 47 [29000/39785 (1%)]\tLoss: 36.470634\n",
      "Train Epoch: 47 [30000/39785 (1%)]\tLoss: 35.267509\n",
      "Train Epoch: 47 [31000/39785 (1%)]\tLoss: 28.320393\n",
      "Train Epoch: 47 [32000/39785 (1%)]\tLoss: 34.673290\n",
      "Train Epoch: 47 [33000/39785 (1%)]\tLoss: 38.984219\n",
      "Train Epoch: 47 [34000/39785 (1%)]\tLoss: 46.547840\n",
      "Train Epoch: 47 [35000/39785 (1%)]\tLoss: 37.786087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 47 [36000/39785 (1%)]\tLoss: 40.898792\n",
      "Train Epoch: 47 [37000/39785 (1%)]\tLoss: 45.971962\n",
      "Train Epoch: 47 [38000/39785 (1%)]\tLoss: 34.702728\n",
      "Train Epoch: 47 [39000/39785 (1%)]\tLoss: 42.223938\n",
      "39760\n",
      "39785\n",
      "Train Epoch: 48 [1000/39785 (0%)]\tLoss: 43.219185\n",
      "Train Epoch: 48 [2000/39785 (0%)]\tLoss: 34.502762\n",
      "Train Epoch: 48 [3000/39785 (0%)]\tLoss: 41.055058\n",
      "Train Epoch: 48 [4000/39785 (0%)]\tLoss: 31.658493\n",
      "Train Epoch: 48 [5000/39785 (0%)]\tLoss: 39.151028\n",
      "Train Epoch: 48 [6000/39785 (0%)]\tLoss: 229.345078\n",
      "Train Epoch: 48 [7000/39785 (0%)]\tLoss: 40.306435\n",
      "Train Epoch: 48 [8000/39785 (0%)]\tLoss: 37.408676\n",
      "Train Epoch: 48 [9000/39785 (0%)]\tLoss: 33.568874\n",
      "Train Epoch: 48 [10000/39785 (0%)]\tLoss: 33.257336\n",
      "Train Epoch: 48 [11000/39785 (0%)]\tLoss: 61.300510\n",
      "Train Epoch: 48 [12000/39785 (0%)]\tLoss: 41.643398\n",
      "Train Epoch: 48 [13000/39785 (0%)]\tLoss: 45.624577\n",
      "Train Epoch: 48 [14000/39785 (0%)]\tLoss: 42.983139\n",
      "Train Epoch: 48 [15000/39785 (0%)]\tLoss: 29.735014\n",
      "Train Epoch: 48 [16000/39785 (0%)]\tLoss: 29.292336\n",
      "Train Epoch: 48 [17000/39785 (0%)]\tLoss: 44.688263\n",
      "Train Epoch: 48 [18000/39785 (0%)]\tLoss: 37.933117\n",
      "Train Epoch: 48 [19000/39785 (0%)]\tLoss: 36.823757\n",
      "Train Epoch: 48 [20000/39785 (1%)]\tLoss: 32.800579\n",
      "Train Epoch: 48 [21000/39785 (1%)]\tLoss: 42.404678\n",
      "Train Epoch: 48 [22000/39785 (1%)]\tLoss: 37.176785\n",
      "Train Epoch: 48 [23000/39785 (1%)]\tLoss: 31.177540\n",
      "Train Epoch: 48 [24000/39785 (1%)]\tLoss: 35.962811\n",
      "Train Epoch: 48 [25000/39785 (1%)]\tLoss: 38.848255\n",
      "Train Epoch: 48 [26000/39785 (1%)]\tLoss: 30.355211\n",
      "Train Epoch: 48 [27000/39785 (1%)]\tLoss: 47.688782\n",
      "Train Epoch: 48 [28000/39785 (1%)]\tLoss: 47.730148\n",
      "Train Epoch: 48 [29000/39785 (1%)]\tLoss: 30.224129\n",
      "Train Epoch: 48 [30000/39785 (1%)]\tLoss: 36.537914\n",
      "Train Epoch: 48 [31000/39785 (1%)]\tLoss: 36.589523\n",
      "Train Epoch: 48 [32000/39785 (1%)]\tLoss: 46.498512\n",
      "Train Epoch: 48 [33000/39785 (1%)]\tLoss: 52.448154\n",
      "Train Epoch: 48 [34000/39785 (1%)]\tLoss: 49.657841\n",
      "Train Epoch: 48 [35000/39785 (1%)]\tLoss: 38.028809\n",
      "Train Epoch: 48 [36000/39785 (1%)]\tLoss: 30.928091\n",
      "Train Epoch: 48 [37000/39785 (1%)]\tLoss: 38.580570\n",
      "Train Epoch: 48 [38000/39785 (1%)]\tLoss: 51.409645\n",
      "Train Epoch: 48 [39000/39785 (1%)]\tLoss: 41.439598\n",
      "39775\n",
      "39785\n",
      "Train Epoch: 49 [1000/39785 (0%)]\tLoss: 41.330036\n",
      "Train Epoch: 49 [2000/39785 (0%)]\tLoss: 28.672546\n",
      "Train Epoch: 49 [3000/39785 (0%)]\tLoss: 36.604824\n",
      "Train Epoch: 49 [4000/39785 (0%)]\tLoss: 46.451191\n",
      "Train Epoch: 49 [5000/39785 (0%)]\tLoss: 24.270449\n",
      "Train Epoch: 49 [6000/39785 (0%)]\tLoss: 34.116055\n",
      "Train Epoch: 49 [7000/39785 (0%)]\tLoss: 42.423832\n",
      "Train Epoch: 49 [8000/39785 (0%)]\tLoss: 53.714394\n",
      "Train Epoch: 49 [9000/39785 (0%)]\tLoss: 151.943665\n",
      "Train Epoch: 49 [10000/39785 (0%)]\tLoss: 37.217930\n",
      "Train Epoch: 49 [11000/39785 (0%)]\tLoss: 38.401695\n",
      "Train Epoch: 49 [12000/39785 (0%)]\tLoss: 47.644871\n",
      "Train Epoch: 49 [13000/39785 (0%)]\tLoss: 37.205799\n",
      "Train Epoch: 49 [14000/39785 (0%)]\tLoss: 50.916710\n",
      "Train Epoch: 49 [15000/39785 (0%)]\tLoss: 34.733078\n",
      "Train Epoch: 49 [16000/39785 (0%)]\tLoss: 36.692932\n",
      "Train Epoch: 49 [17000/39785 (0%)]\tLoss: 32.362244\n",
      "Train Epoch: 49 [18000/39785 (0%)]\tLoss: 41.576344\n",
      "Train Epoch: 49 [19000/39785 (0%)]\tLoss: 58.252323\n",
      "Train Epoch: 49 [20000/39785 (1%)]\tLoss: 36.430908\n",
      "Train Epoch: 49 [21000/39785 (1%)]\tLoss: 45.689995\n",
      "Train Epoch: 49 [22000/39785 (1%)]\tLoss: 34.166611\n",
      "Train Epoch: 49 [23000/39785 (1%)]\tLoss: 43.354519\n",
      "Train Epoch: 49 [24000/39785 (1%)]\tLoss: 42.727013\n",
      "Train Epoch: 49 [25000/39785 (1%)]\tLoss: 29.807272\n",
      "Train Epoch: 49 [26000/39785 (1%)]\tLoss: 29.761736\n",
      "Train Epoch: 49 [27000/39785 (1%)]\tLoss: 41.376820\n",
      "Train Epoch: 49 [28000/39785 (1%)]\tLoss: 38.638870\n",
      "Train Epoch: 49 [29000/39785 (1%)]\tLoss: 58.362591\n",
      "Train Epoch: 49 [30000/39785 (1%)]\tLoss: 34.290485\n",
      "Train Epoch: 49 [31000/39785 (1%)]\tLoss: 27.904562\n",
      "Train Epoch: 49 [32000/39785 (1%)]\tLoss: 39.646061\n",
      "Train Epoch: 49 [33000/39785 (1%)]\tLoss: 48.696342\n",
      "Train Epoch: 49 [34000/39785 (1%)]\tLoss: 37.045948\n",
      "Train Epoch: 49 [35000/39785 (1%)]\tLoss: 37.352562\n",
      "Train Epoch: 49 [36000/39785 (1%)]\tLoss: 57.402382\n",
      "Train Epoch: 49 [37000/39785 (1%)]\tLoss: 53.309990\n",
      "Train Epoch: 49 [38000/39785 (1%)]\tLoss: 44.400478\n",
      "Train Epoch: 49 [39000/39785 (1%)]\tLoss: 53.712257\n",
      "39690\n",
      "39785\n",
      "Train Epoch: 50 [1000/39785 (0%)]\tLoss: 39.030766\n",
      "Train Epoch: 50 [2000/39785 (0%)]\tLoss: 35.629250\n",
      "Train Epoch: 50 [3000/39785 (0%)]\tLoss: 44.028057\n",
      "Train Epoch: 50 [4000/39785 (0%)]\tLoss: 50.482384\n",
      "Train Epoch: 50 [5000/39785 (0%)]\tLoss: 36.782291\n",
      "Train Epoch: 50 [6000/39785 (0%)]\tLoss: 60.279572\n",
      "Train Epoch: 50 [7000/39785 (0%)]\tLoss: 35.341881\n",
      "Train Epoch: 50 [8000/39785 (0%)]\tLoss: 33.121078\n",
      "Train Epoch: 50 [9000/39785 (0%)]\tLoss: 38.046322\n",
      "Train Epoch: 50 [10000/39785 (0%)]\tLoss: 53.047001\n",
      "Train Epoch: 50 [11000/39785 (0%)]\tLoss: 31.434372\n",
      "Train Epoch: 50 [12000/39785 (0%)]\tLoss: 43.813564\n",
      "Train Epoch: 50 [13000/39785 (0%)]\tLoss: 36.264107\n",
      "Train Epoch: 50 [14000/39785 (0%)]\tLoss: 33.790997\n",
      "Train Epoch: 50 [15000/39785 (0%)]\tLoss: 34.298363\n",
      "Train Epoch: 50 [16000/39785 (0%)]\tLoss: 47.195850\n",
      "Train Epoch: 50 [17000/39785 (0%)]\tLoss: 24.638361\n",
      "Train Epoch: 50 [18000/39785 (0%)]\tLoss: 40.137726\n",
      "Train Epoch: 50 [19000/39785 (0%)]\tLoss: 31.360723\n",
      "Train Epoch: 50 [20000/39785 (1%)]\tLoss: 31.348629\n",
      "Train Epoch: 50 [21000/39785 (1%)]\tLoss: 37.492744\n",
      "Train Epoch: 50 [22000/39785 (1%)]\tLoss: 46.502548\n",
      "Train Epoch: 50 [23000/39785 (1%)]\tLoss: 40.792576\n",
      "Train Epoch: 50 [24000/39785 (1%)]\tLoss: 31.882435\n",
      "Train Epoch: 50 [25000/39785 (1%)]\tLoss: 38.682629\n",
      "Train Epoch: 50 [26000/39785 (1%)]\tLoss: 30.587196\n",
      "Train Epoch: 50 [27000/39785 (1%)]\tLoss: 43.772877\n",
      "Train Epoch: 50 [28000/39785 (1%)]\tLoss: 34.527546\n",
      "Train Epoch: 50 [29000/39785 (1%)]\tLoss: 35.693237\n",
      "Train Epoch: 50 [30000/39785 (1%)]\tLoss: 27.667229\n",
      "Train Epoch: 50 [31000/39785 (1%)]\tLoss: 35.313511\n",
      "Train Epoch: 50 [32000/39785 (1%)]\tLoss: 44.024723\n",
      "Train Epoch: 50 [33000/39785 (1%)]\tLoss: 48.034748\n",
      "Train Epoch: 50 [34000/39785 (1%)]\tLoss: 54.968147\n",
      "Train Epoch: 50 [35000/39785 (1%)]\tLoss: 49.436531\n",
      "Train Epoch: 50 [36000/39785 (1%)]\tLoss: 49.317997\n",
      "Train Epoch: 50 [37000/39785 (1%)]\tLoss: 63.273865\n",
      "Train Epoch: 50 [38000/39785 (1%)]\tLoss: 61.601475\n",
      "Train Epoch: 50 [39000/39785 (1%)]\tLoss: 40.346458\n",
      "39705\n",
      "39785\n",
      "Train Epoch: 51 [1000/39785 (0%)]\tLoss: 245.633362\n",
      "Train Epoch: 51 [2000/39785 (0%)]\tLoss: 1290.638184\n",
      "Train Epoch: 51 [3000/39785 (0%)]\tLoss: 975.441162\n",
      "Train Epoch: 51 [4000/39785 (0%)]\tLoss: 370.157867\n",
      "Train Epoch: 51 [5000/39785 (0%)]\tLoss: 212.847961\n",
      "Train Epoch: 51 [6000/39785 (0%)]\tLoss: 211.512665\n",
      "Train Epoch: 51 [7000/39785 (0%)]\tLoss: 165.600113\n",
      "Train Epoch: 51 [8000/39785 (0%)]\tLoss: 118.147331\n",
      "Train Epoch: 51 [9000/39785 (0%)]\tLoss: 102.913536\n",
      "Train Epoch: 51 [10000/39785 (0%)]\tLoss: 83.549240\n",
      "Train Epoch: 51 [11000/39785 (0%)]\tLoss: 79.583832\n",
      "Train Epoch: 51 [12000/39785 (0%)]\tLoss: 81.242493\n",
      "Train Epoch: 51 [13000/39785 (0%)]\tLoss: 56.132122\n",
      "Train Epoch: 51 [14000/39785 (0%)]\tLoss: 88.589600\n",
      "Train Epoch: 51 [15000/39785 (0%)]\tLoss: 62.752037\n",
      "Train Epoch: 51 [16000/39785 (0%)]\tLoss: 51.518368\n",
      "Train Epoch: 51 [17000/39785 (0%)]\tLoss: 56.783573\n",
      "Train Epoch: 51 [18000/39785 (0%)]\tLoss: 63.921070\n",
      "Train Epoch: 51 [19000/39785 (0%)]\tLoss: 79.421753\n",
      "Train Epoch: 51 [20000/39785 (1%)]\tLoss: 57.413986\n",
      "Train Epoch: 51 [21000/39785 (1%)]\tLoss: 60.585175\n",
      "Train Epoch: 51 [22000/39785 (1%)]\tLoss: 58.157043\n",
      "Train Epoch: 51 [23000/39785 (1%)]\tLoss: 49.365593\n",
      "Train Epoch: 51 [24000/39785 (1%)]\tLoss: 40.158810\n",
      "Train Epoch: 51 [25000/39785 (1%)]\tLoss: 49.990444\n",
      "Train Epoch: 51 [26000/39785 (1%)]\tLoss: 38.120300\n",
      "Train Epoch: 51 [27000/39785 (1%)]\tLoss: 45.917286\n",
      "Train Epoch: 51 [28000/39785 (1%)]\tLoss: 47.426804\n",
      "Train Epoch: 51 [29000/39785 (1%)]\tLoss: 44.105030\n",
      "Train Epoch: 51 [30000/39785 (1%)]\tLoss: 37.507378\n",
      "Train Epoch: 51 [31000/39785 (1%)]\tLoss: 36.557926\n",
      "Train Epoch: 51 [32000/39785 (1%)]\tLoss: 32.688835\n",
      "Train Epoch: 51 [33000/39785 (1%)]\tLoss: 33.413982\n",
      "Train Epoch: 51 [34000/39785 (1%)]\tLoss: 47.129299\n",
      "Train Epoch: 51 [35000/39785 (1%)]\tLoss: 35.773319\n",
      "Train Epoch: 51 [36000/39785 (1%)]\tLoss: 40.590256\n",
      "Train Epoch: 51 [37000/39785 (1%)]\tLoss: 50.250336\n",
      "Train Epoch: 51 [38000/39785 (1%)]\tLoss: 52.172821\n",
      "Train Epoch: 51 [39000/39785 (1%)]\tLoss: 37.146942\n",
      "39720\n",
      "39785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 52 [1000/39785 (0%)]\tLoss: 38.039146\n",
      "Train Epoch: 52 [2000/39785 (0%)]\tLoss: 31.030167\n",
      "Train Epoch: 52 [3000/39785 (0%)]\tLoss: 39.226330\n",
      "Train Epoch: 52 [4000/39785 (0%)]\tLoss: 36.567795\n",
      "Train Epoch: 52 [5000/39785 (0%)]\tLoss: 33.601250\n",
      "Train Epoch: 52 [6000/39785 (0%)]\tLoss: 28.641104\n",
      "Train Epoch: 52 [7000/39785 (0%)]\tLoss: 27.337687\n",
      "Train Epoch: 52 [8000/39785 (0%)]\tLoss: 43.180580\n",
      "Train Epoch: 52 [9000/39785 (0%)]\tLoss: 45.932602\n",
      "Train Epoch: 52 [10000/39785 (0%)]\tLoss: 40.116055\n",
      "Train Epoch: 52 [11000/39785 (0%)]\tLoss: 36.361622\n",
      "Train Epoch: 52 [12000/39785 (0%)]\tLoss: 38.304241\n",
      "Train Epoch: 52 [13000/39785 (0%)]\tLoss: 29.187811\n",
      "Train Epoch: 52 [14000/39785 (0%)]\tLoss: 33.373589\n",
      "Train Epoch: 52 [15000/39785 (0%)]\tLoss: 27.518009\n",
      "Train Epoch: 52 [16000/39785 (0%)]\tLoss: 33.967640\n",
      "Train Epoch: 52 [17000/39785 (0%)]\tLoss: 41.698776\n",
      "Train Epoch: 52 [18000/39785 (0%)]\tLoss: 47.840401\n",
      "Train Epoch: 52 [19000/39785 (0%)]\tLoss: 43.202614\n",
      "Train Epoch: 52 [20000/39785 (1%)]\tLoss: 33.501270\n",
      "Train Epoch: 52 [21000/39785 (1%)]\tLoss: 38.126930\n",
      "Train Epoch: 52 [22000/39785 (1%)]\tLoss: 35.162750\n",
      "Train Epoch: 52 [23000/39785 (1%)]\tLoss: 52.933216\n",
      "Train Epoch: 52 [24000/39785 (1%)]\tLoss: 39.443275\n",
      "Train Epoch: 52 [25000/39785 (1%)]\tLoss: 45.812271\n",
      "Train Epoch: 52 [26000/39785 (1%)]\tLoss: 33.513096\n",
      "Train Epoch: 52 [27000/39785 (1%)]\tLoss: 30.105570\n",
      "Train Epoch: 52 [28000/39785 (1%)]\tLoss: 115.162903\n",
      "Train Epoch: 52 [29000/39785 (1%)]\tLoss: 36.799877\n",
      "Train Epoch: 52 [30000/39785 (1%)]\tLoss: 79.641243\n",
      "Train Epoch: 52 [31000/39785 (1%)]\tLoss: 44.683342\n",
      "Train Epoch: 52 [32000/39785 (1%)]\tLoss: 41.861828\n",
      "Train Epoch: 52 [33000/39785 (1%)]\tLoss: 36.386410\n",
      "Train Epoch: 52 [34000/39785 (1%)]\tLoss: 25.169800\n",
      "Train Epoch: 52 [35000/39785 (1%)]\tLoss: 40.090363\n",
      "Train Epoch: 52 [36000/39785 (1%)]\tLoss: 42.937912\n",
      "Train Epoch: 52 [37000/39785 (1%)]\tLoss: 33.451618\n",
      "Train Epoch: 52 [38000/39785 (1%)]\tLoss: 42.912094\n",
      "Train Epoch: 52 [39000/39785 (1%)]\tLoss: 38.099072\n",
      "39735\n",
      "39785\n",
      "Train Epoch: 53 [1000/39785 (0%)]\tLoss: 30.935684\n",
      "Train Epoch: 53 [2000/39785 (0%)]\tLoss: 26.749617\n",
      "Train Epoch: 53 [3000/39785 (0%)]\tLoss: 25.850412\n",
      "Train Epoch: 53 [4000/39785 (0%)]\tLoss: 27.773577\n",
      "Train Epoch: 53 [5000/39785 (0%)]\tLoss: 32.169285\n",
      "Train Epoch: 53 [6000/39785 (0%)]\tLoss: 26.720512\n",
      "Train Epoch: 53 [7000/39785 (0%)]\tLoss: 33.131790\n",
      "Train Epoch: 53 [8000/39785 (0%)]\tLoss: 32.300785\n",
      "Train Epoch: 53 [9000/39785 (0%)]\tLoss: 26.212700\n",
      "Train Epoch: 53 [10000/39785 (0%)]\tLoss: 31.032681\n",
      "Train Epoch: 53 [11000/39785 (0%)]\tLoss: 26.894966\n",
      "Train Epoch: 53 [12000/39785 (0%)]\tLoss: 18.933193\n",
      "Train Epoch: 53 [13000/39785 (0%)]\tLoss: 35.782722\n",
      "Train Epoch: 53 [14000/39785 (0%)]\tLoss: 27.082575\n",
      "Train Epoch: 53 [15000/39785 (0%)]\tLoss: 27.282667\n",
      "Train Epoch: 53 [16000/39785 (0%)]\tLoss: 30.920380\n",
      "Train Epoch: 53 [17000/39785 (0%)]\tLoss: 36.591450\n",
      "Train Epoch: 53 [18000/39785 (0%)]\tLoss: 50.717117\n",
      "Train Epoch: 53 [19000/39785 (0%)]\tLoss: 32.265739\n",
      "Train Epoch: 53 [20000/39785 (1%)]\tLoss: 30.021923\n",
      "Train Epoch: 53 [21000/39785 (1%)]\tLoss: 25.626738\n",
      "Train Epoch: 53 [22000/39785 (1%)]\tLoss: 22.417484\n",
      "Train Epoch: 53 [23000/39785 (1%)]\tLoss: 22.724390\n",
      "Train Epoch: 53 [24000/39785 (1%)]\tLoss: 23.477896\n",
      "Train Epoch: 53 [25000/39785 (1%)]\tLoss: 26.282034\n",
      "Train Epoch: 53 [26000/39785 (1%)]\tLoss: 30.383244\n",
      "Train Epoch: 53 [27000/39785 (1%)]\tLoss: 24.868771\n",
      "Train Epoch: 53 [28000/39785 (1%)]\tLoss: 64.492424\n",
      "Train Epoch: 53 [29000/39785 (1%)]\tLoss: 92.715958\n",
      "Train Epoch: 53 [30000/39785 (1%)]\tLoss: 35.353230\n",
      "Train Epoch: 53 [31000/39785 (1%)]\tLoss: 26.451946\n",
      "Train Epoch: 53 [32000/39785 (1%)]\tLoss: 29.269083\n",
      "Train Epoch: 53 [33000/39785 (1%)]\tLoss: 21.387524\n",
      "Train Epoch: 53 [34000/39785 (1%)]\tLoss: 21.906258\n",
      "Train Epoch: 53 [35000/39785 (1%)]\tLoss: 32.077908\n",
      "Train Epoch: 53 [36000/39785 (1%)]\tLoss: 25.747593\n",
      "Train Epoch: 53 [37000/39785 (1%)]\tLoss: 24.909128\n",
      "Train Epoch: 53 [38000/39785 (1%)]\tLoss: 28.247156\n",
      "Train Epoch: 53 [39000/39785 (1%)]\tLoss: 35.585445\n",
      "39750\n",
      "39785\n",
      "Train Epoch: 54 [1000/39785 (0%)]\tLoss: 38.301025\n",
      "Train Epoch: 54 [2000/39785 (0%)]\tLoss: 29.294937\n",
      "Train Epoch: 54 [3000/39785 (0%)]\tLoss: 29.747690\n",
      "Train Epoch: 54 [4000/39785 (0%)]\tLoss: 25.025486\n",
      "Train Epoch: 54 [5000/39785 (0%)]\tLoss: 27.431581\n",
      "Train Epoch: 54 [6000/39785 (0%)]\tLoss: 22.694452\n",
      "Train Epoch: 54 [7000/39785 (0%)]\tLoss: 21.327778\n",
      "Train Epoch: 54 [8000/39785 (0%)]\tLoss: 41.254696\n",
      "Train Epoch: 54 [9000/39785 (0%)]\tLoss: 22.047335\n",
      "Train Epoch: 54 [10000/39785 (0%)]\tLoss: 26.420328\n",
      "Train Epoch: 54 [11000/39785 (0%)]\tLoss: 28.410940\n",
      "Train Epoch: 54 [12000/39785 (0%)]\tLoss: 22.802069\n",
      "Train Epoch: 54 [13000/39785 (0%)]\tLoss: 31.644939\n",
      "Train Epoch: 54 [14000/39785 (0%)]\tLoss: 23.006321\n",
      "Train Epoch: 54 [15000/39785 (0%)]\tLoss: 26.353680\n",
      "Train Epoch: 54 [16000/39785 (0%)]\tLoss: 26.631203\n",
      "Train Epoch: 54 [17000/39785 (0%)]\tLoss: 23.190664\n",
      "Train Epoch: 54 [18000/39785 (0%)]\tLoss: 26.910025\n",
      "Train Epoch: 54 [19000/39785 (0%)]\tLoss: 25.698818\n",
      "Train Epoch: 54 [20000/39785 (1%)]\tLoss: 23.420612\n",
      "Train Epoch: 54 [21000/39785 (1%)]\tLoss: 23.000835\n",
      "Train Epoch: 54 [22000/39785 (1%)]\tLoss: 29.026093\n",
      "Train Epoch: 54 [23000/39785 (1%)]\tLoss: 1100.053955\n",
      "Train Epoch: 54 [24000/39785 (1%)]\tLoss: 46.140652\n",
      "Train Epoch: 54 [25000/39785 (1%)]\tLoss: 37.831635\n",
      "Train Epoch: 54 [26000/39785 (1%)]\tLoss: 27.915199\n",
      "Train Epoch: 54 [27000/39785 (1%)]\tLoss: 34.284264\n",
      "Train Epoch: 54 [28000/39785 (1%)]\tLoss: 33.783222\n",
      "Train Epoch: 54 [29000/39785 (1%)]\tLoss: 31.164474\n",
      "Train Epoch: 54 [30000/39785 (1%)]\tLoss: 25.536428\n",
      "Train Epoch: 54 [31000/39785 (1%)]\tLoss: 31.094143\n",
      "Train Epoch: 54 [32000/39785 (1%)]\tLoss: 35.221264\n",
      "Train Epoch: 54 [33000/39785 (1%)]\tLoss: 48.122154\n",
      "Train Epoch: 54 [34000/39785 (1%)]\tLoss: 30.693888\n",
      "Train Epoch: 54 [35000/39785 (1%)]\tLoss: 31.075033\n",
      "Train Epoch: 54 [36000/39785 (1%)]\tLoss: 28.720274\n",
      "Train Epoch: 54 [37000/39785 (1%)]\tLoss: 34.185329\n",
      "Train Epoch: 54 [38000/39785 (1%)]\tLoss: 27.552822\n",
      "Train Epoch: 54 [39000/39785 (1%)]\tLoss: 24.004080\n",
      "39765\n",
      "39785\n",
      "Train Epoch: 55 [1000/39785 (0%)]\tLoss: 26.820412\n",
      "Train Epoch: 55 [2000/39785 (0%)]\tLoss: 22.456963\n",
      "Train Epoch: 55 [3000/39785 (0%)]\tLoss: 20.800056\n",
      "Train Epoch: 55 [4000/39785 (0%)]\tLoss: 20.737953\n",
      "Train Epoch: 55 [5000/39785 (0%)]\tLoss: 38.617222\n",
      "Train Epoch: 55 [6000/39785 (0%)]\tLoss: 30.537565\n",
      "Train Epoch: 55 [7000/39785 (0%)]\tLoss: 21.307657\n",
      "Train Epoch: 55 [8000/39785 (0%)]\tLoss: 24.560526\n",
      "Train Epoch: 55 [9000/39785 (0%)]\tLoss: 21.507814\n",
      "Train Epoch: 55 [10000/39785 (0%)]\tLoss: 19.101578\n",
      "Train Epoch: 55 [11000/39785 (0%)]\tLoss: 30.676962\n",
      "Train Epoch: 55 [12000/39785 (0%)]\tLoss: 20.019526\n",
      "Train Epoch: 55 [13000/39785 (0%)]\tLoss: 30.597519\n",
      "Train Epoch: 55 [14000/39785 (0%)]\tLoss: 27.169111\n",
      "Train Epoch: 55 [15000/39785 (0%)]\tLoss: 38.400932\n",
      "Train Epoch: 55 [16000/39785 (0%)]\tLoss: 21.111471\n",
      "Train Epoch: 55 [17000/39785 (0%)]\tLoss: 26.386221\n",
      "Train Epoch: 55 [18000/39785 (0%)]\tLoss: 22.191681\n",
      "Train Epoch: 55 [19000/39785 (0%)]\tLoss: 23.658970\n",
      "Train Epoch: 55 [20000/39785 (1%)]\tLoss: 27.168459\n",
      "Train Epoch: 55 [21000/39785 (1%)]\tLoss: 32.701488\n",
      "Train Epoch: 55 [22000/39785 (1%)]\tLoss: 44.660763\n",
      "Train Epoch: 55 [23000/39785 (1%)]\tLoss: 18.578144\n",
      "Train Epoch: 55 [24000/39785 (1%)]\tLoss: 26.849394\n",
      "Train Epoch: 55 [25000/39785 (1%)]\tLoss: 26.129923\n",
      "Train Epoch: 55 [26000/39785 (1%)]\tLoss: 24.893652\n",
      "Train Epoch: 55 [27000/39785 (1%)]\tLoss: 29.704638\n",
      "Train Epoch: 55 [28000/39785 (1%)]\tLoss: 21.593313\n",
      "Train Epoch: 55 [29000/39785 (1%)]\tLoss: 23.262680\n",
      "Train Epoch: 55 [30000/39785 (1%)]\tLoss: 24.529600\n",
      "Train Epoch: 55 [31000/39785 (1%)]\tLoss: 22.473238\n",
      "Train Epoch: 55 [32000/39785 (1%)]\tLoss: 35.434097\n",
      "Train Epoch: 55 [33000/39785 (1%)]\tLoss: 28.401802\n",
      "Train Epoch: 55 [34000/39785 (1%)]\tLoss: 34.122269\n",
      "Train Epoch: 55 [35000/39785 (1%)]\tLoss: 23.281935\n",
      "Train Epoch: 55 [36000/39785 (1%)]\tLoss: 37.065605\n",
      "Train Epoch: 55 [37000/39785 (1%)]\tLoss: 28.325369\n",
      "Train Epoch: 55 [38000/39785 (1%)]\tLoss: 30.771664\n",
      "Train Epoch: 55 [39000/39785 (1%)]\tLoss: 25.156414\n",
      "39780\n",
      "39785\n",
      "Train Epoch: 56 [1000/39785 (0%)]\tLoss: 40.346943\n",
      "Train Epoch: 56 [2000/39785 (0%)]\tLoss: 23.813725\n",
      "Train Epoch: 56 [3000/39785 (0%)]\tLoss: 19.729685\n",
      "Train Epoch: 56 [4000/39785 (0%)]\tLoss: 18.700098\n",
      "Train Epoch: 56 [5000/39785 (0%)]\tLoss: 19.951433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 56 [6000/39785 (0%)]\tLoss: 29.782087\n",
      "Train Epoch: 56 [7000/39785 (0%)]\tLoss: 24.443213\n",
      "Train Epoch: 56 [8000/39785 (0%)]\tLoss: 22.717445\n",
      "Train Epoch: 56 [9000/39785 (0%)]\tLoss: 25.146513\n",
      "Train Epoch: 56 [10000/39785 (0%)]\tLoss: 23.430019\n",
      "Train Epoch: 56 [11000/39785 (0%)]\tLoss: 25.108603\n",
      "Train Epoch: 56 [12000/39785 (0%)]\tLoss: 348.366119\n",
      "Train Epoch: 56 [13000/39785 (0%)]\tLoss: 25.848816\n",
      "Train Epoch: 56 [14000/39785 (0%)]\tLoss: 18.311773\n",
      "Train Epoch: 56 [15000/39785 (0%)]\tLoss: 24.141392\n",
      "Train Epoch: 56 [16000/39785 (0%)]\tLoss: 27.882996\n",
      "Train Epoch: 56 [17000/39785 (0%)]\tLoss: 25.426882\n",
      "Train Epoch: 56 [18000/39785 (0%)]\tLoss: 21.784637\n",
      "Train Epoch: 56 [19000/39785 (0%)]\tLoss: 23.197939\n",
      "Train Epoch: 56 [20000/39785 (1%)]\tLoss: 18.033220\n",
      "Train Epoch: 56 [21000/39785 (1%)]\tLoss: 19.812561\n",
      "Train Epoch: 56 [22000/39785 (1%)]\tLoss: 22.534891\n",
      "Train Epoch: 56 [23000/39785 (1%)]\tLoss: 21.964024\n",
      "Train Epoch: 56 [24000/39785 (1%)]\tLoss: 18.788759\n",
      "Train Epoch: 56 [25000/39785 (1%)]\tLoss: 23.283951\n",
      "Train Epoch: 56 [26000/39785 (1%)]\tLoss: 22.836489\n",
      "Train Epoch: 56 [27000/39785 (1%)]\tLoss: 23.572117\n",
      "Train Epoch: 56 [28000/39785 (1%)]\tLoss: 18.990105\n",
      "Train Epoch: 56 [29000/39785 (1%)]\tLoss: 29.902483\n",
      "Train Epoch: 56 [30000/39785 (1%)]\tLoss: 16.541397\n",
      "Train Epoch: 56 [31000/39785 (1%)]\tLoss: 23.624159\n",
      "Train Epoch: 56 [32000/39785 (1%)]\tLoss: 18.775612\n",
      "Train Epoch: 56 [33000/39785 (1%)]\tLoss: 23.158756\n",
      "Train Epoch: 56 [34000/39785 (1%)]\tLoss: 28.654356\n",
      "Train Epoch: 56 [35000/39785 (1%)]\tLoss: 33.441929\n",
      "Train Epoch: 56 [36000/39785 (1%)]\tLoss: 27.516497\n",
      "Train Epoch: 56 [37000/39785 (1%)]\tLoss: 33.032768\n",
      "Train Epoch: 56 [38000/39785 (1%)]\tLoss: 23.497335\n",
      "Train Epoch: 56 [39000/39785 (1%)]\tLoss: 22.804968\n",
      "39695\n",
      "39785\n",
      "Train Epoch: 57 [1000/39785 (0%)]\tLoss: 25.039886\n",
      "Train Epoch: 57 [2000/39785 (0%)]\tLoss: 25.011707\n",
      "Train Epoch: 57 [3000/39785 (0%)]\tLoss: 32.049129\n",
      "Train Epoch: 57 [4000/39785 (0%)]\tLoss: 24.985491\n",
      "Train Epoch: 57 [5000/39785 (0%)]\tLoss: 25.667234\n",
      "Train Epoch: 57 [6000/39785 (0%)]\tLoss: 19.750296\n",
      "Train Epoch: 57 [7000/39785 (0%)]\tLoss: 33.184429\n",
      "Train Epoch: 57 [8000/39785 (0%)]\tLoss: 21.804970\n",
      "Train Epoch: 57 [9000/39785 (0%)]\tLoss: 25.020594\n",
      "Train Epoch: 57 [10000/39785 (0%)]\tLoss: 26.220009\n",
      "Train Epoch: 57 [11000/39785 (0%)]\tLoss: 37.190399\n",
      "Train Epoch: 57 [12000/39785 (0%)]\tLoss: 20.889578\n",
      "Train Epoch: 57 [13000/39785 (0%)]\tLoss: 27.368891\n",
      "Train Epoch: 57 [14000/39785 (0%)]\tLoss: 26.183420\n",
      "Train Epoch: 57 [15000/39785 (0%)]\tLoss: 24.276604\n",
      "Train Epoch: 57 [16000/39785 (0%)]\tLoss: 33.221066\n",
      "Train Epoch: 57 [17000/39785 (0%)]\tLoss: 28.273907\n",
      "Train Epoch: 57 [18000/39785 (0%)]\tLoss: 17.077488\n",
      "Train Epoch: 57 [19000/39785 (0%)]\tLoss: 22.683292\n",
      "Train Epoch: 57 [20000/39785 (1%)]\tLoss: 22.721943\n",
      "Train Epoch: 57 [21000/39785 (1%)]\tLoss: 26.080614\n",
      "Train Epoch: 57 [22000/39785 (1%)]\tLoss: 21.812300\n",
      "Train Epoch: 57 [23000/39785 (1%)]\tLoss: 37.637207\n",
      "Train Epoch: 57 [24000/39785 (1%)]\tLoss: 21.816526\n",
      "Train Epoch: 57 [25000/39785 (1%)]\tLoss: 20.450565\n",
      "Train Epoch: 57 [26000/39785 (1%)]\tLoss: 25.512480\n",
      "Train Epoch: 57 [27000/39785 (1%)]\tLoss: 21.828575\n",
      "Train Epoch: 57 [28000/39785 (1%)]\tLoss: 20.260138\n",
      "Train Epoch: 57 [29000/39785 (1%)]\tLoss: 32.379028\n",
      "Train Epoch: 57 [30000/39785 (1%)]\tLoss: 25.099968\n",
      "Train Epoch: 57 [31000/39785 (1%)]\tLoss: 27.966480\n",
      "Train Epoch: 57 [32000/39785 (1%)]\tLoss: 25.878149\n",
      "Train Epoch: 57 [33000/39785 (1%)]\tLoss: 23.647371\n",
      "Train Epoch: 57 [34000/39785 (1%)]\tLoss: 24.067379\n",
      "Train Epoch: 57 [35000/39785 (1%)]\tLoss: 21.702301\n",
      "Train Epoch: 57 [36000/39785 (1%)]\tLoss: 28.761503\n",
      "Train Epoch: 57 [37000/39785 (1%)]\tLoss: 36.000648\n",
      "Train Epoch: 57 [38000/39785 (1%)]\tLoss: 31.441282\n",
      "Train Epoch: 57 [39000/39785 (1%)]\tLoss: 26.025558\n",
      "39710\n",
      "39785\n",
      "Train Epoch: 58 [1000/39785 (0%)]\tLoss: 28.076509\n",
      "Train Epoch: 58 [2000/39785 (0%)]\tLoss: 26.691439\n",
      "Train Epoch: 58 [3000/39785 (0%)]\tLoss: 21.993858\n",
      "Train Epoch: 58 [4000/39785 (0%)]\tLoss: 18.267784\n",
      "Train Epoch: 58 [5000/39785 (0%)]\tLoss: 26.297415\n",
      "Train Epoch: 58 [6000/39785 (0%)]\tLoss: 22.787678\n",
      "Train Epoch: 58 [7000/39785 (0%)]\tLoss: 28.706373\n",
      "Train Epoch: 58 [8000/39785 (0%)]\tLoss: 22.808325\n",
      "Train Epoch: 58 [9000/39785 (0%)]\tLoss: 36.795284\n",
      "Train Epoch: 58 [10000/39785 (0%)]\tLoss: 28.987705\n",
      "Train Epoch: 58 [11000/39785 (0%)]\tLoss: 27.497620\n",
      "Train Epoch: 58 [12000/39785 (0%)]\tLoss: 38.966774\n",
      "Train Epoch: 58 [13000/39785 (0%)]\tLoss: 28.732668\n",
      "Train Epoch: 58 [14000/39785 (0%)]\tLoss: 25.281929\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-7791b9d3356e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_hard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecr_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# test(model_hard, device, test_set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-8503584ebb6d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_set, batch_size, optimizer, epoch, per_epoch, decr_rate)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-140-8503584ebb6d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# x = self.gated(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m#x = self.pool2(self.conv2(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# tf.reshape(x,[batchSize,-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/se3cnn-0.0.0-py3.7.egg/se3cnn/convolution.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "device = torch.device('cpu')\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# matrix_size = 16\n",
    "# train_set = untransformed(50, matrix_size)\n",
    "'''\n",
    "test_set = [(a_i, torch.tensor([1,1,1], dtype=torch.float32)), \n",
    "            (rotation_1(a_i), torch.tensor([-1,1,1], dtype=torch.float32)), \n",
    "            (rotation_1(rotation_1(a_i)), torch.tensor([-1,1,-1], dtype=torch.float32)), \n",
    "            (rotation_1(rotation_1(rotation_1(a_i))), torch.tensor([1,1,-1], dtype=torch.float32))]\n",
    "'''\n",
    "\n",
    "model_hard = EquiNet().to(device)\n",
    "learning_rate = 5e-3\n",
    "optimizer = torch.optim.Adam(model_hard.parameters(), lr=learning_rate)\n",
    "\n",
    "per_epoch = 150\n",
    "decr_rate = 0.99\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model_hard, device, train_set, batch_size, optimizer, epoch, per_epoch, decr_rate)\n",
    "    # test(model_hard, device, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = model_hard(torch.from_numpy(train_set._images[:100].reshape(batch_size,1,24,24,24)).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4324,  0.8455,  0.0822,  0.1313, -0.1046, -0.4683,  0.2780],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.86292964,  0.03844497,  0.09862536, -0.08708213,\n",
       "       -0.41256937,  0.25758612], dtype=float32)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set._labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../deepSymmetry/data/fromScripts/dataReady0\n",
      "Extracting ../deepSymmetry/data/fromScripts/dataReady0_label\n",
      "(39785, 13824)\n"
     ]
    }
   ],
   "source": [
    "train_name = '../deepSymmetry/data/fromScripts/dataReady0'\n",
    "train_set = load_data.read_data_set(train_name, dtype=dtypes.float16, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../deepSymmetry/data/densityMaps/density_testf\n",
      "Extracting ../deepSymmetry/data/densityMaps/density_testf_label\n",
      "(120000, 13824)\n"
     ]
    }
   ],
   "source": [
    "test_name = '../deepSymmetry/data/densityMaps/density_testf'\n",
    "test_set = load_data.read_data_set(test_name, dtype=dtypes.float16, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_test = model_hard(torch.from_numpy(test_set._images[:100].reshape(batch_size,1,24,24,24)).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.681089878082275, 0.8225629329681396, 0.24174951016902924, 0.033087629824876785, 0.010919880121946335, -0.17829450964927673, 0.6451407074928284]\n",
      "[ 7.          0.8140826   0.16944312  0.01647427 -0.07471883 -0.16377677\n",
      "  0.5252441 ]\n",
      "\n",
      "[7.156384468078613, -0.09772703796625137, 0.8188420534133911, 0.08037760853767395, -0.6574083566665649, 0.03126903250813484, -0.26504260301589966]\n",
      "[ 7.          0.01401387  0.8029993   0.18298681 -0.5421038   0.071615\n",
      " -0.15002087]\n",
      "\n",
      "[8.394238471984863, 1.0517383813858032, 0.08680028468370438, 0.1367802619934082, 0.03753487020730972, 0.0565904825925827, -0.18338654935359955]\n",
      "[ 7.0000000e+00  9.5146030e-01  4.8440255e-02  9.9439159e-05\n",
      " -3.1038227e-03  1.3755901e-02 -3.0360824e-01]\n",
      "\n",
      "[7.138195991516113, 0.21090511977672577, 0.5391544103622437, 0.294271856546402, -0.5449842214584351, -0.480374276638031, 0.7109291553497314]\n",
      "[ 7.          0.23470888  0.4653611   0.29993004 -0.5283479  -0.37522325\n",
      "  0.46738502]\n",
      "\n",
      "[8.730587005615234, 0.6358888149261475, 0.33428075909614563, 0.17487573623657227, -0.42174822092056274, 0.6431983709335327, -0.7727349400520325]\n",
      "[ 7.          0.45699063  0.31786323  0.22514616 -0.37832707  0.4536291\n",
      " -0.53900003]\n",
      "\n",
      "[8.575613975524902, 0.2678661346435547, 0.5918046832084656, 0.21760225296020508, 0.8090437650680542, 0.49064743518829346, 0.668545663356781]\n",
      "[7.         0.22514616 0.45699063 0.31786323 0.53900003 0.37832707\n",
      " 0.4536291 ]\n",
      "\n",
      "[6.385571479797363, -0.04580746591091156, 0.9540625810623169, 0.1501503735780716, -0.4468027353286743, 0.10782847553491592, -0.11363757401704788]\n",
      "[ 7.          0.01647427  0.8140826   0.16944312 -0.5252441   0.07471883\n",
      " -0.16377677]\n",
      "\n",
      "[7.198699474334717, 0.14840549230575562, -0.035713471472263336, 0.7983440160751343, 0.34743165969848633, 0.9519135355949402, 0.2301645129919052]\n",
      "[7.         0.18298681 0.01401387 0.8029993  0.15002087 0.5421038\n",
      " 0.071615  ]\n",
      "\n",
      "[8.586642265319824, -0.06985707581043243, 0.9221779704093933, 0.09014852344989777, 0.39408835768699646, -0.0018035918474197388, 0.10627113282680511]\n",
      "[7.0000000e+00 9.9439159e-05 9.5146030e-01 4.8440255e-02 3.0360824e-01\n",
      " 3.1038227e-03 1.3755901e-02]\n",
      "\n",
      "[6.30518913269043, 0.2668163776397705, 0.06963256001472473, 0.39703357219696045, -0.5968856811523438, 0.6275244951248169, -0.49467432498931885]\n",
      "[ 7.          0.29993004  0.23470888  0.4653611  -0.46738502  0.5283479\n",
      " -0.37522325]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(answer_test[i].data.tolist())\n",
    "    print(test_set._labels[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7283, -0.5375]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7258, -0.5350]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7233, -0.5325]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7208, -0.5300]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7183, -0.5275]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7158, -0.5250]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7133, -0.5225]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7108, -0.5200]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7083, -0.5175]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.9414, -0.5585]]) tensor([[-0.7058, -0.5150]], grad_fn=<AddmmBackward>)\n",
      "---\n",
      "tensor([[-0.7058, -0.5150]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-1.0793,  0.6403],\n",
      "        [-2.4632,  1.4614]])\n"
     ]
    }
   ],
   "source": [
    "def my_dirty_loss(output, target):\n",
    "    loss = torch.mean((output[0] - target[0])**2)\n",
    "    return loss\n",
    "\n",
    "def my_loss1(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "model = nn.Linear(2, 2)\n",
    "x = torch.randn(1, 2)\n",
    "target = torch.randn(1, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for i in range(10):\n",
    "    output = model(x)\n",
    "    loss = my_dirty_loss(output, target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(x, model(x))\n",
    "\n",
    "print('---')\n",
    "print(model(x))\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, target):\n",
    "    order_out = output[:, 0 : NUM_CLASSES]\n",
    "    order_target = target[:, 0 : 1].type(torch.LongTensor).squeeze_()\n",
    "    print(order_out.shape)\n",
    "    print(order_target.shape)\n",
    "    axis_out = output[:, NUM_CLASSES : NUM_CLASSES + 6]\n",
    "    axis_target = target[:, 1 : 7]\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()(order_out, order_target) + nn.MSELoss(reduction='sum')(axis_out, axis_target)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquiNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = SE3Convolution(repr_in_1, repr_out_2, size=4)\n",
    "        self.pool1 = nn.AvgPool3d(pool_size, pool_stride)\n",
    "        self.conv2 = SE3Convolution(repr_in_2, repr_out_2, size=4)\n",
    "        self.pool2 = nn.AvgPool3d(pool_size, pool_stride)\n",
    "        \n",
    "        self.lin1 = nn.Linear(n_input_1, n_output_1)\n",
    "        self.drop1 = nn.Dropout(prob)\n",
    "        self.lin2 = nn.Linear(n_output_1, n_output_2)\n",
    "        self.drop2 = nn.Dropout(prob)\n",
    "        self.lin3 = nn.Linear(n_output_2, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.gated(x)\n",
    "        x = self.pool1(self.conv1(x))\n",
    "        #x = self.pool2(self.conv2(x))\n",
    "        x = x.view(batch_size,-1) # tf.reshape(x,[batchSize,-1])\n",
    "        x = F.leaky_relu(self.lin1(x))\n",
    "        # x = self.drop1(x)\n",
    "        # x = F.relu(self.lin2(x))\n",
    "        # x = self.drop2(x)\n",
    "        return self.lin2(x) #self.lin3(x)\n",
    "    \n",
    "    \n",
    "def train(model, device, train_set, batch_size, optimizer, epoch, per_epoch, decr_rate):\n",
    "    model.train()\n",
    "    flag = True\n",
    "    new_epoch = True\n",
    "    \n",
    "    if new_epoch:\n",
    "        batch_idx = 1\n",
    "        new_epoch = False\n",
    "        data, target, _ = train_set.next_batch(batch_size)\n",
    "        data = torch.from_numpy(data.reshape(batch_size,1,24,24,24)).type(torch.FloatTensor)\n",
    "        target = torch.from_numpy(target.reshape(batch_size,-1)).type(torch.FloatTensor)\n",
    "        cnt = epoch // per_epoch\n",
    "        if ((epoch+1) // per_epoch > cnt) and flag:\n",
    "            lr = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            print(lr)\n",
    "            lr *= decr_rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss_fn = nn.MSELoss(reduction='sum')\n",
    "        # loss = loss_fn(output, target)\n",
    "        loss = custom_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    while (train_set._index_in_epoch + batch_size) < train_set._num_examples:\n",
    "        batch_idx += 1\n",
    "        data, target, _ = train_set.next_batch(batch_size)\n",
    "        data = torch.from_numpy(data.reshape(batch_size,1,24,24,24)).type(torch.FloatTensor)\n",
    "        target = torch.from_numpy(target.reshape(batch_size,-1)).type(torch.FloatTensor)\n",
    "        cnt = epoch // per_epoch\n",
    "        if ((epoch+1) // per_epoch > cnt) and flag:\n",
    "            lr = 0\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr = param_group['lr']\n",
    "            print(lr)\n",
    "            lr *= decr_rate\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # loss_fn = nn.MSELoss(reduction='sum')\n",
    "        # loss = loss_fn(output, target)\n",
    "        loss = custom_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, train_set._num_examples,\n",
    "                100. * batch_idx / train_set._num_examples, loss.item()))\n",
    "\n",
    "def test(model, device, test_set):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_set:\n",
    "            output = model(data)\n",
    "            loss_fn = nn.MSELoss(reduction='sum')\n",
    "            test_loss += loss_fn(output, target) \n",
    "            # pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += int(torch.argmax(output) == torch.argmax(target))\n",
    "\n",
    "    test_loss /= len(test_set)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_set),\n",
    "        100. * correct / len(test_set)))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_in_1 = [(1,0)]\n",
    "repr_out_1 = [(2,0),(2,1),(2,2)]\n",
    "repr_in_2 = [(2,0),(2,1),(2,2)]\n",
    "repr_out_2 = [(1,0),(2,1)]\n",
    "size = 4\n",
    "activation = (None, F.leaky_relu)\n",
    "pool_size = 2\n",
    "pool_stride = 2\n",
    "bias = True\n",
    "\n",
    "n_input_1 = 7000\n",
    "n_output_1 = 3500 \n",
    "n_output_2 = 7 \n",
    "\n",
    "batch_size = 100\n",
    "prob = 0.5\n",
    "NUM_CLASSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../deepSymmetry/data/fromScripts/dataReady0\n",
      "Extracting ../deepSymmetry/data/fromScripts/dataReady0_label\n",
      "(39785, 13824)\n"
     ]
    }
   ],
   "source": [
    "train_name = '../deepSymmetry/data/fromScripts/dataReady0'\n",
    "train_set = load_data.read_data_set(train_name, dtype=dtypes.float16, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.7/pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-3911606d3a5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_custom_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecr_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# test(model_hard, device, test_set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-195-53da222def66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_set, batch_size, optimizer, epoch, per_epoch, decr_rate)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# loss_fn = nn.MSELoss(reduction='sum')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# loss = loss_fn(output, target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-197-573cb0a8d66c>\u001b[0m in \u001b[0;36mcustom_loss\u001b[0;34m(output, target)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maxis_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder_target\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 904\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1788\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /Users/soumith/b101_2/2019_02_08/wheel_build_dirs/wheel_3.7/pytorch/aten/src/THNN/generic/ClassNLLCriterion.c:93"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "device = torch.device('cpu')\n",
    "torch.manual_seed(1)\n",
    "\n",
    "model_custom_loss = EquiNet().to(device)\n",
    "learning_rate = 5e-3\n",
    "optimizer = torch.optim.Adam(model_custom_loss.parameters(), lr=learning_rate)\n",
    "\n",
    "per_epoch = 50\n",
    "decr_rate = 0.99\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model_custom_loss, device, train_set, batch_size, optimizer, epoch, per_epoch, decr_rate)\n",
    "    # test(model_hard, device, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
